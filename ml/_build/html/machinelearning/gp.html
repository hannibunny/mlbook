
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Gaussian Process &#8212; Machine Learning Lecture</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.5f77b4aec8189eecf79907ce328c390d.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Gaussian Process: Implementation in Python" href="GaussianProcessRegression.html" />
    <link rel="prev" title="Support Vector Machines (SVM)" href="svm.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      <img src="../_static/hdmlogomed.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Machine Learning Lecture</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Intro and Overview Machine Learning Lecture
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../00BasicConcepts.html">
   Basic Concepts of Data Mining and Machine Learning
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Conventional ML
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="knn.html">
   K - Nearest Neighbour Classification / Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="parametricClassification1D.html">
   Bayes- and Naive Bayes Classifier
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="LinReg.html">
   Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="heartRateRegression.html">
   Example Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="LinearClassification.html">
   Linear Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="svm.html">
   Support Vector Machines (SVM)
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Gaussian Process
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="GaussianProcessRegression.html">
   Gaussian Process: Implementation in Python
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Neural Networks
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../neuralnetworks/01NeuralNets.html">
   Neural Networks Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../neuralnetworks/02RecurrentNeuralNetworks.html">
   Recurrent Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../neuralnetworks/03ConvolutionNeuralNetworks.html">
   Convolutional Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../neuralnetworks/convolutionDemos.html">
   Animations of Convolution and Deconvolution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../neuralnetworks/03KerasMLPandCNNcifar.html">
   Implementing Neural Networks with Keras
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../neuralnetworks/04KerasPretrainedClassifiers.html">
   Applying Pretrained Deep Neural Networks for Image Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../neuralnetworks/05KerasPretrainedCovid.html">
   Apply Pretrained Neural Networks on new Task
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Autoencoder
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../neuralnetworks/04VariationalAutoencoder.html">
   Variational Autoencoder (VAE) to generate handwritten digits
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  GAN
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../gan/GAN.html">
   Generative Adversarial Nets (GAN)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../gan/DCGAN.html">
   DCGAN Keras Implementation
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Reinforcement Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../rl/reinforcement.html">
   Reinforcement Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../rl/DQN.html">
   Deep Reinforcement Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../rl/QLearnFrozenLake.html">
   Example Q-Learning
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Text
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../text/01ModellingWordsAndTexts.html">
   Representations for Words and Texts
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../text/02TextClassification.html">
   Text classification with CNNs and LSTMs
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Transformer
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../transformer/attention.html">
   Sequence-To-Sequence, Attention, Transformer
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../transformer/intent_classification_with_bert.html">
   Intent Classification with BERT
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../referenceSection.html">
   References
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        <a class="dropdown-buttons"
            href="../_sources/machinelearning/gp.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download notebook file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/machinelearning/gp.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/machinelearning/gp.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   Introduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#recap-gaussian-normal-distribution">
   Recap Gaussian Normal Distribution
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#univariate">
     Univariate
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multivariate">
     Multivariate
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   Gaussian Process
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#covariance-function">
     Covariance Function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generate-samples-of-gaussian-process">
     Generate Samples of Gaussian Process
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#implementation-generate-samples-of-gp">
       Implementation: Generate Samples of GP
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gaussian-process-regression">
   Gaussian Process Regression
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#learn-hyperparameters-of-mean-and-covariance-function">
   Learn hyperparameters of mean- and covariance-function
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#final-remarks-on-the-covariance-function">
   Final remarks on the covariance function
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <hr class="docutils" />
<div class="section" id="gaussian-process">
<h1>Gaussian Process<a class="headerlink" href="#gaussian-process" title="Permalink to this headline">¶</a></h1>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>In the previous sections <strong>parametric</strong> and <strong>non-parametric</strong> supervised ML-methods have been introduced. In parametric algorithms, such as <a class="reference internal" href="LinReg.html"><span class="doc std std-doc">Linear Regression</span></a> one assumes a certain model type (e.g. the model is a linear function) and the algorithm learns the parameters of this model type (e.g. slope and bias) such that the concrete model fits well to the given training data. One the other hand a non-parametric approach such as <a class="reference internal" href="knn.html"><span class="doc std std-doc">K-Nearest Neighbors</span></a> does not require any assumptions about the model-type and it does not learn any parameters, that define a model. Instead it just saves all training data and predicts the output of new data by determining the nearest training instances.</p>
<p>Parametric methods are weak, if the assumption on the model-type is inadequate. After the training phase the entire knowledge of the training data is compressed in a few model parameters. This may constitute waste of information. For example, after training we do not know in which regions much training data has been available and hence predictions may have an increased reliabilty. The drawback of non-parametric methods is their large memory footprint and their long inference time. Moreover, since we do not have a model, predictions in regions, where no training-data has been available, are quite unreliable.</p>
<p>In this context a <strong>Gaussian Process</strong> can be considered to be a <strong>semi-parametric</strong> supervised ML-algorithm. In the inference phase the predictions are calculated from training data. It is not necessary to assume a certain model type (therefore non-parametric). However, on must assume a way how predictions are calculated from training data and possibly learn parameters, which specify this way (therefore parametric).</p>
<p>Gaussian Processes can be applied for regression and classification. However, in practice they are mostly applied for regression. In this lecture only the regression-case is considered.</p>
</div>
<div class="section" id="recap-gaussian-normal-distribution">
<h2>Recap Gaussian Normal Distribution<a class="headerlink" href="#recap-gaussian-normal-distribution" title="Permalink to this headline">¶</a></h2>
<p>A Gaussian Process is closely related to a <strong>Multidimensional Gaussian Distribution</strong>. Therefore, we first recall univariate and multivariate Gaussian distributions, before the Gaussian Process and it’s application for Regression will be described.</p>
<div class="section" id="univariate">
<h3>Univariate<a class="headerlink" href="#univariate" title="Permalink to this headline">¶</a></h3>
<p>The Power Density Function (PDF) of a <strong>Gaussian distribed random variable <span class="math notranslate nohighlight">\(X\)</span></strong> is:</p>
<div class="math notranslate nohighlight" id="equation-gausspdfuni">
<span class="eqno">(52)<a class="headerlink" href="#equation-gausspdfuni" title="Permalink to this equation">¶</a></span>\[
p_X(x)=\frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{(x-\mu)^2}{2\sigma^2}}, 
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mu\)</span> is the mean and <span class="math notranslate nohighlight">\(\sigma\)</span> is the standard deviation. This distribution is plotted below for two different standard deviations.</p>
<div class="figure align-default" id="id5">
<img alt="https://maucher.home.hdm-stuttgart.de/Pics/univariateGaussPDF.png" src="https://maucher.home.hdm-stuttgart.de/Pics/univariateGaussPDF.png" />
<p class="caption"><span class="caption-number">Fig. 18 </span><span class="caption-text">align: center
width: 600pt
name:  gausspdf</span><a class="headerlink" href="#id5" title="Permalink to this image">¶</a></p>
<div class="legend">
<p>PDFs of univariate Gaussian distribution with different standard deviations.</p>
</div>
</div>
<p>In the sequel a Gaussian distributed random variable <span class="math notranslate nohighlight">\(X\)</span> with mean <span class="math notranslate nohighlight">\(\mu\)</span> and standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span> is denoted by</p>
<div class="math notranslate nohighlight">
\[
X \sim \mathcal{N}(\mu,\sigma^2)
\]</div>
<p><strong>Estimate parameters from data:</strong></p>
<p>The univariate Gaussian distribution, as defined in equation <a class="reference internal" href="#equation-gausspdfuni">(52)</a> is completely defined by the parameters <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span>. If a sample of <span class="math notranslate nohighlight">\(Z\)</span> values <span class="math notranslate nohighlight">\(x_i\)</span> of a univariate random variable <span class="math notranslate nohighlight">\(X\)</span> are given and it can be assumed that the random variable is Gaussian distributed, the mean-value and the standard deviation can be estimated as follows.</p>
<p>Estimate for <span class="math notranslate nohighlight">\(\mu\)</span>:</p>
<div class="math notranslate nohighlight">
\[
m=\frac{1}{Z}\sum\limits_{i=1}^Z x_i 
\]</div>
<p>Estimate for <span class="math notranslate nohighlight">\(\sigma\)</span>:</p>
<div class="math notranslate nohighlight">
\[
s=\sqrt{\frac{1}{Z-1}\sum_{i=1}^Z (x_i-m)^2}.
\]</div>
</div>
<div class="section" id="multivariate">
<h3>Multivariate<a class="headerlink" href="#multivariate" title="Permalink to this headline">¶</a></h3>
<p>The Power Density Function (PDF) of a <strong>Multidimensional Gaussian Distribution</strong> is:</p>
<div class="math notranslate nohighlight" id="equation-gausspdfmulti">
<span class="eqno">(53)<a class="headerlink" href="#equation-gausspdfmulti" title="Permalink to this equation">¶</a></span>\[
  p(\mathbf{x})=\frac{1}{(2 \pi)^{d/2} |\Sigma|^{1/2}} \exp\left[-\frac{1}{2}(\mathbf{x}- \boldsymbol\mu)^T \Sigma^{-1}(\mathbf{x}-\boldsymbol\mu)\right] , \quad -\infty &lt; x &lt; \infty 
\]</div>
<p>Here</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\mathbf{x}=\left[x_1,x_2,\ldots,x_d \right]\)</span> are the values of <span class="math notranslate nohighlight">\(d\)</span> random variables, which are jointly Gaussian distributed.</p></li>
<li><p>the <strong>mean-value-vektor</strong> is</p>
<div class="math notranslate nohighlight" id="equation-meanvec">
<span class="eqno">(54)<a class="headerlink" href="#equation-meanvec" title="Permalink to this equation">¶</a></span>\[
	\mathbf{\mu}=[\mu_1,\mu_2,\ldots, \mu_d]
	\]</div>
</li>
<li><p>the <strong>covariance matrix</strong> is</p>
<div class="math notranslate nohighlight" id="equation-covmat">
<span class="eqno">(55)<a class="headerlink" href="#equation-covmat" title="Permalink to this equation">¶</a></span>\[\begin{split} 
	\Sigma = \left(
	\begin{array}{cccc}
	\sigma_{11}^2 &amp; \sigma_{12} &amp;\cdots &amp; \sigma_{1d} \\
	\sigma_{21} &amp; \sigma_{22}^2 &amp;\cdots &amp; \sigma_{2d} \\
	\vdots      &amp; \vdots      &amp; \ddots &amp;  \vdots \\
	\sigma_{d1} &amp; \sigma_{d2} &amp; \cdots &amp; \sigma_{dd}^2 \\
	\end{array} \right)
	\end{split}\]</div>
<p>In this matrix the elements on the principal diagonal <span class="math notranslate nohighlight">\(\sigma_{ii}^2\)</span> are the variances along the corresponding axis. All other elements are covariances, which describe the correlation between the axes. If <span class="math notranslate nohighlight">\(\sigma_{ij}=0\)</span>, then the random variables <span class="math notranslate nohighlight">\(X_i\)</span> and <span class="math notranslate nohighlight">\(X_j\)</span> are not correlated. The higher the absolute value of <span class="math notranslate nohighlight">\(\sigma_{ij}\)</span>, the stronger the correlation. From the variances and the covariances the <strong>linear correlation-coefficient <span class="math notranslate nohighlight">\(\rho_{ij}\)</span></strong> can be calculated as follows:</p>
<div class="math notranslate nohighlight" id="equation-corr">
<span class="eqno">(56)<a class="headerlink" href="#equation-corr" title="Permalink to this equation">¶</a></span>\[
	\rho_{ij}=\frac{\sigma_{ij}}{\sigma_{ii} \sigma_{jj}}
	\]</div>
<p>The correlation coefficient has a value-range from <span class="math notranslate nohighlight">\(-1\)</span> to <span class="math notranslate nohighlight">\(1\)</span> and helps to better <em>quantify</em> the correlation between the axis.</p>
</li>
<li><p><span class="math notranslate nohighlight">\(|\Sigma|\)</span> is the determinant of the covariance matrix</p></li>
<li><p><span class="math notranslate nohighlight">\(\Sigma^{-1}\)</span> is the inverse of the covariance matrix</p></li>
</ul>
<p>Below, the PDF of a 2-dimensional Gaussian distribution with</p>
<div class="math notranslate nohighlight">
\[
\mathbf{\mu}=[0,0]
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\Sigma = \left(
	\begin{array}{cc}
	1.5 &amp; 0  \\
	0 &amp; 1.5  \\
	\end{array} \right)
\end{split}\]</div>
<p>is plotted.</p>
<div class="figure align-center" id="gausspdf">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/gauss2dimpdf.png"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/gauss2dimpdf.png" src="https://maucher.home.hdm-stuttgart.de/Pics/gauss2dimpdf.png" style="width: 400pt;" /></a>
<p class="caption"><span class="caption-number">Fig. 19 </span><span class="caption-text">PDF of a 2-dimensional Gaussian distribution.</span><a class="headerlink" href="#gausspdf" title="Permalink to this image">¶</a></p>
</div>
<p><strong>Estimate parameters from data:</strong></p>
<p>In order to estimate a multi-dimensional Gaussian distribution from a dataset <span class="math notranslate nohighlight">\(T\)</span>, the mean-value-vektor <span class="math notranslate nohighlight">\(\mathbf{\mu}\)</span> and the covariance matrix <span class="math notranslate nohighlight">\(\mathbf{\Sigma}\)</span> must be estimated.</p>
<p>We denote the <strong>estimation of the mean-value-vektor <span class="math notranslate nohighlight">\(\mu\)</span></strong> by <span class="math notranslate nohighlight">\(m=[m_1,m_2,\ldots m_N]\)</span>. The components of this vektor are just the columnwise mean-values of the datamatrix:</p>
<div class="math notranslate nohighlight">
\[
m_i=\frac{1}{Z}\sum_{k=1}^Z x_{k,i} \quad \forall i \in \left\{ 1,N \right\}, 
\]</div>
<p>where <span class="math notranslate nohighlight">\(x_{k,i}\)</span> is the value of random variable <span class="math notranslate nohighlight">\(X_i\)</span> of instance <span class="math notranslate nohighlight">\(k\)</span>.</p>
<p>Moreover, the <strong>estimation of the covariance matrix <span class="math notranslate nohighlight">\(\Sigma\)</span></strong> is denoted by <span class="math notranslate nohighlight">\(S\)</span>. And the components of <span class="math notranslate nohighlight">\(S\)</span> are</p>
<ul class="simple">
<li><p>the estimations of the variances <span class="math notranslate nohighlight">\(\sigma_{ii}^2\)</span>, which are denoted by <span class="math notranslate nohighlight">\(s_{ii}^2\)</span></p></li>
<li><p>the estimations of the covariances <span class="math notranslate nohighlight">\(\sigma_{ij}\)</span>, which are denoted by <span class="math notranslate nohighlight">\(s_{ij}\)</span>.</p></li>
</ul>
<p>From a given dataset <span class="math notranslate nohighlight">\(T\)</span> with <span class="math notranslate nohighlight">\(Z\)</span> instances (rows) and <span class="math notranslate nohighlight">\(N\)</span> random variables (columns), the variances and covariances are estimated as follows:</p>
<div class="math notranslate nohighlight">
\[
s_{ii}^2=\frac{1}{Z-1}\sum_{k=1}^Z (x_{k,i}-m_i)^2
\]</div>
<div class="math notranslate nohighlight">
\[
s_{ij}=\frac{1}{Z-1}\sum_{k=1}^Z (x_{k,i}-m_i) \cdot (x_{k,j}-m_j)
\]</div>
<p>Below for two distinct 2-dimensional Gaussian distributions the PDF and a corresponding data sample are visualized. In the first example the two random variables are uncorrelated, in the second plot correlated.</p>
<div class="figure align-center" id="dimgauss1">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/2dimGaussSigma0.png"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/2dimGaussSigma0.png" src="https://maucher.home.hdm-stuttgart.de/Pics/2dimGaussSigma0.png" style="width: 600pt;" /></a>
<p class="caption"><span class="caption-number">Fig. 20 </span><span class="caption-text">Left: PDF of a 2-dimensional Gaussian distribution with no correlation between the two random variables <span class="math notranslate nohighlight">\(X_1\)</span> and <span class="math notranslate nohighlight">\(X_2\)</span>. Right: Sample of data, drawn from the PDF on the left hand side.</span><a class="headerlink" href="#dimgauss1" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-center" id="dimgauss2">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/2dimGaussSigma1.png"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/2dimGaussSigma1.png" src="https://maucher.home.hdm-stuttgart.de/Pics/2dimGaussSigma1.png" style="width: 600pt;" /></a>
<p class="caption"><span class="caption-number">Fig. 21 </span><span class="caption-text">Left: PDF of a 2-dimensional Gaussian distribution with strong positive correlation between the two random variables <span class="math notranslate nohighlight">\(X_1\)</span> and <span class="math notranslate nohighlight">\(X_2\)</span>. Right: Sample of data, drawn from the PDF on the left hand side.</span><a class="headerlink" href="#dimgauss2" title="Permalink to this image">¶</a></p>
</div>
</div>
</div>
<div class="section" id="id1">
<h2>Gaussian Process<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>After recalling Multidimensional Gaussian Distributions, it’s no big deal to understand Gaussian Processes. In a nutshell: Multidimensioanl Gaussian Distributions are distributions over a finite set of <span class="math notranslate nohighlight">\(d\)</span> correlated random variables. A Gaussian Process extends this to an infinite set of random variables. The differences are listed in the two panels below:</p>
<div class="sphinx-bs container pb-4 docutils">
<div class="row docutils">
<div class="d-flex col-lg-6 col-md-6 col-sm-6 col-xs-12 p-2 docutils">
<div class="card w-100 shadow docutils">
<div class="card-header docutils">
<p class="card-text">Multidimensional Gaussian Distribution</p>
</div>
<div class="card-body docutils">
<ul>
<li><p class="card-text">Joint Distribution over d Gaussian Variables</p>
<div class="math notranslate nohighlight">
\[
	X=\left[ X_1,X_2,\ldots X_d \right]
	\]</div>
</li>
<li><p class="card-text">At each index <span class="math notranslate nohighlight">\(i\)</span>, with <span class="math notranslate nohighlight">\(i \in \{1,\ldots,d\}\)</span> a Gaussian distributed variable <span class="math notranslate nohighlight">\(X_i\)</span> with mean <span class="math notranslate nohighlight">\(\mu_i\)</span> and variance <span class="math notranslate nohighlight">\(\sigma_i\)</span> is defined.</p></li>
<li><p class="card-text">The random variables <span class="math notranslate nohighlight">\(X_i\)</span> and <span class="math notranslate nohighlight">\(X_j\)</span> are correlated with covariance <span class="math notranslate nohighlight">\(\sigma_{ij}\)</span></p></li>
<li><p class="card-text">Each subset of the <span class="math notranslate nohighlight">\(d\)</span> random variables is again a Multidimensional Gaussian Distribution</p></li>
<li><p class="card-text">The  Multidimensional Gaussian Distribution is completely defined by it’s mean-vector <span class="math notranslate nohighlight">\(\mathbf{\mu}=[\mu_1,\mu_2,\ldots, \mu_d]\)</span> and it’s covariance matrix <span class="math notranslate nohighlight">\(\Sigma\)</span></p></li>
</ul>
</div>
</div>
</div>
<div class="d-flex col-lg-6 col-md-6 col-sm-6 col-xs-12 p-2 docutils">
<div class="card w-100 shadow docutils">
<div class="card-header docutils">
<p class="card-text">Gaussian Process</p>
</div>
<div class="card-body docutils">
<ul>
<li><p class="card-text">Distribution over continous functions</p>
<div class="math notranslate nohighlight">
\[
  f(x)
  \]</div>
</li>
<li><p class="card-text">For each <span class="math notranslate nohighlight">\(x\)</span> with <span class="math notranslate nohighlight">\(-\infty &lt; x &lt; \infty\)</span> a Gaussian distributed <span class="math notranslate nohighlight">\(f(x)\)</span> with mean <span class="math notranslate nohighlight">\(m(x)\)</span> and variance <span class="math notranslate nohighlight">\(k(x,x)\)</span> is defined.</p></li>
<li><p class="card-text">The function values <span class="math notranslate nohighlight">\(f(x_i)\)</span> and <span class="math notranslate nohighlight">\(f(x_j)\)</span> are correlated with covariance <span class="math notranslate nohighlight">\(k(x_i,x_j)\)</span></p></li>
<li><p class="card-text">Each finite subset of the infinity number of function values <span class="math notranslate nohighlight">\(f(x_i)\)</span> is a Multidimensional Gaussian Distribution</p></li>
<li><p class="card-text">The Gaussian Process is completely defined by it’s mean-function <span class="math notranslate nohighlight">\(m(x)\)</span> and it’s covariance function <span class="math notranslate nohighlight">\(k(x_i,x_j)\)</span></p></li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="admonition-gaussian-process admonition">
<p class="admonition-title">Gaussian Process</p>
<p>A Gaussian Process is a Probabilistic Distribution over functions <span class="math notranslate nohighlight">\(f(x)\)</span>, with <span class="math notranslate nohighlight">\(-\infty &lt; x &lt; \infty\)</span>. Since there exists an infinite number of values for <span class="math notranslate nohighlight">\(x\)</span> it can be considered as an infinite-dimensional Gaussian distribution. Each finite subset of function-values <span class="math notranslate nohighlight">\(f(x_1),f(x_2),\ldots,f(x_d)\)</span> is a usual Multidimensional Gaussian Distribution.</p>
</div>
<p>In the sequel a Gaussian Process with mean-function <span class="math notranslate nohighlight">\(m(x)\)</span> and covariance function <span class="math notranslate nohighlight">\(k(x,x')\)</span> is denoted by</p>
<div class="math notranslate nohighlight">
\[
f \sim \mathcal{GP}(m,k)
\]</div>
<div class="section" id="covariance-function">
<h3>Covariance Function<a class="headerlink" href="#covariance-function" title="Permalink to this headline">¶</a></h3>
<p>The most common covariance-function <span class="math notranslate nohighlight">\(k(x,x')\)</span> is the <strong>squared exponential</strong></p>
<div class="math notranslate nohighlight" id="equation-squaredexp">
<span class="eqno">(57)<a class="headerlink" href="#equation-squaredexp" title="Permalink to this equation">¶</a></span>\[
k(x,x')= \sigma_f^2 \cdot e^{- \frac{(x-x')^2}{2\ell^2}}
\]</div>
<p>Parameters and characteristics of this covariance function are</p>
<ul class="simple">
<li><p>The correlation between <span class="math notranslate nohighlight">\(f(x)\)</span> and <span class="math notranslate nohighlight">\(f(x')\)</span> decreases with increasing distance between <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(x'\)</span>.</p></li>
<li><p><strong>Length-Scale <span class="math notranslate nohighlight">\(\ell\)</span></strong>: The higher <span class="math notranslate nohighlight">\(\ell\)</span> the slower the decrease of the correlation between <span class="math notranslate nohighlight">\(f(x)\)</span> and <span class="math notranslate nohighlight">\(f(x')\)</span> with increasing distance between <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(x'\)</span>. A high value <span class="math notranslate nohighlight">\(\ell\)</span> means strong correlation between neighbouring function-values. This yields <em>smooth</em> curves. Small values for <span class="math notranslate nohighlight">\(\ell\)</span> means less correlation and the potential for high differences in neighbouring function-values.</p></li>
<li><p><strong>Variance <span class="math notranslate nohighlight">\(\sigma_f^2\)</span></strong>. This is the maximal covariance value and defines the value on the main diagonal of the covariance-matrix. This hyperparameter should be large, if one can assume a strong deviation around the mean-value.</p></li>
</ul>
<div class="sphinx-bs container pb-4 docutils">
<div class="row docutils">
<div class="d-flex col-lg-6 col-md-6 col-sm-6 col-xs-12 p-2 docutils">
<div class="card w-100 shadow docutils">
<div class="card-header docutils">
<p class="card-text">Length-scale <span class="math notranslate nohighlight">\(\ell=1.0\)</span></p>
</div>
<div class="card-body docutils">
<div class="figure align-default" id="id6">
<img alt="https://maucher.home.hdm-stuttgart.de/Pics/gaussianProcessSamplesZeroMean.png" src="https://maucher.home.hdm-stuttgart.de/Pics/gaussianProcessSamplesZeroMean.png" />
<p class="caption"><span class="caption-number">Fig. 22 </span><span class="caption-text">High length-scale in squared-exponential covariance function</span><a class="headerlink" href="#id6" title="Permalink to this image">¶</a></p>
</div>
</div>
</div>
</div>
<div class="d-flex col-lg-6 col-md-6 col-sm-6 col-xs-12 p-2 docutils">
<div class="card w-100 shadow docutils">
<div class="card-header docutils">
<p class="card-text">Length-scale <span class="math notranslate nohighlight">\(\ell=0.408\)</span></p>
</div>
<div class="card-body docutils">
<div class="figure align-default" id="id7">
<img alt="https://maucher.home.hdm-stuttgart.de/Pics/gaussianProcessSamplesZeroMeanTheta3.png" src="https://maucher.home.hdm-stuttgart.de/Pics/gaussianProcessSamplesZeroMeanTheta3.png" />
<p class="caption"><span class="caption-number">Fig. 23 </span><span class="caption-text">Smaller length-scale in squared-exponential covariance function</span><a class="headerlink" href="#id7" title="Permalink to this image">¶</a></p>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="generate-samples-of-gaussian-process">
<h3>Generate Samples of Gaussian Process<a class="headerlink" href="#generate-samples-of-gaussian-process" title="Permalink to this headline">¶</a></h3>
<p>Even though functions assign one function value to each argument of a possibly <strong>infinite domain</strong>, in computer programs functions are evaluated always at a finite set of arguments. Therefore, in computer programs one can think of functions as tables, in which to each domain value <span class="math notranslate nohighlight">\(x\)</span> a corresponding function value <span class="math notranslate nohighlight">\(f(x)\)</span> is mapped. <strong>Since in computer programs we always have finite subsets and any finite subset of a Gaussian Process is a Multidimensional Gaussisan Distribution, we can generate samples of a Gaussian Process in exactly the same was as we generate samples of a Multidimensional Gaussian Distribution.</strong> For generating samples of a Multidimensional Gaussian Distribution we have to specify the mean vector (<a class="reference internal" href="#equation-meanvec">(54)</a>) and the covariance matrix (<a class="reference internal" href="#equation-covmat">(55)</a>).</p>
<p>The mean-value vector is obtained by evaluating the mean function <span class="math notranslate nohighlight">\(m(x)\)</span> at all <span class="math notranslate nohighlight">\(x_i\)</span> of the domain.</p>
<div class="math notranslate nohighlight" id="equation-kmue">
<span class="eqno">(58)<a class="headerlink" href="#equation-kmue" title="Permalink to this equation">¶</a></span>\[
\boldsymbol\mu=[\mu_1,\mu_2,\ldots, \mu_N]= [m(x_1), m(x_2), \ldots m(x_N)]
\]</div>
<p>The covariance matrix <span class="math notranslate nohighlight">\(K\)</span> is obtained by evaluating the covariance function <span class="math notranslate nohighlight">\(k(x,x')\)</span> at all possible pairs of arguments, i.e. the entry in row <span class="math notranslate nohighlight">\(i\)</span>, column <span class="math notranslate nohighlight">\(j\)</span> of the covariance matrix is <span class="math notranslate nohighlight">\(k(x_i,x_j)\)</span>.</p>
<div class="math notranslate nohighlight" id="equation-kk">
<span class="eqno">(59)<a class="headerlink" href="#equation-kk" title="Permalink to this equation">¶</a></span>\[\begin{split}
K=\left( 
\begin{array}{cccc}
k(x_1,x_1) &amp; k(x_1,x_2) &amp; \ldots &amp; k(x_1,x_N) \\
k(x_2,x_1) &amp; k(x_2,x_2) &amp; \ldots &amp; k(x_2,x_N) \\
\vdots     &amp; \vdots     &amp; \ddots &amp; \vdots     \\
k(x_N,x_1) &amp; k(x_N,x_2) &amp; \ldots &amp; k(x_N,x_N) \\ 
\end{array}
\right)
\end{split}\]</div>
<div class="tip admonition">
<p class="admonition-title">Example: Calculation of mean-vector and covariance matrix</p>
<p>Assume that the domain has only 4 elements</p>
<div class="math notranslate nohighlight">
\[
   \mathbf{x}=[1,2,3,4]
   \]</div>
<p>For the mean function</p>
<div class="math notranslate nohighlight">
\[
   m(x)=\frac{x^2}{4}
   \]</div>
<p>and the covariance function</p>
<div class="math notranslate nohighlight">
\[
   k(x,x')=2 \cdot e^{-\frac{1}{2}(x-x')^2}
   \]</div>
<p>the corresponding mean-vector and covariance matrix are:</p>
<div class="math notranslate nohighlight">
\[
   \boldsymbol\mu=[0.25, 1.0 , 2.25, 4.0]
   \]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[\begin{split}
   K=\left( 
   \begin{array}{cccc}
   2.0   &amp;  1.213 &amp; 0.271 &amp; 0.022 \\
   1.213 &amp; 2.0    &amp; 1.213 &amp; 0.271 \\
   0.271 &amp; 1.213  &amp;2.0    &amp; 1.213 \\
   0.022 &amp; 0.271  &amp; 1.213 &amp; 2.0   \\
   \end{array}
   \right),
   \end{split}\]</div>
<p>respectively.</p>
</div>
<div class="section" id="implementation-generate-samples-of-gp">
<h4>Implementation: Generate Samples of GP<a class="headerlink" href="#implementation-generate-samples-of-gp" title="Permalink to this headline">¶</a></h4>
<p>Below it is shown how samples of a Gaussian Process can be generated.</p>
<p>Generate domain, mean-vector and covariance-matrix:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">x</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">35</span><span class="p">)</span> 
<span class="n">mx</span><span class="o">=</span><span class="mf">0.5</span><span class="o">+</span><span class="n">x</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>    
<span class="n">K</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">),</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)):</span>
        <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-</span><span class="n">x</span><span class="p">[</span><span class="n">j</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="c1">#covariance function</span>
        <span class="n">K</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span><span class="o">=</span><span class="n">k</span>
        <span class="n">K</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="n">k</span>
</pre></div>
</div>
</div>
</div>
<p>Generate 3 samples of a Multidimensional Gausian Distribution</p>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pointset</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mx</span><span class="p">,</span><span class="n">K</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span> <span class="c1">#Erzeugt 3 Samples einer multivariaten</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pointset</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[ 1.54027242  1.76048874  2.00491668  2.23107281  2.42603769  2.60014035
   2.76747691  2.92645275  3.0517938   3.10146836  3.03321733  2.82088566
   2.46230134  1.9762509   1.3922605   0.73985412  0.04257636 -0.6819736
  -1.41833385 -2.14856287 -2.84825195 -3.48322203 -4.00738347 -4.36362313
  -4.4896482  -4.3285432  -3.84104605 -3.01563567 -1.87398413 -0.47141197
   1.10716319  2.75569974  4.35999634  5.81399539  7.03606692]
 [-0.97194541 -0.85825396 -0.50521377  0.03264014  0.66984002  1.29493088
   1.7833814   2.02014871  1.92633763  1.48026863  0.72510416 -0.23955942
  -1.28009747 -2.26049443 -3.07605735 -3.67935289 -4.0880845  -4.36967865
  -4.6072958  -4.86135538 -5.14324584 -5.41176903 -5.59176933 -5.6045907
  -5.39608457 -4.95089014 -4.28942556 -3.45248287 -2.48327889 -1.41591101
  -0.27357277  0.9261506   2.16402836  3.41344108  4.63690566]
 [-0.52563715 -0.18522723  0.28864203  0.83159417  1.37452955  1.85428363
   2.22372722  2.45739977  2.55031446  2.51058327  2.34960841  2.07503339
   1.6902748   1.20058313  0.62132969 -0.01759752 -0.67520595 -1.30943998
  -1.88809287 -2.39411392 -2.82254509 -3.17164319 -3.43366566 -3.5895871
  -3.60858918 -3.45105235 -3.07452326 -2.44368033 -1.54435127 -0.3974723
   0.9348691   2.35767474  3.76433325  5.06106698  6.18199884]]
</pre></div>
</div>
</div>
</div>
<p>Visualize the generated samples and the mean-function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">r_</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">mx</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;mean $m(x)=0.5+x*sin(x)$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">pointset</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="s1">&#39;--&#39;</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">pointset</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="s1">&#39;-.&#39;</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">pointset</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="s2">&quot;$m(x)=0.5+x*sin(x)$ </span><span class="se">\n</span><span class="s2">$k(x,x&#39;)=2 \cdot \exp(0.5\cdot(x-x&#39;)^2)$&quot;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Random Samples and Mean from a Gaussian Process&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;f(x)&#39;</span><span class="p">)</span>
<span class="n">fillx</span> <span class="o">=</span> <span class="n">r_</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
<span class="nb">vars</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">K</span><span class="p">)</span>
<span class="n">stds</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="nb">vars</span><span class="p">)</span>
<span class="n">filly</span> <span class="o">=</span> <span class="n">r_</span><span class="p">[</span><span class="n">mx</span><span class="o">+</span><span class="mi">2</span><span class="o">*</span><span class="n">stds</span><span class="p">,</span> <span class="n">mx</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">stds</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="n">fillx</span><span class="p">,</span> <span class="n">filly</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/gp_7_0.png" src="../_images/gp_7_0.png" />
</div>
</div>
</div>
</div>
</div>
<div class="section" id="gaussian-process-regression">
<h2>Gaussian Process Regression<a class="headerlink" href="#gaussian-process-regression" title="Permalink to this headline">¶</a></h2>
<p>In the previous subsection it was described how samples of a Gaussian Process can be generated, given a mean-function and a covariance function. However, up to now nothing has been said, how this can be applied for a supervised ML regression task. The idea is that the Gaussian Process with defined mean- and covariance function constiute a <strong>prior</strong>. On the basis of this prior we calculate a <strong>posterior</strong> for the given training data</p>
<div class="math notranslate nohighlight">
\[
T=\{x_t,y_t \}_{t=1}^N
\]</div>
<p>In particular the given training data are considered to be support-points of a sample from</p>
<div class="math notranslate nohighlight">
\[
\mathcal{GP}(m,k)
\]</div>
<p>and we can calculate all other points <span class="math notranslate nohighlight">\((x,f(x))\)</span> on this particular GP sample from the mean- and covariance function and the given support-points. The prediction at an arbitrary argument <span class="math notranslate nohighlight">\(x\)</span> is <span class="math notranslate nohighlight">\(f(x)\)</span>.</p>
<div class="figure align-center" id="supportpoints">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/gaussianProcessPosteriorZeroMean.png"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/gaussianProcessPosteriorZeroMean.png" src="https://maucher.home.hdm-stuttgart.de/Pics/gaussianProcessPosteriorZeroMean.png" style="width: 600pt;" /></a>
<p class="caption"><span class="caption-number">Fig. 24 </span><span class="caption-text">Training data is considered to constitute support-points (red) of a GP sample. By applying the mean- and covariance-function of the GP all other points of this sample (green dashed line) can be determined.</span><a class="headerlink" href="#supportpoints" title="Permalink to this image">¶</a></p>
</div>
<p>Recall that in <a class="reference internal" href="LinReg.html"><span class="doc std std-doc">Linear Regression</span></a>, it is assumed that the output <span class="math notranslate nohighlight">\(y_t\)</span> is the sum of a deterministic term <span class="math notranslate nohighlight">\(f(x_t)\)</span> and a noise term <span class="math notranslate nohighlight">\(z_t\)</span></p>
<div class="math notranslate nohighlight">
\[
y_t=f(x_t)+n_t
\]</div>
<p>The noise term <span class="math notranslate nohighlight">\(n_t\)</span> is assumed to be a sample of a 1-dimensional Gaussian distribution with zero mean and variance <span class="math notranslate nohighlight">\(\sigma_n^2\)</span>. In Linear Regression one tries to estimate a good approximation</p>
<div class="math notranslate nohighlight">
\[
g(x)=w_0+w_1x_1+w_2x_2+\ldots+w_dx_d
\]</div>
<p>for the determinisic part <span class="math notranslate nohighlight">\(f(x)\)</span>. This approximation <span class="math notranslate nohighlight">\(g(x)\)</span> minimizes the Mean Squared Error between the given labels <span class="math notranslate nohighlight">\(y\)</span> and the model’s prediction <span class="math notranslate nohighlight">\(g(x)\)</span> on the training data.</p>
<p><strong>Now in Gaussian Process Regression</strong> we assume that training data constitutes support-points on a sample of a Gaussian Process with predefined mean- and covariance function. In contrast to Linear Regression now the assumption is</p>
<div class="math notranslate nohighlight" id="equation-gpass">
<span class="eqno">(60)<a class="headerlink" href="#equation-gpass" title="Permalink to this equation">¶</a></span>\[
y_t=f(x_t)+n_t+z_t,
\]</div>
<p><strong>where <span class="math notranslate nohighlight">\(z_t\)</span> is considered to be a component of a sample of an N-dimensional Gaussian Distribution with zero mean.</strong> As in Linear Regression <span class="math notranslate nohighlight">\(n_t\)</span> is assumed to be a sample of a 1-dimensional Gaussian distribution with zero mean and variance <span class="math notranslate nohighlight">\(\sigma_n^2\)</span>. It is independent of all other training instances. The independent noise term <span class="math notranslate nohighlight">\(n_t\)</span> and the correlated noise term <span class="math notranslate nohighlight">\(z_t\)</span> can be integrated into a single Multidimensional Gaussian distribution by adding the term</p>
<div class="math notranslate nohighlight">
\[
\sigma_n^2 \delta(x,x')
\]</div>
<p>to the covariance function <span class="math notranslate nohighlight">\(k(x,x')\)</span>, with the <em>Kronecker function</em></p>
<div class="math notranslate nohighlight">
\[\begin{split}
\delta(x,x')= \left\{ 
\begin{array}{lcl} 
1 &amp; \mbox{ falls } &amp; x=x' \\ 
0 &amp; \mbox{ falls } &amp; x \neq x'
\end{array}
\right.
\end{split}\]</div>
<p>For example if the original covariance function is the squared exponential, as definied in equation <a class="reference internal" href="#equation-squaredexp">(57)</a> the new covariance function which integrates the noise term is</p>
<div class="math notranslate nohighlight" id="equation-squaredexpex">
<span class="eqno">(61)<a class="headerlink" href="#equation-squaredexpex" title="Permalink to this equation">¶</a></span>\[
k_n(x,x')= \sigma_f^2 \cdot e^{- \frac{(x-x')^2}{2\ell^2}} + \sigma_n^2 \delta(x,x')
\]</div>
<p>If the covariance-matrix is calculated with this noise-integrating covariance function the difference to the covariance matrix without noise-integration is, that now the values of the main diagonal are <span class="math notranslate nohighlight">\(\sigma_f^2+\sigma_n^2\)</span>, rather than <span class="math notranslate nohighlight">\(\sigma_f^2\)</span>.</p>
<p>The main differences between GP Regression and Linear Regression are:</p>
<ol class="simple">
<li><p>in GP Regression the covariances between the training-instances are regarded, i.e. dependecies between neighboring training instances are taken into account.</p></li>
<li><p>in GP Regression the goal is not to learn an approximation for <span class="math notranslate nohighlight">\(f(x)\)</span>. Instead, the values <span class="math notranslate nohighlight">\(y_*\)</span> at the arguments of interest <span class="math notranslate nohighlight">\(x_*\)</span> are estimated directly.</p></li>
</ol>
<p>The first difference has already been discussed in the context of the assumption stated in equation <a class="reference internal" href="#equation-gpass">(60)</a>. In the sequel we will take a look on the second difference. In particular it will be described how the predictions <span class="math notranslate nohighlight">\(y_*\)</span> will be calculated for new inputs <span class="math notranslate nohighlight">\(x_*\)</span>:</p>
<p>As we already know, the mean vector <span class="math notranslate nohighlight">\(\boldsymbol\mu\)</span> and the covariance matrix <span class="math notranslate nohighlight">\(K\)</span> can be calculated from the training dataset <span class="math notranslate nohighlight">\(T=\{x_t,y_t \}_{t=1}^N\)</span>. The targets of the training dataset <span class="math notranslate nohighlight">\(\mathbf{y} = [y_1, y_2, \ldots y_N]\)</span> constitute a sample of an <span class="math notranslate nohighlight">\(N-\)</span>dimensional Gaussian Distribution</p>
<div class="math notranslate nohighlight">
\[
[\mathbf{y}] \sim \mathcal{N}(\boldsymbol\mu,K). 
\]</div>
<p>Now, if we want to estimate the value of <span class="math notranslate nohighlight">\(y_*\)</span> at a new input of interest <span class="math notranslate nohighlight">\(x_*\)</span>, we assume that the extended target vector</p>
<div class="math notranslate nohighlight">
\[
[y_1, y_2, \ldots y_N,y_*]
\]</div>
<p>is a sample of an <span class="math notranslate nohighlight">\((N+1)-\)</span>dimensional Gaussian Distribution with mean vector</p>
<div class="math notranslate nohighlight" id="equation-meanext">
<span class="eqno">(62)<a class="headerlink" href="#equation-meanext" title="Permalink to this equation">¶</a></span>\[
[\mu_1,\mu_2,\ldots, \mu_N,  \mu_*]= [m(x_1), m(x_2), \ldots ,m(x_N),m(x_*)]
\]</div>
<p>and covariance matrix</p>
<div class="math notranslate nohighlight" id="equation-kext">
<span class="eqno">(63)<a class="headerlink" href="#equation-kext" title="Permalink to this equation">¶</a></span>\[\begin{split}
\left(
\begin{array}{cc}
K &amp; K_*^T \\
K_* &amp; K_{**} \\
\end{array}
\right),
\end{split}\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
K_*=[k(x_*,x_1), k(x_*,x_2), \ldots, k(x_*,x_N)] \quad \mbox{und} \quad K_{**}=k(x_*,x_*).
\]</div>
<p>The corresponding notation is</p>
<div class="math notranslate nohighlight" id="equation-sampleext">
<span class="eqno">(64)<a class="headerlink" href="#equation-sampleext" title="Permalink to this equation">¶</a></span>\[\begin{split}
\left[ \begin{array}{c}
\mathbf{y}\\
y_*
\end{array} \right] \sim \mathcal{N}
\left( \begin{array}{cc}
\left[
\begin{array}{c}
\boldsymbol\mu \\ \mu_*
\end{array}
\right]
,
&amp;
\left[
\begin{array}{cc}
K &amp; K_*^T \\
K_* &amp; K_{**} \\
\end{array}
\right]   
\end{array} \right)
\end{split}\]</div>
<p>Since the values of <span class="math notranslate nohighlight">\(\mathbf{y} = [y_1, y_2, \ldots y_N]\)</span> are given, we are not interested in the joint distribution, but rather in the conditional distribution for <span class="math notranslate nohighlight">\(y_*\)</span>, given <span class="math notranslate nohighlight">\(\mathbf{y}=[y_1, y_2, \ldots y_N]\)</span>:</p>
<div class="math notranslate nohighlight">
\[
p(y_*|\mathbf{y}).
\]</div>
<p>It is known, that the conditional probability distribution of a multidimensional Gaussian distribution (see figure <a class="reference internal" href="#condprob"><span class="std std-numref">Fig. 25</span></a>) is also a multidimensional Gaussian distribution, which in the given case is specified as follows:</p>
<div class="math notranslate nohighlight">
\[
y_*|\mathbf{y} \sim \mathcal{N} \left(\mu_* + K_*K^{-1}(\mathbf{y}-\boldsymbol\mu) \; , \; K_{**} - K_*K^{-1}K_*^T\right)
\]</div>
<div class="admonition-gp-regression-predict-target-y-at-input-x admonition">
<p class="admonition-title">GP Regression: predict target <span class="math notranslate nohighlight">\(y_*\)</span> at input <span class="math notranslate nohighlight">\(x_*\)</span></p>
<p>The prediction <span class="math notranslate nohighlight">\(y_*\)</span> is the mean</p>
<div class="math notranslate nohighlight" id="equation-estmean">
<span class="eqno">(65)<a class="headerlink" href="#equation-estmean" title="Permalink to this equation">¶</a></span>\[
\overline{y_*}= \mu_* + K_*K^{-1}(\mathbf{y}-\boldsymbol\mu)
\]</div>
<p>The corresponding variance of the prediction is</p>
<div class="math notranslate nohighlight" id="equation-estvar">
<span class="eqno">(66)<a class="headerlink" href="#equation-estvar" title="Permalink to this equation">¶</a></span>\[
var(y_*)=diag\left( K_{**} - K_*K^{-1}K_*^T \right)
\]</div>
<p>and the standard-deviation of the prediction <span class="math notranslate nohighlight">\(y_*\)</span> is</p>
<div class="math notranslate nohighlight" id="equation-eststd">
<span class="eqno">(67)<a class="headerlink" href="#equation-eststd" title="Permalink to this equation">¶</a></span>\[
std(y_*)=\sqrt(var(y_*))
\]</div>
<p>Here <span class="math notranslate nohighlight">\(diag(X)\)</span> is the main-diagonal of matrix <span class="math notranslate nohighlight">\(X\)</span>. In equation <a class="reference internal" href="#equation-estvar">(66)</a> the argument of <span class="math notranslate nohighlight">\(diag()\)</span> is a single value, if the prediction <span class="math notranslate nohighlight">\(y_*\)</span> has been calculated only at a single input <span class="math notranslate nohighlight">\(x_*\)</span>, but it is a <span class="math notranslate nohighlight">\(Z \times Z\)</span> matrix, if predictions are calculated at <span class="math notranslate nohighlight">\(Z\)</span> inputs.</p>
</div>
<p>The standard deviations <span class="math notranslate nohighlight">\(std(y_*)\)</span> indicate, that the confidence that the true prediction lies within an interval <span class="math notranslate nohighlight">\(y_* \pm 2 \cdot std(y_*)\)</span> is <span class="math notranslate nohighlight">\(95\%\)</span>.</p>
<div class="figure align-center" id="condprob">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/conditionalGaussianDistribution.PNG"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/conditionalGaussianDistribution.PNG" src="https://maucher.home.hdm-stuttgart.de/Pics/conditionalGaussianDistribution.PNG" style="width: 400pt;" /></a>
<p class="caption"><span class="caption-number">Fig. 25 </span><span class="caption-text">The conditional probability distribution of a 2-dimensional Gaussian distribution is a 1-dimensional Gaussian distribution</span><a class="headerlink" href="#condprob" title="Permalink to this image">¶</a></p>
</div>
<div class="admonition-gp-regression-predict-target-mathbf-y-y-1-y-2-ldots-y-z-at-many-inputs-mathbf-x-x-1-x-2-ldots-x-z admonition">
<p class="admonition-title">GP Regression: predict target <span class="math notranslate nohighlight">\(\mathbf{y_*}=[y_{*1},y_{*2},\ldots, y_{*Z} ]\)</span> at many inputs <span class="math notranslate nohighlight">\(\mathbf{x_*}=[x_{*1},x_{*2},\ldots, x_{*Z} ]\)</span></p>
<p>Calculation of <span class="math notranslate nohighlight">\(\boldsymbol\mu_*\)</span> (to be used in <a class="reference internal" href="#equation-meanext">(62)</a>):</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol\mu_*= [ \mu_{*1}, \ldots ,\mu_{*Z} ] \; = \; [m(x_{*1}), \ldots, m(x_{*Z})]
\]</div>
<p>Calculation of <span class="math notranslate nohighlight">\(K_*\)</span> (to be used in <a class="reference internal" href="#equation-kext">(63)</a>)</p>
<div class="math notranslate nohighlight" id="equation-k">
<span class="eqno">(69)<a class="headerlink" href="#equation-k" title="Permalink to this equation">¶</a></span>\[\begin{split}
K_*= \left( 
\begin{array}{ccc}
k(x_{*1},x_1) &amp; \ldots &amp; k(x_{*1},x_N) \\
k(x_{*2},x_1) &amp; \ldots &amp; k(x_{*2},x_N) \\
\vdots        &amp; \ddots &amp; \vdots      \\
k(x_{*Z},x_1) &amp; \ldots &amp; k(x_{*Z},x_N) \\ 
\end{array}
\right)
\end{split}\]</div>
<p>Calculation of <span class="math notranslate nohighlight">\(K_{**}\)</span> (to be used in <a class="reference internal" href="#equation-kext">(63)</a>)</p>
<div class="math notranslate nohighlight" id="equation-k">
<span class="eqno">(69)<a class="headerlink" href="#equation-k" title="Permalink to this equation">¶</a></span>\[\begin{split}
K_{**}= \left( 
\begin{array}{ccc}
k(x_{*1},x_{*1}) &amp; \ldots &amp; k(x_{*1},x_{*Z}) \\
k(x_{*2},x_{*1}) &amp; \ldots &amp; k(x_{*2},x_{*Z}) \\
\vdots        &amp; \ddots &amp; \vdots      \\
k(x_{*Z},x_{*1}) &amp; \ldots &amp; k(x_{*Z},x_{*Z}) \\ 
\end{array}
\right)
\end{split}\]</div>
<p>Apply <span class="math notranslate nohighlight">\(\boldsymbol\mu_*\)</span>, <span class="math notranslate nohighlight">\(K_*\)</span> and <span class="math notranslate nohighlight">\(K_{**}\)</span> in equations <a class="reference internal" href="#equation-estmean">(65)</a> and <a class="reference internal" href="#equation-estvar">(66)</a> in order to compute all <span class="math notranslate nohighlight">\(Z\)</span> predictions in one step.</p>
</div>
<div class="tip admonition">
<p class="admonition-title">Example: GP Regression</p>
<p><strong>Task:</strong> For the training dataset</p>
<div class="math notranslate nohighlight">
\[
T=\left\{ (1.0,0.32),(2.0,0.81),(3.0,2.75),(4.0,3.6)\right\}
\]</div>
<p>predictions at the inputs <span class="math notranslate nohighlight">\(x=5, x=6\)</span> and <span class="math notranslate nohighlight">\(x=7\)</span> and the corresponding reliability of the predictions shall be calculated. The mean function is</p>
<div class="math notranslate nohighlight">
\[
m(x)=\frac{x^2}{4}
\]</div>
<p>and the covariance function which integrates the noise <span class="math notranslate nohighlight">\(\sigma_n^2=0.005\)</span> is</p>
<div class="math notranslate nohighlight">
\[
k(x,x')= 2 \cdot \exp(-0.5(x-x')^2)+0.005 \cdot \delta(x,x')
\]</div>
<p><strong>Solution:</strong></p>
<p>In order to calculate equations <a class="reference internal" href="#equation-estmean">(65)</a> and <a class="reference internal" href="#equation-estvar">(66)</a>, first the required matrices must be determined:</p>
<ul>
<li><p>Covariance Matrix of training data (equation <code class="xref eq docutils literal notranslate"><span class="pre">kk</span></code>):</p>
<div class="math notranslate nohighlight">
\[\begin{split}
   K=\left( 
   \begin{array}{cccc}
   2.005   &amp;  1.213 &amp; 0.271 &amp; 0.022 \\
   1.213 &amp; 2.005    &amp; 1.213 &amp; 0.271 \\
   0.271 &amp; 1.213  &amp;2.005    &amp; 1.213 \\
   0.022 &amp; 0.271  &amp; 1.213 &amp; 2.005   \\
   \end{array}
   \right)
   \end{split}\]</div>
</li>
<li><p>Inverse of <span class="math notranslate nohighlight">\(K\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
   K^{-1}=\left( 
   \begin{array}{cccc}
   0.953   &amp;  -0.862 &amp; 0.519 &amp; -0.208 \\
   -0.862 &amp; 1.688 &amp;-1.219 &amp; 0.519 \\
   0.519 &amp; -1.219 &amp; 1.688 &amp; -0.862 \\
   -0.208 &amp; 0.519 &amp; -0.862 &amp; 0.953   \\
   \end{array}
   \right)
   \end{split}\]</div>
</li>
<li><p>Covariance between training data inputs <span class="math notranslate nohighlight">\(x\)</span> and inputs <span class="math notranslate nohighlight">\(x_*\)</span>, for which predictions <span class="math notranslate nohighlight">\(y_*\)</span> shall be calculated (equation <a class="reference internal" href="#equation-k">(69)</a>):</p>
<div class="math notranslate nohighlight">
\[\begin{split}
   K_{*}=\left( 
   \begin{array}{cccc}
   6.7\cdot 10^{-4}  &amp;  2.22\cdot 10^{-2} &amp;  2.706\cdot 10^{-1} &amp;  1.2130 \\
   7.4\cdot 10^{-6} &amp;  6.7\cdot 10^{-4}  &amp; 2.22\cdot 10^{-2}  &amp; 2.706\cdot 10^{-1} \\
   3.1\cdot 10^{-8} &amp;  7.4\cdot 10^{-6}  &amp; 6.70\cdot 10^{-4}  &amp; 2.22\cdot 10^{-2} \\
   \end{array}
   \right)
   \end{split}\]</div>
</li>
<li><p>Covariance between different inputs <span class="math notranslate nohighlight">\(x_*\)</span>, for which predictions <span class="math notranslate nohighlight">\(y_*\)</span> shall be calculated (equation <a class="reference internal" href="#equation-k">(69)</a>):</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
 K_{**}=\left( 
 \begin{array}{ccc}
 2.005   &amp;  1.213 &amp; 0.271 \\
 1.213 &amp; 2.005    &amp; 1.213 \\
 0.271 &amp; 1.213  &amp;2.005    \\
 \end{array}
 \right)
 \end{split}\]</div>
<p>Applying equation <a class="reference internal" href="#equation-estmean">(65)</a> the estimation <span class="math notranslate nohighlight">\(\mathbf{y_*}=[y_{5},y_{6},y_{7} ]\)</span> at the inputs <span class="math notranslate nohighlight">\(\mathbf{x_*}=[5,6,7]\)</span> can be calculated to be:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{y_*}=[5.495, 8.781, 12.230]
\]</div>
<p>The standard deviation of the predictions is, according to equations <a class="reference internal" href="#equation-estvar">(66)</a> and <a class="reference internal" href="#equation-eststd">(67)</a></p>
<div class="math notranslate nohighlight">
\[
std(y_*)=[ 1.016 , 1.394 , 1.416 ]
\]</div>
<div class="figure align-center" id="gpregex">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/gaussianProcessPredictionDiscrete.png"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/gaussianProcessPredictionDiscrete.png" src="https://maucher.home.hdm-stuttgart.de/Pics/gaussianProcessPredictionDiscrete.png" style="width: 400pt;" /></a>
<p class="caption"><span class="caption-number">Fig. 26 </span><span class="caption-text">Training data (red points), predictions <span class="math notranslate nohighlight">\(y_*\)</span> (green points) and the <span class="math notranslate nohighlight">\(95\%\)</span>-confidence intervalls around the predictions (vertical bars) for this example</span><a class="headerlink" href="#gpregex" title="Permalink to this image">¶</a></p>
</div>
</div>
</div>
<div class="section" id="learn-hyperparameters-of-mean-and-covariance-function">
<h2>Learn hyperparameters of mean- and covariance-function<a class="headerlink" href="#learn-hyperparameters-of-mean-and-covariance-function" title="Permalink to this headline">¶</a></h2>
<p>Up to now we have assumed, that the mean <span class="math notranslate nohighlight">\(m(x)\)</span> and the covariance <span class="math notranslate nohighlight">\(k(x,x')\)</span> are predefined. The question where the necessary knowledge for the definition of these functions comes from has been ignored so far. In general there exists two options:</p>
<ul class="simple">
<li><p><strong>Domain Knowledge:</strong> Somebody knows the rough structure of the data (i.e. <em>increasing</em> or <em>periodic</em>) and mean and covariance are configured according to this rough structure. Note that in this way Gaussian Processes provide a comfortable way to integrate domain knowledge into supervised learning.</p></li>
<li><p><strong>Data Knowledge:</strong> If enough training data is available the hyperparameters of the mean- and covariance-function can be learned.</p></li>
</ul>
<p>In this section we consider the second option, i.e. we describe <em>How to learn the hyperparameters of <span class="math notranslate nohighlight">\(m(x)\)</span> and <span class="math notranslate nohighlight">\(k(x,x')\)</span></em>. It is important to note, that the types of the functions must also be pre-selected, only the hyperparameters of the corresponding function types are learned. For example one can assume</p>
<ul class="simple">
<li><p>a quadratic mean function of type</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
m(x)=a x^2 + b x +c
\]</div>
<ul class="simple">
<li><p>and a squared exponential covariance function of type</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
k(x,x')= \sigma_f^2 \cdot e^{- \frac{(x-x')^2}{2\ell^2}} + \sigma_n^2 \cdot \delta(x,x')
\]</div>
<p>Then the set of hyperparameters, which can be learned from data is</p>
<div class="math notranslate nohighlight">
\[
\theta = \left\{ a,b,c,\sigma_f,\sigma_n,\ell \right\}.
\]</div>
<p>In order to determine these hyperparameters a <strong>Maximum Likelihood</strong> approach is applied. This means that the hyperparameters are specified such that the probability for the given training data <span class="math notranslate nohighlight">\(\mathbf{x},\mathbf{y}\)</span></p>
<div class="math notranslate nohighlight">
\[
p(\mathbf{y}|\mathbf{x},\theta)
\]</div>
<p>is maximized. Since, the logarithmic function is monotonic increasing one can equivalently maximize</p>
<div class="math notranslate nohighlight">
\[
L=\log\left(p(\mathbf{y}|\mathbf{x},\theta\right)
\]</div>
<p>This yields a simplification of the following calculations.</p>
<p>In the case of multivariate Gaussian distributed data this logartihmic probability is</p>
<div class="math notranslate nohighlight" id="equation-loglike">
<span class="eqno">(70)<a class="headerlink" href="#equation-loglike" title="Permalink to this equation">¶</a></span>\[
L=\log\left(p(\mathbf{y}|\mathbf{x},\theta\right)=-\frac{1}{2} \log|K| - \frac{1}{2}(\mathbf{y}-\boldsymbol\mu)^T K^{-1} (\mathbf{y}-\boldsymbol\mu) - \frac{N}{2} \log(2 \pi),
\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol\mu\)</span> is the mean vector according to equation <a class="reference internal" href="#equation-kmue">(58)</a>, <span class="math notranslate nohighlight">\(K\)</span> is the covariance matrix according to equation <a class="reference internal" href="#equation-kk">(59)</a> and <span class="math notranslate nohighlight">\(N\)</span> is the number of training-instances. The hyperparameters are part of <span class="math notranslate nohighlight">\(\boldsymbol\mu\)</span> and <span class="math notranslate nohighlight">\(K\)</span>. In order to find the hyperparameters, which maximize equation <a class="reference internal" href="#equation-loglike">(70)</a> one must apply a numeric optimisation algorithm. (see e.g. <a class="reference external" href="http://docs.scipy.org/doc/scipy/reference/optimize.html">numeric optimisation with scipy</a>). The application of such a numeric optimisation process is demonstrated in the <a class="reference internal" href="GaussianProcessRegression.html"><span class="doc std std-doc">next section</span></a>. The numeric optimisation process, e.g. <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/optimize.minimize-cobyla.html">COBYLA</a>, requires</p>
<ul class="simple">
<li><p>the function, which must be minimized<a class="footnote-reference brackets" href="#fn1" id="id2">1</a> in parametric form</p></li>
<li><p>a list of start-values for all parameters</p></li>
<li><p>a list of constraints for all parameters</p></li>
</ul>
<p>In the figure below learned GP regression modells for two different types of mean-function (same training data in both cases) are shown. Note that in the areas, where no training-data is available (<span class="math notranslate nohighlight">\(x&lt;1\)</span> or <span class="math notranslate nohighlight">\(x&gt;9\)</span>) the predictions approximate the learned mean-function and the uncertainty (variance) increases in these areas.</p>
<div class="figure align-center" id="gphyppar">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/gpHyperparameterLearning.png"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/gpHyperparameterLearning.png" src="https://maucher.home.hdm-stuttgart.de/Pics/gpHyperparameterLearning.png" style="width: 600pt;" /></a>
<p class="caption"><span class="caption-number">Fig. 27 </span><span class="caption-text">Left: A static mean <span class="math notranslate nohighlight">\(m(x)=0\)</span> is assumed. Right: A quadratic mean of type <span class="math notranslate nohighlight">\(m(x)=a x^2 + b x +c\)</span> is assumed. In both cases a covariance function of type <em>squared exponential</em> has been assumed. The plot show the learned hyperparameters and the resulting GP regression models for both cases.</span><a class="headerlink" href="#gphyppar" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="final-remarks-on-the-covariance-function">
<h2>Final remarks on the covariance function<a class="headerlink" href="#final-remarks-on-the-covariance-function" title="Permalink to this headline">¶</a></h2>
<p>In the examples above covariance functions of type <em>squared exponential</em> have been applied. A crucial hyperparameter for this type of function is the horizontal length-scale <span class="math notranslate nohighlight">\(\ell\)</span>. The figure below demonstrates the influence of this parameter.</p>
<div class="figure align-center" id="lengthscale">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/04NoisySinNoTrainZeroMeanVaryTheta.png"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/04NoisySinNoTrainZeroMeanVaryTheta.png" src="https://maucher.home.hdm-stuttgart.de/Pics/04NoisySinNoTrainZeroMeanVaryTheta.png" style="width: 600pt;" /></a>
<p class="caption"><span class="caption-number">Fig. 28 </span><span class="caption-text">Decreasing horizontal length-scale <span class="math notranslate nohighlight">\(\ell\)</span> from top left to bottom right</span><a class="headerlink" href="#lengthscale" title="Permalink to this image">¶</a></p>
</div>
<p>As can be seen a decreasing length-scale <span class="math notranslate nohighlight">\(\ell\)</span> yields</p>
<ul class="simple">
<li><p>an increasing variance (uncertainty) in the ranges of no data</p></li>
<li><p>a faster approximation of the predictions to the mean-function</p></li>
</ul>
<p>Even though the <em>squared exponential</em> is the most common covariance function, it does not fit in all cases. The <em>sqaured exponential</em> implies that correlation between neighbouring instances decreases with increasing distance between these instances. But this is not true for all types of data, as the figure below demonstrates. In both plots data has a long-term and a short-term-behaviour. In such cases more complex types of covariance functions are required.</p>
<div class="figure align-center" id="periodic">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/gaussianProcessExamplesEbden.PNG"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/gaussianProcessExamplesEbden.PNG" src="https://maucher.home.hdm-stuttgart.de/Pics/gaussianProcessExamplesEbden.PNG" style="width: 600pt;" /></a>
</div>
<p>A covariance functions, which may be suitable for the data in the left plot of the figure above is</p>
<div class="math notranslate nohighlight">
\[
k(x,x')= \sigma_{f1}^2 \cdot e^{- \frac{(x-x')^2}{2\ell_1^2}} + \sigma_{f2}^2 \cdot e^{- \frac{(x-x')^2}{2\ell_2^2}} +\sigma_n^2 \delta(x,x').
\]</div>
<p>If <span class="math notranslate nohighlight">\(\ell_2\)</span> has a significatnt higher value than <span class="math notranslate nohighlight">\(\ell_1\)</span>, then the second term models long-term-correlations and the first term models short-term correlations between data.</p>
<p>A covariance functions, which may be suitable for the data in the right plot of the figure above is</p>
<div class="math notranslate nohighlight">
\[
k(x,x')= \sigma_{f}^2 \cdot e^{- \frac{(x-x')^2}{2\ell^2}} + e^{-2 \sin^2(\nu \pi(x-x'))} +\sigma_n^2 \delta(x,x').
\]</div>
<p>Here the long-term behaviour is modelled by the first term and the periodic short-term behaviour is modelled by the second term.</p>
<p>In general arbitrary covariance-functions can be configured, as long as the corresponding covariance matrix <span class="math notranslate nohighlight">\(K\)</span> is <a class="reference external" href="https://en.wikipedia.org/wiki/Definite_matrix">positive definite</a>.</p>
<p>For a deeper dive into Gaussian processes the tutorial <span id="id3">[<a class="reference internal" href="../referenceSection.html#id3">Ebd15</a>]</span> and the book <span id="id4">[<a class="reference internal" href="../referenceSection.html#id4">RW</a>]</span> are recommended.</p>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="fn1"><span class="brackets"><a class="fn-backref" href="#id2">1</a></span></dt>
<dd><p>In scipy all optimisation algorithms are implemented sucht that they find minimas. Maximisation of equation <a class="reference internal" href="#equation-loglike">(70)</a> is the same as minimisation of the negative of this function. I.e. one minimzes <span class="math notranslate nohighlight">\(-L\)</span>.</p>
</dd>
</dl>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./machinelearning"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="svm.html" title="previous page">Support Vector Machines (SVM)</a>
    <a class='right-next' id="next-link" href="GaussianProcessRegression.html" title="next page">Gaussian Process: Implementation in Python</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Prof. Dr. Johannes Maucher<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>