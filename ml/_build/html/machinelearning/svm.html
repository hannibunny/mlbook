
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Support Vector Machines (SVM) &#8212; Machine Learning Lecture</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=62ba249389abaaa9ffc34bf36a076bdc1d65ee18" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=f31d14ad54b65d19161ba51d4ffff3a77ae00456"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Gaussian Process" href="gp.html" />
    <link rel="prev" title="Linear Classification" href="LinearClassification.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/hdmlogomed.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Machine Learning Lecture</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Intro and Overview Machine Learning Lecture
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../00BasicConcepts.html">
   Basic Concepts of Data Mining and Machine Learning
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Conventional ML
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="knn.html">
   K - Nearest Neighbour Classification / Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="parametricClassification1D.html">
   Bayes- and Naive Bayes Classifier
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="LinReg.html">
   Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="heartRateRegression.html">
   Example Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="LinearClassification.html">
   Linear Classification
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Support Vector Machines (SVM)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="gp.html">
   Gaussian Process
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="GaussianProcessRegression.html">
   Gaussian Process: Implementation in Python
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Neural Networks
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../neuralnetworks/01NeuralNets.html">
   Neural Networks Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../neuralnetworks/02RecurrentNeuralNetworks.html">
   Recurrent Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../neuralnetworks/03ConvolutionNeuralNetworks.html">
   Convolutional Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../neuralnetworks/convolutionDemos.html">
   Animations of Convolution and Deconvolution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../neuralnetworks/03KerasMLPandCNNcifar.html">
   Implementing Neural Networks with Keras
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../neuralnetworks/04KerasPretrainedClassifiers.html">
   Applying Pretrained Deep Neural Networks for Image Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../neuralnetworks/05KerasPretrainedCovid.html">
   Apply Pretrained Neural Networks on new Task
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Autoencoder
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../neuralnetworks/04VariationalAutoencoder.html">
   Variational Autoencoder (VAE) to generate handwritten digits
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  GAN
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../gan/GAN.html">
   Generative Adversarial Nets (GAN)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../gan/DCGAN.html">
   DCGAN Keras Implementation
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Diffusion Models
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../diffusion/denoisingDiffusion.html">
   Difussion Models
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Reinforcement Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../rl/reinforcement.html">
   Reinforcement Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../rl/DQN.html">
   Deep Reinforcement Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../rl/QLearnFrozenLake.html">
   Example Q-Learning
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Text
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../text/01ModellingWordsAndTexts.html">
   Representations for Words and Texts
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../text/02TextClassification.html">
   Text classification with CNNs and LSTMs
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Graph Neural Networks
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../neuralnetworks/GraphNeuralNetworks.html">
   Graph Neural Networks (GNN)
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Transformer
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../transformer/attention.html">
   Sequence-To-Sequence, Attention, Transformer
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../transformer/intent_classification_with_bert.html">
   Intent Classification with BERT
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../referenceSection.html">
   References
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/machinelearning/svm.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#svms-for-classification">
   SVMs for Classification
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#finding-robust-linear-discriminants">
     Finding Robust Linear Discriminants
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#dual-form">
       Dual Form
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#inference">
       Inference
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#training-data-can-not-be-separated-error-free">
       Training-Data can not be separated error-free
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#non-linear-svm-classification">
     Non-linear SVM classification
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#kernel-trick">
       Kernel Trick
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#linear-kernel">
         Linear Kernel
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#polynomial-kernel">
         Polynomial Kernel
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#rbf-kernel">
         RBF Kernel
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#svm-for-regression-svr">
   SVM for Regression (SVR)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#recap-linear-regression">
     Recap Linear Regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optimisation-objective-in-svr-training">
     Optimisation Objective in SVR training
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#influence-of-the-hyperparameters">
     Influence of the hyperparameters
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Support Vector Machines (SVM)</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#svms-for-classification">
   SVMs for Classification
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#finding-robust-linear-discriminants">
     Finding Robust Linear Discriminants
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#dual-form">
       Dual Form
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#inference">
       Inference
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#training-data-can-not-be-separated-error-free">
       Training-Data can not be separated error-free
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#non-linear-svm-classification">
     Non-linear SVM classification
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#kernel-trick">
       Kernel Trick
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#linear-kernel">
         Linear Kernel
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#polynomial-kernel">
         Polynomial Kernel
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#rbf-kernel">
         RBF Kernel
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#svm-for-regression-svr">
   SVM for Regression (SVR)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#recap-linear-regression">
     Recap Linear Regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optimisation-objective-in-svr-training">
     Optimisation Objective in SVR training
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#influence-of-the-hyperparameters">
     Influence of the hyperparameters
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="support-vector-machines-svm">
<h1>Support Vector Machines (SVM)<a class="headerlink" href="#support-vector-machines-svm" title="Permalink to this headline">#</a></h1>
<p>Support Vector Machines (SVM) have been introduced in <span id="id1">[<a class="reference internal" href="../referenceSection.html#id8" title="C. Cortes and V. Vapnik. Support vector networks. Machine Learning, 20:273-297, 1995.">CV95</a>]</span>. They can be applied for supervised machine learning, both for classification and regression tasks. Depending on the selected hyperparameter <code class="docutils literal notranslate"><span class="pre">kernel</span></code> (linear, polynomial or rbf), SVMs can learn linear or non-linear models. Other positive features of SVMs are</p>
<ul class="simple">
<li><p>many different types of functions can be learned by SVMs, i.e. they have a low bias (for non-linear kernels)</p></li>
<li><p>they scale good with high-dimensional data</p></li>
<li><p>only a small set of hyperparameters must be configured</p></li>
<li><p>overfitting/generalisation can easily be controlled, regularisation is inherent</p></li>
</ul>
<div class="section" id="svms-for-classification">
<h2>SVMs for Classification<a class="headerlink" href="#svms-for-classification" title="Permalink to this headline">#</a></h2>
<p>SVMs learn class boundaries, which discriminate a pair of classes. In the case of non-binary classification with <span class="math notranslate nohighlight">\(K\)</span> classes, <span class="math notranslate nohighlight">\(K\)</span> class boundaries are learned. Each of which discriminates one class from all others. In any case, the learned class-boundaries are linear hyperplanes.</p>
<ul class="simple">
<li><p>In the case of a <strong>linear kernel</strong>, each learned (d-1)-hyperplane discriminates the d-dimensional space into two subspaces. <span class="math notranslate nohighlight">\(d\)</span> is the dimesionality of the original space, i.e. the number of components in the input-vector <span class="math notranslate nohighlight">\(x\)</span>.</p></li>
<li><p>In the case of a <strong>non-linear kernel</strong>, the original d-dimensional input data is virtually transformed into a higher-dimensional space with <span class="math notranslate nohighlight">\(m &gt; d\)</span> dimensions. In this m-dimensional space training data is hopefully linear-separable. The learnd (m-1)-dimensional hyperplane linearly discriminates the m-dimensional space. But the back-transformation of this hyperplane is a non-linear discrimator in the original d-dimensional space.</p></li>
</ul>
<p>The picture below sketches data, which is not linear-separable in the original 2-dimensional space. However, there exists a transformation into a 3-dimensional space, in which the given training data can be discriminated by a 2-dimensional hyperplane.</p>
<div class="figure align-center" id="transformhigh">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/svmtransform.jpg"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/svmtransform.jpg" src="https://maucher.home.hdm-stuttgart.de/Pics/svmtransform.jpg" style="width: 600pt;" /></a>
<p class="caption"><span class="caption-number">Fig. 2 </span><span class="caption-text">Data, which is not linear-separable in the original 2-dimensional space may be linear separable in a higher dimensional space.</span><a class="headerlink" href="#transformhigh" title="Permalink to this image">#</a></p>
</div>
<p>Another important property of SVM classifiers is that <strong>they find good class-boundaries</strong>. In order to understand what is meant by <em>good class-boundary</em> take a look at the picture below. The 4 subplots contain the same training-data but four different class-boundaries, each of which discriminates the given training data error-free. The question is <em>which of these discriminantes is the best?</em> The discriminantes in the right column are not robust, because in some regions the datapoints are quite close to the boundary. The discriminant in the top left subplot is the most robust, i.e. the one which generalizes best, because the training-data-free range around the discriminant is maximal. <strong>A SVM classifier actually finds such robust class-boundaries by maximizing the training-data-free range around the discriminant.</strong></p>
<div class="figure align-center" id="id3">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/bestdisk.jpg"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/bestdisk.jpg" src="https://maucher.home.hdm-stuttgart.de/Pics/bestdisk.jpg" style="width: 600pt;" /></a>
<p class="caption"><span class="caption-number">Fig. 3 </span><span class="caption-text">The discriminant in the top left subplot is the most robust one, because it has a maximal training-data-free range around it.</span><a class="headerlink" href="#id3" title="Permalink to this image">#</a></p>
</div>
<div class="section" id="finding-robust-linear-discriminants">
<h3>Finding Robust Linear Discriminants<a class="headerlink" href="#finding-robust-linear-discriminants" title="Permalink to this headline">#</a></h3>
<p>As mentioned above, SVM classifiers find robust discriminants in the sense that the discriminant is determined such that it not only separates the given training-data well, but also has a maximal training-data free range around it. In this subsection it is shown, how such discriminantes are learned from data. To illustrate this we apply the example depicted below. We have a set of 8 training-instances, partitioned into 2 classes. The task is to find a robust discriminant with the properties mentioned above.</p>
<div class="figure align-center" id="id4">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/svmExampleIntro.png"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/svmExampleIntro.png" src="https://maucher.home.hdm-stuttgart.de/Pics/svmExampleIntro.png" style="width: 600pt;" /></a>
<p class="caption"><span class="caption-number">Fig. 4 </span><span class="caption-text">Example: Learn a good discriminant from the set of 8 training-instances.</span><a class="headerlink" href="#id4" title="Permalink to this image">#</a></p>
</div>
<p>As ususal in supervised machine learning, we start from a set of <span class="math notranslate nohighlight">\(N\)</span> labeled training instances</p>
<div class="math notranslate nohighlight">
\[ 
T=\{\mathbf{x}_p,r_p\}_{p=1}^N,
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{x}_p\)</span> is the <a class="reference external" href="http://p.th">p.th</a> input-vector and <span class="math notranslate nohighlight">\(r_p\)</span> is the corresponding label. In the context of binary SVMs the label-values are:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(r_p=-1\)</span>, if <span class="math notranslate nohighlight">\(\mathbf{x}_p \in C_1\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(r_p=+1\)</span>, if <span class="math notranslate nohighlight">\(\mathbf{x}_p \in C_2\)</span></p></li>
</ul>
<p>The <strong>training goal</strong> is: Determine the weights <span class="math notranslate nohighlight">\(\mathbf{w}=(w_1,\ldots,w_d)\)</span> and <span class="math notranslate nohighlight">\(w_0\)</span>, such that</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{w}^T \mathbf{x_p} + w_0 \geq +1 &amp; \mbox{ for } &amp; r_p=+1 \nonumber \\
\mathbf{w}^T \mathbf{x_p} + w_0 \leq -1 &amp; \mbox{ for } &amp; r_p=-1 \nonumber 
\label{eq:k2svmklass}
\end{split}\]</div>
<p>This goal can equivalently be formulated by imposing the following condition</p>
<div class="math notranslate nohighlight" id="equation-condregion">
<span class="eqno">(24)<a class="headerlink" href="#equation-condregion" title="Permalink to this equation">#</a></span>\[
r_p (\mathbf{w}^T \mathbf{x_p} + w_0 ) \geq 1
\]</div>
<p>to be fullfilled for all training-instances. Note, that this condition defines a <strong>boundary area</strong>, rather than just a <strong>boundary line</strong>, as in other algorithms, where the condition, that must be fullfilled is:</p>
<div class="math notranslate nohighlight" id="equation-condline">
<span class="eqno">(25)<a class="headerlink" href="#equation-condline" title="Permalink to this equation">#</a></span>\[
r_p (\mathbf{w}^T \mathbf{x_p} + w_0 ) \geq 0
\]</div>
<p>The difference between the two conditions is visualized in the following picture. The plot on the left refers to condition <a class="reference internal" href="#equation-condline">(25)</a>. With this condition a discriminant is learned such that it separates the training-instances of the two classes. The plot on the right refers to condition <a class="reference internal" href="#equation-condregion">(24)</a>. Here, the discriminant is learned such that the two classes are separated and the training-data-free range around the discriminant is as large as possible. The discriminant learned in this way generalizes better (less overfitting). The vectors, which ly on the region-boundary are called <strong>support vectors</strong>.</p>
<div class="figure align-center" id="id5">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/svmcomb.jpg"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/svmcomb.jpg" src="https://maucher.home.hdm-stuttgart.de/Pics/svmcomb.jpg" style="width: 600pt;" /></a>
<p class="caption"><span class="caption-number">Fig. 5 </span><span class="caption-text">Left: Other linear classification algorithms learn discriminantes, which separate the classes with as less as possible errors. Right: Discriminantes learned by SVMs have the additional property, that the training-data-free range around them is maximized.</span><a class="headerlink" href="#id5" title="Permalink to this image">#</a></p>
</div>
<p>The distance of training-instance <span class="math notranslate nohighlight">\(\mathbf{x}_p\)</span> to the discriminant is:</p>
<div class="math notranslate nohighlight">
\[
\frac{|\mathbf{w}^T\mathbf{x_p}+w_0|}{||\mathbf{w}||} = \frac{r_p(\mathbf{w}^T\mathbf{x}_p+w_0)}{||\mathbf{w}||}, \quad \mbox{ where } ||\mathbf{w}|| = \sqrt{\sum\limits_{i=1}^d w_i^2}
\]</div>
<p>The SVM training goal is to find a discrimante, i.e. parameters <span class="math notranslate nohighlight">\(\mathbf{w}\)</span>, such that the minimum distance between a training-instance and the discriminante is maximal. Thus <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> must be determined such that the value <span class="math notranslate nohighlight">\(\rho\)</span> with</p>
<div class="math notranslate nohighlight">
\[
\frac{r_p(\mathbf{w}^T\mathbf{x}_p+w_0)}{||\mathbf{w}||}\geq \rho, \quad \forall p.
\]</div>
<p>is maximal. Since there exists infinite many combinations of weights <span class="math notranslate nohighlight">\(w_i\)</span>, which define the same hyperplane, one can impose an additional condition on these weights. This additional condition is</p>
<div class="math notranslate nohighlight">
\[
\rho ||\mathbf{w}|| = 1.
\]</div>
<p>This condition implies, that <span class="math notranslate nohighlight">\(||\mathbf{w}||\)</span> must be minimized in order to maximize the distance <span class="math notranslate nohighlight">\(\rho\)</span>. We find the minimal weights and therefore the maximal <span class="math notranslate nohighlight">\(\rho\)</span> by minimizing</p>
<div class="math notranslate nohighlight" id="equation-optw">
<span class="eqno">(26)<a class="headerlink" href="#equation-optw" title="Permalink to this equation">#</a></span>\[
\frac{1}{2}||\mathbf{w}||^2
\]</div>
<p>under the constraints</p>
<div class="math notranslate nohighlight" id="equation-optwr">
<span class="eqno">(27)<a class="headerlink" href="#equation-optwr" title="Permalink to this equation">#</a></span>\[
r_p(\mathbf{w}^T\mathbf{x}_p+w_0)\geq 1, \quad \forall p
\]</div>
<p>This is a standard quadratic optimisation problem with constraints. The complexity of the numeric solution of this problem is proportional to the number of dimensions in the given space. A possible optimisation method is <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_cobyla.html">Constraint BY Linear Approximation</a>.</p>
<p>The code below shows the implementation of this example and the calculation of the discriminant from the given training data. The optimisation problem is solved by <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_cobyla.html">Constraint BY Linear Approximation</a>.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>from scipy.optimize import fmin_cobyla
from matplotlib import pyplot as plt
import numpy as np

#Define points and the corresponding class labels###########################
p=[[3,2],[1,4],[2,4],[0.5,4.8],[3,5],[5,4],[3.5,5.5],[5.7,3]]
c=[-1,-1,-1,-1,1,1,1,1]
#Define class which returns the constraints functions#######################
class Constraint:
    def __init__(self, points,classes):
        self.p = points
        self.c =classes
    def __len__(self):
        return len(self.p)
    def __getitem__(self, i):
        def c(x):
            return self.c[i]*(x[0]*1+x[1]*self.p[i][0]+x[2]*self.p[i][1])-1
        return c
#Define the function that shall be minimized################################
def objective(x):
    return 0.5*(x[1]**2+x[2]**2)
#Create a list of all constraints using the class defined above#############
const=Constraint(p,c)
cl=[const.__getitem__(i) for i in range(len(c))]
#Call the scipy optimization method#########################################
res = fmin_cobyla(objective,[1.0,1.0,1.0],cl)
print &quot;Found weights of the optimal discriminant:    &quot;,res
</pre></div>
</div>
<p>The figure below visualizes the discriminant, as learned in the code-cell above.</p>
<div class="figure align-center" id="id6">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/nonlinsep8pointsDiscriminant.png"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/nonlinsep8pointsDiscriminant.png" src="https://maucher.home.hdm-stuttgart.de/Pics/nonlinsep8pointsDiscriminant.png" style="width: 500pt;" /></a>
<p class="caption"><span class="caption-number">Fig. 6 </span><span class="caption-text">The learned discriminant is characterised by having a maximum training-data free margin around it.</span><a class="headerlink" href="#id6" title="Permalink to this image">#</a></p>
</div>
<p>The complexity of the numeric solution of the quadratic minimization problem with constraints increases strongly with the dimension the underlying space. This is a problem for high-dimensional data, such as text and images. However, even if the input data contains only a moderate number of features, the space in which the optimisation problem must be solved can be extremly high-dimensional. This is because non-linear SVMs transform the given data in a high-dimensional space, where it is hopefully linear separable (as mentioned above).</p>
<p>This drawback of dimension-dependent complexity can be circumvented by transforming the optimisation problem into it’s <strong>dual form</strong> and solving this dual optimisation problem. The complexity of solving the dual form scales with the number of training-instances <span class="math notranslate nohighlight">\(N\)</span>, but not with the dimensionality. Another crucial advantage of the dual form is that it allows the application of the <strong>kernel trick</strong> (see below).</p>
<div class="section" id="dual-form">
<h4>Dual Form<a class="headerlink" href="#dual-form" title="Permalink to this headline">#</a></h4>
<p>A function <span class="math notranslate nohighlight">\(f(x)\)</span>, such as <a class="reference internal" href="#equation-optw">(26)</a>, that shall be minimized w.r.t. a set of <span class="math notranslate nohighlight">\(N\)</span> constraints, such as <a class="reference internal" href="#equation-optwr">(27)</a>, can always be formulated as an optimisation problems without constraints as follows:</p>
<ol>
<li><p>reformulate all constraints to a form <span class="math notranslate nohighlight">\(c_p(x) \geq 0\)</span></p></li>
<li><p>the optimisation-problem without constraints is then: Minimize</p>
<div class="math notranslate nohighlight">
\[
	L=f(x)-\sum\limits_{p=1}^N\alpha_p \cdot c_p(x),
	\]</div>
<p>with positive-valued <strong>Lagrange Coefficients <span class="math notranslate nohighlight">\(\alpha_p\)</span></strong>.</p>
</li>
</ol>
<p>The given optimization problem defined by  <a class="reference internal" href="#equation-optw">(26)</a> and  <a class="reference internal" href="#equation-optwr">(27)</a> can then be reformulated as follows: Minimize</p>
<div class="math notranslate nohighlight" id="equation-lagrangeorig">
<span class="eqno">(28)<a class="headerlink" href="#equation-lagrangeorig" title="Permalink to this equation">#</a></span>\[
  L = \frac{1}{2}||\mathbf{w}||^2 \, - \, \sum\limits_{p=1}^N \alpha_p \left(r_p(\mathbf{w}^T\mathbf{x}_p+w_0)-1\right),
\]</div>
<p>For this representation without constraints the <strong>partial derivates</strong> w.r.t. all parameters <span class="math notranslate nohighlight">\(w_i\)</span> are determined:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{\partial L}{\partial w_i} &amp; = &amp; w_i - \, \sum\limits_{p=1}^N \alpha_p r_p x_{p,i} \quad \mbox{  for  } i=1,\ldots,d \\
\frac{\partial L}{\partial w_0} &amp; = &amp;  - \sum\limits_{p=1}^N \alpha_p r_p 
\end{split}\]</div>
<p>At the location of the minimum all of these partial derivatives must be 0. Setting all these equations to 0 and resolving them, such that the <span class="math notranslate nohighlight">\(w_i\)</span> are in isolated form on the left side of the equations yields</p>
<div class="math notranslate nohighlight">
\[\begin{split}	 
	\mathbf{w} &amp; = &amp; \sum\limits_{p=1}^N \alpha_p r_p \mathbf{x}_{p} \\
	0 &amp; = &amp; \sum\limits_{p=1}^N \alpha_p r_p 
\end{split}\]</div>
<p>The <strong>dual form</strong> can then be obtained by inserting these equations for <span class="math notranslate nohighlight">\(w_i\)</span> into equation <a class="reference internal" href="#equation-lagrangeorig">(28)</a>. This dual form is:</p>
<p>Maximize</p>
<div class="math notranslate nohighlight" id="equation-eq-dualopt">
<span class="eqno">(29)<a class="headerlink" href="#equation-eq-dualopt" title="Permalink to this equation">#</a></span>\[ 
L_d=-\frac{1}{2}\sum\limits_{p=1}^N \sum\limits_{s=1}^N \left( \alpha_p \alpha_s r_p r_s \mathbf{x}_p^T \mathbf{x}_s \right)+\sum\limits_{p=1}^N \alpha_p 
\]</div>
<p>w.r.t. the Lagrange-Coefficients <span class="math notranslate nohighlight">\(\alpha_p\)</span> under the restriction</p>
<div class="math notranslate nohighlight" id="equation-eq-dualoptr">
<span class="eqno">(30)<a class="headerlink" href="#equation-eq-dualoptr" title="Permalink to this equation">#</a></span>\[
\sum\limits_{p=1}^N \alpha_p r_p = 0 \quad \mbox{and} \quad \alpha_p \geq 0 \quad \forall p
\]</div>
<p>This dual form can be solved by numeric algorithms for quadratic optimization. The solution reveals, that almost all of the <span class="math notranslate nohighlight">\(N\)</span> Lagrange-Coefficients <span class="math notranslate nohighlight">\(\alpha_p\)</span> are 0. The training instances <span class="math notranslate nohighlight">\(\mathbf{x}_p\)</span>, for which <span class="math notranslate nohighlight">\(\alpha_p&gt;0\)</span> are called <strong>Support Vectors</strong>.</p>
<p>From the <span class="math notranslate nohighlight">\(\alpha_p&gt;0\)</span> and the Support Vectors, the parameters <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> can be determined as follows:</p>
<div class="math notranslate nohighlight" id="equation-sumofsupport">
<span class="eqno">(31)<a class="headerlink" href="#equation-sumofsupport" title="Permalink to this equation">#</a></span>\[
\mathbf{w}=\sum\limits_{p=1}^N \alpha_p r_p \mathbf{x}_p
\]</div>
<p>This sum depends only on the Support Vectors. Note that <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> doesn’t contain <span class="math notranslate nohighlight">\(w_0\)</span>. In order to determine this remaining parameter we can exploit the property of Support Vectors to lie exactly on the boundary of the region around the discriminant. This means that for all Support Vectors we have:</p>
<div class="math notranslate nohighlight">
\[
r_p(\mathbf{w}^T\mathbf{x}_p+w_0) = 1.
\]</div>
<p>Since we already know <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> and we also know the Support Vectors, <span class="math notranslate nohighlight">\(w_0\)</span> can be calculated from this equation. Depending on which Support Vector is inserted in the equation above, the resulting <span class="math notranslate nohighlight">\(w_0\)</span> may vary. It is recommended to calculate for each Support Vector the corresponding <span class="math notranslate nohighlight">\(w_0\)</span> and choose the mean over all this values to be the final <span class="math notranslate nohighlight">\(w_0\)</span>. Together, <span class="math notranslate nohighlight">\(w_0\)</span> and <span class="math notranslate nohighlight">\(\mathbf{w}\)</span>, define the discriminant, which is called <strong>Supported Vector Machine</strong>.</p>
<p>As already mentioned above, in the case of non-binary classification <span class="math notranslate nohighlight">\(K\)</span> discriminantes must be learned. Each of which discriminates one class from all others.</p>
</div>
<div class="section" id="inference">
<h4>Inference<a class="headerlink" href="#inference" title="Permalink to this headline">#</a></h4>
<p>Once the Support Vector Machine is trained, for a new input vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> the discriminant-equation</p>
<div class="math notranslate nohighlight">
\[
g(\mathbf{x})=\mathbf{w}^T \mathbf{x} +w_0
\]</div>
<p>can be calculated. If the result is <span class="math notranslate nohighlight">\(&lt; 0\)</span> the input <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is assigned to class <span class="math notranslate nohighlight">\(C_0\)</span>, otherwise to <span class="math notranslate nohighlight">\(C_1\)</span>. For non-binary classification, the <span class="math notranslate nohighlight">\(K\)</span> discriminant-equations</p>
<div class="math notranslate nohighlight">
\[
g_j(\mathbf{x})=\mathbf{w}_j^T \mathbf{x} +w_{0,j}
\]</div>
<p>are evaluated, and the class for which the resulting value is maximal is selected.</p>
</div>
<div class="section" id="training-data-can-not-be-separated-error-free">
<h4>Training-Data can not be separated error-free<a class="headerlink" href="#training-data-can-not-be-separated-error-free" title="Permalink to this headline">#</a></h4>
<p>In the entire description above, it was assumed, that the given training-data can be separated by a linear discriminant in an error-free manner. I.e. a discriminant can be found, such that all training-data of one class lies on one side of the discriminant and all training data of the other class lies on the other side of the discriminant. This assumption usually does not hold for real-world datasets. In this section SVM training is described for the general case, where training data is not linearly separable, as depicted in the image below. Again, we like to find a good discrimator for the given training-data:</p>
<div class="figure align-center" id="id7">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/svmExampleNonSep.png"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/svmExampleNonSep.png" src="https://maucher.home.hdm-stuttgart.de/Pics/svmExampleNonSep.png" style="width: 600pt;" /></a>
<p class="caption"><span class="caption-number">Fig. 7 </span><span class="caption-text">Example: Learn a good linear discriminant from the set of 8 training-instances. Now training-data of the two classes is not linearly separable.</span><a class="headerlink" href="#id7" title="Permalink to this image">#</a></p>
</div>
<p>If training-data is not linear-separabel, the goal is to determine the boundary-region, which yields a minimum amount of errors on the training-data. For this to each training-instance <span class="math notranslate nohighlight">\(\mathbf{x}_p\)</span> an error-margin <span class="math notranslate nohighlight">\(\zeta_p\)</span> is assigned. The error-margin <span class="math notranslate nohighlight">\(\zeta_p\)</span> is</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\zeta_p = 0\)</span> if <span class="math notranslate nohighlight">\(\mathbf{x}_p\)</span> lies on the correct side of the discriminant and outside the boundary region</p></li>
<li><p><span class="math notranslate nohighlight">\(\zeta_p \in \left[ 0,1 \right]\)</span>, if <span class="math notranslate nohighlight">\(\mathbf{x}_p\)</span> lies on the correct side of the discriminant but inside the boundary region</p></li>
<li><p><span class="math notranslate nohighlight">\(\zeta_p &gt;1\)</span> if <span class="math notranslate nohighlight">\(\mathbf{x}_p\)</span> lies on the wrong side of the discriminant</p></li>
</ul>
<p>The so called <strong>Soft Error</strong> is then the sum over all error-margins</p>
<div class="math notranslate nohighlight">
\[
\sum\limits_{p=1}^N \zeta_p
\]</div>
<p>With this, the optimisation problem can now be reformulated to minimize</p>
<div class="math notranslate nohighlight" id="equation-minsofterror">
<span class="eqno">(32)<a class="headerlink" href="#equation-minsofterror" title="Permalink to this equation">#</a></span>\[
\min(\frac{1}{2}||\mathbf{w}||^2 + C\sum\limits_{p=1}^N \zeta_p  )
\]</div>
<p>under the restriction</p>
<div class="math notranslate nohighlight">
\[ 
r_p(\mathbf{w}^T\mathbf{x}_p+w_0)\geq 1 - \zeta_p, \quad \forall p
\]</div>
<p>with <span class="math notranslate nohighlight">\(\zeta_p \geq 0\)</span></p>
<p>The corresponding <strong>dual form</strong> is: Maximize</p>
<div class="math notranslate nohighlight" id="equation-linmax">
<span class="eqno">(33)<a class="headerlink" href="#equation-linmax" title="Permalink to this equation">#</a></span>\[	 
L_d=-\frac{1}{2}\sum\limits_{p=1}^N \sum\limits_{s=1}^N \left( \alpha_p \alpha_s r_p r_s \mathbf{x}_p^T \mathbf{x}_s \right)+\sum\limits_{p=1}^N \alpha_p
\]</div>
<p>w.r.t. the Lagrange-Coefficients <span class="math notranslate nohighlight">\(\alpha_p\)</span> under the restriction</p>
<div class="math notranslate nohighlight">
\[	 
\sum\limits_{p=1}^N \alpha_p r_p = 0 \quad \mbox{und} \quad  0 \leq \alpha_p \leq C \quad \forall p.
\]</div>
<p>The parameter <span class="math notranslate nohighlight">\(C\)</span> in equation <a class="reference internal" href="#equation-minsofterror">(32)</a> is an important hyperparameter, which can be configured to control overfitting. A large value for <span class="math notranslate nohighlight">\(C\)</span> means that in the minimization process the minimization of the soft-error is more important than the minimization of the weights <span class="math notranslate nohighlight">\(||\mathbf{w}||^2\)</span> (i.e. the regularisation). On the other hand, a small value for <span class="math notranslate nohighlight">\(C\)</span> yields a discriminant with a wider margin around it and therefore a better generalizing model. This is visualized in the two plots of the figure below.</p>
<div class="figure align-center" id="id8">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/svmsoftmarg.jpg"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/svmsoftmarg.jpg" src="https://maucher.home.hdm-stuttgart.de/Pics/svmsoftmarg.jpg" style="width: 600pt;" /></a>
<p class="caption"><span class="caption-number">Fig. 8 </span><span class="caption-text">Left: A high value for C implies that in the minimization process the minimization of the soft-error is more important than the maximisation of the margin around the discriminant. This yields a model, which is stronger fitted to the trainng-data than the model on the right hand side. Here a smaller value of C yields more regularisation and a thus a smaller risk of overfitting.</span><a class="headerlink" href="#id8" title="Permalink to this image">#</a></p>
</div>
<p>Again, the training instances <span class="math notranslate nohighlight">\(\mathbf{x}_p\)</span>, for which <span class="math notranslate nohighlight">\(\alpha_p&gt;0\)</span> are called <strong>Support Vectors</strong>. They lie either on the boundary of the margin around the discriminant or inside this margin.</p>
</div>
</div>
<div class="section" id="non-linear-svm-classification">
<h3>Non-linear SVM classification<a class="headerlink" href="#non-linear-svm-classification" title="Permalink to this headline">#</a></h3>
<p>SVMs, as described in the previous subsections, find robust linear discriminants. These SVMs are said to have a <em>linear kernel</em>. In particular,the scalar product <span class="math notranslate nohighlight">\(\mathbf{x}_p^T \mathbf{x}_s\)</span> in equation <a class="reference internal" href="#equation-linmax">(33)</a> constitutes the linear kernel. In the sequel non-linear kernels will be introduced. The idea is to transform the inputs <span class="math notranslate nohighlight">\(\mathbf{x}_p\)</span> into a higher-dimensional space, where this input data is linear-separable. This idea has already been sketched in the <a class="reference internal" href="#transformhigh"><span class="std std-ref">figure above</span></a>. In the figure below a concrete transformation is shown.</p>
<div class="figure align-center" id="id9">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/svmTransformedData.png"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/svmTransformedData.png" src="https://maucher.home.hdm-stuttgart.de/Pics/svmTransformedData.png" style="width: 800pt;" /></a>
<p class="caption"><span class="caption-number">Fig. 9 </span><span class="caption-text">Left: Original 2-dimensional space. As can be seen the two classes are not linearly separable in this space. Right: After a non-linear transformation into a new space, in this case also a 2-dimensional space, data is linearly separable. A SVM with a non-linear kernel, learns a linear discriminant into a new space, which corrsponds to a non-linear class-boundary in the original space. The transformation applied here is defined by <span class="math notranslate nohighlight">\(z_1 = \Phi_1(x_1)=x_1^3\)</span> and <span class="math notranslate nohighlight">\(z_2 = \Phi_2(x_2)=x_2\)</span></span><a class="headerlink" href="#id9" title="Permalink to this image">#</a></p>
</div>
<p>However, as we will see later, the transformation to a higher-dimensional space need not be performed explicitely, because we can apply a <em>kernel-trick</em>. This kernel-trick yields the same result as the one we will get, if we would actually transform the data in a higher-dimensional space.</p>
<div class="admonition-general-notation admonition">
<p class="admonition-title">General Notation</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(X\)</span> is the d-dimensional original space in which the input-vectors  <span class="math notranslate nohighlight">\(\mathbf{x}_p\)</span> lie.</p></li>
<li><p><span class="math notranslate nohighlight">\(Z\)</span> is the r-dimensional high-dimensional space (<span class="math notranslate nohighlight">\(r&gt;d\)</span>), which contains the transformed input-vectors  <span class="math notranslate nohighlight">\(\mathbf{z}_p\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\Phi: X \rightarrow Z\)</span> is the transformation from the original space <span class="math notranslate nohighlight">\(X\)</span> to the high-dimensional space <span class="math notranslate nohighlight">\(Z\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{z}_p = \Phi(\mathbf{x}_p)\)</span> is the representation of <span class="math notranslate nohighlight">\(\mathbf{x}_p\)</span> in <span class="math notranslate nohighlight">\(Z\)</span>.</p></li>
</ul>
</div>
<div class="tip admonition" id="ex1">
<p class="admonition-title">Example Transformation</p>
<ul class="simple">
<li><p>Original space <span class="math notranslate nohighlight">\(X\)</span>: <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span> with <span class="math notranslate nohighlight">\(d=2\)</span> basis-functions <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(x_2\)</span></p></li>
<li><p>High-dimensional space <span class="math notranslate nohighlight">\(Z\)</span>: <span class="math notranslate nohighlight">\(\mathbb{R}^6\)</span> with <span class="math notranslate nohighlight">\(r=6\)</span> basis functions</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
   z_1=\Phi_1(\mathbf{x}) &amp; = &amp; 1 \nonumber\\
   z_2=\Phi_2(\mathbf{x}) &amp; = &amp; \sqrt{2}x_1 \nonumber\\
   z_3=\Phi_3(\mathbf{x}) &amp; = &amp; \sqrt{2}x_2 \nonumber\\
   z_4=\Phi_4(\mathbf{x})&amp; = &amp; \sqrt{2}x_1 x_2 \nonumber\\
   z_5=\Phi_5(\mathbf{x}) &amp; = &amp; x_1^2 \nonumber\\
   z_6=\Phi_6(\mathbf{x}) &amp;=&amp; x_2^2 
   \end{split}\]</div>
</div>
<p>The linear discriminant in the high-dimensional space <span class="math notranslate nohighlight">\(Z\)</span> is defined by:</p>
<div class="math notranslate nohighlight" id="equation-diskz">
<span class="eqno">(34)<a class="headerlink" href="#equation-diskz" title="Permalink to this equation">#</a></span>\[
g(\mathbf{z})=\mathbf{w}^T\mathbf{z} +w_0 \quad = \mathbf{w}^T \Phi(\mathbf{x}) +w_0 \quad = \sum\limits_{j=1}^6 w_j \Phi_j (\mathbf{x}) +w_0
\]</div>
<div class="section" id="kernel-trick">
<h4>Kernel Trick<a class="headerlink" href="#kernel-trick" title="Permalink to this headline">#</a></h4>
<p>In the example above the number of dimensions in the high-dimensional space, in which the discriminant is determined, has been <span class="math notranslate nohighlight">\(r=6\)</span>. In practical cases however, the dimensionality of the new space can be extremely large, such that it would be computational infeasible to transform data in this space and calculate the discriminant there. The transformation can be avoided by applying the <em>kernel-trick</em>, which is described here:</p>
<p>As in equation <a class="reference internal" href="#equation-sumofsupport">(31)</a>, we assume that the weights can be obtained as a weighted sum of support vectors. The only difference is that now we sum up the transformations of the support vectors:</p>
<div class="math notranslate nohighlight" id="equation-sumofsupporttrans">
<span class="eqno">(35)<a class="headerlink" href="#equation-sumofsupporttrans" title="Permalink to this equation">#</a></span>\[
\mathbf{w}= \sum\limits_{p=1}^N \alpha_p r_p \Phi(\mathbf{x}_p) 
\]</div>
<p>By inserting equation <a class="reference internal" href="#equation-sumofsupporttrans">(35)</a> into the discriminant definition <a class="reference internal" href="#equation-diskz">(34)</a> we obtain:</p>
<div class="math notranslate nohighlight" id="equation-diskx">
<span class="eqno">(36)<a class="headerlink" href="#equation-diskx" title="Permalink to this equation">#</a></span>\[
g(\mathbf{x})= \mathbf{w}^T \Phi(\mathbf{x}) +w_0 \quad = \sum\limits_{p=1}^N \left( \alpha_p r_p \Phi(\mathbf{x}_p)^T \Phi(\mathbf{x})\right) +w_0
\]</div>
<p>The <strong>kernel-trick</strong> is now to apply a non-linear kernel function <span class="math notranslate nohighlight">\(K(\mathbf{x}_p,\mathbf{x})\)</span>, which yields the same result as the scalar-product <span class="math notranslate nohighlight">\(\Phi(\mathbf{x}_p)^T \Phi(\mathbf{x})\)</span> in the high-dimensional space, but can be performed in the original space.</p>
<div class="math notranslate nohighlight" id="equation-diskkern">
<span class="eqno">(37)<a class="headerlink" href="#equation-diskkern" title="Permalink to this equation">#</a></span>\[
g(\mathbf{x})=  \sum\limits_{p=1}^N \left( \alpha_p r_p  K(\mathbf{x}_p,\mathbf{x}) \right) +w_0
\]</div>
<p>In order to calculate this discriminant the Langrange-coefficients <span class="math notranslate nohighlight">\(\alpha_p\)</span> must be determined. They are obtained, as in the linear case, by maximizing</p>
<div class="math notranslate nohighlight" id="equation-ldnonlinear">
<span class="eqno">(38)<a class="headerlink" href="#equation-ldnonlinear" title="Permalink to this equation">#</a></span>\[
L_d=-\frac{1}{2}\sum\limits_{p=1}^N \sum\limits_{s=1}^N \left( \alpha_p \alpha_s r_p r_s K(\mathbf{x}_p^T, \mathbf{x}_s) \right)+\sum\limits_{p=1}^N \alpha_p 
\]</div>
<p>w.r.t. the Langrange-coefficients under the constraints</p>
<div class="math notranslate nohighlight" id="equation-ldrestrictions">
<span class="eqno">(39)<a class="headerlink" href="#equation-ldrestrictions" title="Permalink to this equation">#</a></span>\[
\sum\limits_{p=1}^N \alpha_p r_p = 0 \quad \mbox{and} \quad  0 \leq \alpha_p \leq C \quad \forall p .
\]</div>
<p>This already describes the entire training-process for non-linear SVM classifiers. However, you may wonder how to find a suitable transformation <span class="math notranslate nohighlight">\(\Phi\)</span> and a corresponding kernel <span class="math notranslate nohighlight">\(K(\mathbf{x}_p^T, \mathbf{x}_s)\)</span>? Actually, in practical SVMs we do not take care about a concrete transformation. Instead we select a type of kernel (linear, polynomial or RBF). The selected kernel corresponds to a transformation into a higher-dimensional space, but we do not have to care about this transformation. We just need the kernel-function.</p>
<div class="section" id="linear-kernel">
<h5>Linear Kernel<a class="headerlink" href="#linear-kernel" title="Permalink to this headline">#</a></h5>
<p>The linear kernel is just the scalar-product of the input vectors:</p>
<div class="math notranslate nohighlight" id="equation-linkernel">
<span class="eqno">(40)<a class="headerlink" href="#equation-linkernel" title="Permalink to this equation">#</a></span>\[
K_{lin}(\mathbf{x}_p,\mathbf{x}) = \mathbf{x}_p^T \mathbf{x}
\]</div>
<p>By inserting this linear kernel into equation <a class="reference internal" href="#equation-ldnonlinear">(38)</a> we obtain equation <a class="reference internal" href="#equation-linmax">(33)</a>. I.e. by applying the linear kernel no transform to a higher-dimensional space is performed. Instead just a linear discriminant is learned in the original space, as described in previous section.</p>
</div>
<div class="section" id="polynomial-kernel">
<h5>Polynomial Kernel<a class="headerlink" href="#polynomial-kernel" title="Permalink to this headline">#</a></h5>
<p>Polynomial kernels are defined by</p>
<div class="math notranslate nohighlight" id="equation-polkernel">
<span class="eqno">(41)<a class="headerlink" href="#equation-polkernel" title="Permalink to this equation">#</a></span>\[
K_{pol}(\mathbf{x}_p,\mathbf{x}) = \left( \mathbf{x}^T\mathbf{x}_p+1\right)^q,
\]</div>
<p>where the degree <span class="math notranslate nohighlight">\(q\)</span> is a hyperparameter and helps to control the complexity of the learned models. The higher the degree <span class="math notranslate nohighlight">\(q\)</span>, the higher the dimension of the corresponding space, to which data is virtually transformed, and the higher the complexity of the learned model.</p>
<div class="tip admonition">
<p class="admonition-title">Example Polynomial kernel with degree <span class="math notranslate nohighlight">\(d=2\)</span></p>
<p>For a degree of <span class="math notranslate nohighlight">\(d=2\)</span>, the polynomial kernel as defined in <a class="reference internal" href="#equation-polkernel">(41)</a> is (for a better readability we replaced <span class="math notranslate nohighlight">\(\mathbf{x}_p\)</span> by <span class="math notranslate nohighlight">\(\mathbf{y}\)</span>):</p>
<div class="math notranslate nohighlight">
\[\begin{split}
K(\mathbf{y},\mathbf{x}) &amp; = &amp; \left( \mathbf{x}^T\mathbf{y}+1\right)^2  \\
   							 &amp; = &amp; \left( x_1y_1+x_2y_2+1 \right)^2  \\
   							 &amp; = &amp; \left( 1+2x_1y_1+2x_2y_2+2x_1x_2y_1y_2+x_1^2y_1^2+x_2^2y_2^2 \right) 
\end{split}\]</div>
<p>Note that this kernel yields exactly the same result as the scalar product <span class="math notranslate nohighlight">\(\Phi(\mathbf{y})^T \Phi(\mathbf{x})\)</span> in the 6-dimensional space, whose dimensions are defined as in the <a class="reference internal" href="#ex1"><span class="std std-ref">example above</span></a>.</p>
</div>
<p>In <a class="reference internal" href="#vardegree"><span class="std std-ref">the image below</span></a> it is shown how the different kernels (linear or polynomial) and different degrees, influence the type of discriminant. The higher the degree, the better fit to the training data but the higher the potential for overfitting.</p>
<div class="figure align-center" id="vardegree">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/svmPolyDegree.PNG"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/svmPolyDegree.PNG" src="https://maucher.home.hdm-stuttgart.de/Pics/svmPolyDegree.PNG" style="width: 800pt;" /></a>
<p class="caption"><span class="caption-number">Fig. 10 </span><span class="caption-text">Linear kernel vs. polynomial kernel of degree 2 versus polynomial kernel of degree 5. The corresponding discriminant in the original space can be complexer for increasing degree.</span><a class="headerlink" href="#vardegree" title="Permalink to this image">#</a></p>
</div>
</div>
<div class="section" id="rbf-kernel">
<h5>RBF Kernel<a class="headerlink" href="#rbf-kernel" title="Permalink to this headline">#</a></h5>
<p>An important non-linear kernel type is the <strong>Radial Basis Function (RBF)</strong>, which is defined as follows:</p>
<div class="math notranslate nohighlight" id="equation-rbfkernel">
<span class="eqno">(42)<a class="headerlink" href="#equation-rbfkernel" title="Permalink to this equation">#</a></span>\[
K_{rbf}(\mathbf{x}_p,\mathbf{x})=\exp \left[ - \gamma ||\mathbf{x}_p - \mathbf{x}    ||^2   \right]
\]</div>
<p>This function defines a spherical kernel around the support vector <span class="math notranslate nohighlight">\(\mathbf{x}_p\)</span>. The hyperparameter <span class="math notranslate nohighlight">\(\gamma\)</span> determines the <em>width</em> of the spherical kernel. Each of the 4 plots in the image below sketches 3 RBF-kernels around different centers (support vectors) and their sum. In each of the 4 plots a different hyperparameter <span class="math notranslate nohighlight">\(\gamma\)</span> has been applied.</p>
<div class="figure align-center" id="id10">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/1dimensionalRBFs.png"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/1dimensionalRBFs.png" src="https://maucher.home.hdm-stuttgart.de/Pics/1dimensionalRBFs.png" style="width: 800pt;" /></a>
<p class="caption"><span class="caption-number">Fig. 11 </span><span class="caption-text">1-dimensional RBFs of different degrees and different parameters <span class="math notranslate nohighlight">\(\gamma\)</span>. In each of the 4 plots the sum of the 3 kernel-functions is plotted as a dashed-line.</span><a class="headerlink" href="#id10" title="Permalink to this image">#</a></p>
</div>
<p>In <a class="reference internal" href="#svcgammainfluence"><span class="std std-ref">the image below</span></a> for 4 different values of <span class="math notranslate nohighlight">\(\gamma\)</span> the discriminant learned from the given training data is shown. As can be seen, the smaller the value of <span class="math notranslate nohighlight">\(\gamma\)</span>, the wider the spherical kernel and the smoother the sum of the kernels and the corresponding discriminant. I.e. a smaller <span class="math notranslate nohighlight">\(\gamma\)</span> reduces the potentential for overfitting.</p>
<div class="figure align-center" id="svcgammainfluence">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/svmRbfGamma.PNG"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/svmRbfGamma.PNG" src="https://maucher.home.hdm-stuttgart.de/Pics/svmRbfGamma.PNG" style="width: 600pt;" /></a>
<p class="caption"><span class="caption-number">Fig. 12 </span><span class="caption-text">1-dimensional RBFs of different degrees and different parameters <span class="math notranslate nohighlight">\(\gamma\)</span>. In each of the 4 plots the sum of the 3 kernel-functions is plotted as a dashed-line.</span><a class="headerlink" href="#svcgammainfluence" title="Permalink to this image">#</a></p>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="svm-for-regression-svr">
<h2>SVM for Regression (SVR)<a class="headerlink" href="#svm-for-regression-svr" title="Permalink to this headline">#</a></h2>
<div class="section" id="recap-linear-regression">
<h3>Recap Linear Regression<a class="headerlink" href="#recap-linear-regression" title="Permalink to this headline">#</a></h3>
<p>The supervised learning category regression has already been introduced in section <a class="reference internal" href="LinReg.html"><span class="doc std std-doc">Linear Regression</span></a>. Recall that in linear regression during training in general the weights <span class="math notranslate nohighlight">\(w_i \in \Theta\)</span> are determined, which minimize the regularized loss function:</p>
<div class="math notranslate nohighlight" id="equation-elasticnet2">
<span class="eqno">(43)<a class="headerlink" href="#equation-elasticnet2" title="Permalink to this equation">#</a></span>\[
\mathbf{w}=argmin\left( \sum\limits_{t=1}^N [r_t-g(\mathbf{x}_t|\Theta)]^2 + \lambda \rho ||w||_1  +  \frac{\lambda (1-\rho)}{2} ||w||_2^2 \right),
\]</div>
<p>In this function, the case</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\lambda = 0\)</span> refers to <em>Ordinary Least Square</em> without regularisation</p></li>
<li><p><span class="math notranslate nohighlight">\(\lambda &gt; 0 \mbox{ and } \rho=0\)</span> refers to <em>Ridge Regression</em></p></li>
<li><p><span class="math notranslate nohighlight">\(\lambda &gt; 0 \mbox{ and } \rho=1\)</span> refers to <em>Lasso Regression</em></p></li>
<li><p><span class="math notranslate nohighlight">\(\lambda &gt; 0 \mbox{ and } 0&lt; \rho &lt; 1\)</span> refers to <em>Elastic Net Regression</em></p></li>
</ul>
</div>
<div class="section" id="optimisation-objective-in-svr-training">
<h3>Optimisation Objective in SVR training<a class="headerlink" href="#optimisation-objective-in-svr-training" title="Permalink to this headline">#</a></h3>
<p>The goal of SVR training is not to find the <span class="math notranslate nohighlight">\(g(\mathbf{x}|\Theta)\)</span>, which minimizes any variant of equation <a class="reference internal" href="#equation-elasticnet2">(43)</a>. Instead a <span class="math notranslate nohighlight">\(g(\mathbf{x}|\Theta)\)</span> has to be determined, for which the relation</p>
<div class="math notranslate nohighlight" id="equation-epscond">
<span class="eqno">(44)<a class="headerlink" href="#equation-epscond" title="Permalink to this equation">#</a></span>\[
\mid g(\mathbf{x}_t|\Theta) - r_t \mid &lt; \epsilon
\]</div>
<p>is true for all <span class="math notranslate nohighlight">\((\mathbf{x}_t,r_t) \in T\)</span>. This means that all training instances must be located within a region of radius <span class="math notranslate nohighlight">\(\epsilon\)</span> around <span class="math notranslate nohighlight">\(g(\mathbf{x}|\Theta)\)</span>. This region will be called the <strong><span class="math notranslate nohighlight">\(\epsilon\)</span>-tube</strong> of <span class="math notranslate nohighlight">\(g(\mathbf{x}|\Theta)\)</span>. The radius <span class="math notranslate nohighlight">\(\epsilon\)</span> of the tube is a hyperparameter, which is configured by the user.</p>
<p>In the image below a <span class="math notranslate nohighlight">\(g(\mathbf{x}|\Theta)\)</span>, which fulfills the condition that all training instances are located within it’s <span class="math notranslate nohighlight">\(\epsilon\)</span>-tube is shown.</p>
<div class="figure align-center" id="svrlineps">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/svrLinEps08.png"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/svrLinEps08.png" src="https://maucher.home.hdm-stuttgart.de/Pics/svrLinEps08.png" style="width: 600pt;" /></a>
<p class="caption"><span class="caption-number">Fig. 13 </span><span class="caption-text">For the configured <span class="math notranslate nohighlight">\(\epsilon\)</span> a <span class="math notranslate nohighlight">\(g(\mathbf{x}|\Theta)\)</span> has been determined, which fulfills condition <a class="reference internal" href="#equation-epscond">(44)</a> for all training elements. I.e. all training instances lie within the <span class="math notranslate nohighlight">\(\epsilon\)</span>-tube.</span><a class="headerlink" href="#svrlineps" title="Permalink to this image">#</a></p>
</div>
<p>In order to find a regularized <span class="math notranslate nohighlight">\(g(\mathbf{x}|\Theta)\)</span>, which fulfills condition <a class="reference internal" href="#equation-epscond">(44)</a> the following quadratic optimisation problem with constraints is solved:
Find</p>
<div class="math notranslate nohighlight" id="equation-svrminweight">
<span class="eqno">(45)<a class="headerlink" href="#equation-svrminweight" title="Permalink to this equation">#</a></span>\[
\min(\frac{1}{2}||\mathbf{w}||^2) 
\]</div>
<p>such that</p>
<div class="math notranslate nohighlight" id="equation-svrcond">
<span class="eqno">(46)<a class="headerlink" href="#equation-svrcond" title="Permalink to this equation">#</a></span>\[
\mathbf{w}^T\mathbf{x}_t+w_0 -r_t \leq \epsilon, \quad \mbox{ or } \quad r_t-(\mathbf{w}^T\mathbf{x}_t+w_0) \leq \epsilon \quad \forall t
\]</div>
<p>Depending on the configured value for <span class="math notranslate nohighlight">\(\epsilon\)</span> this optimisation problem may be not solvable. In this case a similar approach is implemented as in equation <a class="reference internal" href="#equation-minsofterror">(32)</a> for SVM classification. I.e. one accepts a <strong>soft-error</strong> for each training instance <span class="math notranslate nohighlight">\((\mathbf{x}_t,r_t) \in T\)</span> but tries to keep the sum over all soft-errors as small as possible.</p>
<p>The soft-error of a single training instance <span class="math notranslate nohighlight">\((\mathbf{x}_t,r_t)\)</span> is defined to be the distance between the target <span class="math notranslate nohighlight">\(r_t\)</span> and the boundary of the <span class="math notranslate nohighlight">\(\epsilon\)</span>-tube around <span class="math notranslate nohighlight">\(g(\mathbf{x}|\Theta)\)</span>. This means that all instances, which lie within the <span class="math notranslate nohighlight">\(\epsilon\)</span>-tube do not contribute to the error at all!</p>
<p>Formally, the so called <strong><span class="math notranslate nohighlight">\(\epsilon\)</span>-sensitive error function</strong> is</p>
<div class="math notranslate nohighlight" id="equation-epserror">
<span class="eqno">(47)<a class="headerlink" href="#equation-epserror" title="Permalink to this equation">#</a></span>\[\begin{split}
E(T)=  \sum\limits_{t=1}^N E_{\epsilon}(g(x_t)-r_t), \mbox{ where }\\
E_{\epsilon}(g(x_t)-r_t) = \left\{ 
	\begin{array}{ll}
	0 &amp; \mbox{ if }  |g(x_t)-r_t| &lt; \epsilon \\
	|g(x_t)-r_t| - \epsilon &amp; else \\
	\end{array}
	\right.
\end{split}\]</div>
<p>Besides the property that deviations within the <span class="math notranslate nohighlight">\(\epsilon\)</span>-tube are ignored, there is another crucial difference compared to the MSE: The difference between the model’s prediction and the target contributes only linearly to the entire soft-error. This linear increase of the error, compared to the quadratic-increase of the MSE, is depicted in the image below. Because of these properties SVR regression is more robust in the presence of outliers.</p>
<div class="figure align-center" id="svrerrorfunction">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/svmErrorFunc.png"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/svmErrorFunc.png" src="https://maucher.home.hdm-stuttgart.de/Pics/svmErrorFunc.png" style="width: 600pt;" /></a>
<p class="caption"><span class="caption-number">Fig. 14 </span><span class="caption-text">Linear Increase of <span class="math notranslate nohighlight">\(\epsilon\)</span>-sensitive error (red) compared to quadratic increase of MSE (blue).</span><a class="headerlink" href="#svrerrorfunction" title="Permalink to this image">#</a></p>
</div>
<p>As already mentioned, the soft-error of a single training instance <span class="math notranslate nohighlight">\((\mathbf{x}_t,r_t)\)</span> is defined to be the distance between the target <span class="math notranslate nohighlight">\(r_t\)</span> and the boundary of the <span class="math notranslate nohighlight">\(\epsilon\)</span>-tube around <span class="math notranslate nohighlight">\(g(\mathbf{x}|\Theta)\)</span>. For training instances <span class="math notranslate nohighlight">\((\mathbf{x}_t,r_t)\)</span> above the <strong><span class="math notranslate nohighlight">\(\epsilon\)</span>-tube</strong>, the error-margin is denoted by <span class="math notranslate nohighlight">\(\zeta_t\)</span>. For training instances <span class="math notranslate nohighlight">\((\mathbf{x}_t,r_t)\)</span> below the <strong><span class="math notranslate nohighlight">\(\epsilon\)</span>-tube</strong>, the error-margin is denoted by <span class="math notranslate nohighlight">\(\widehat{\zeta}_t\)</span>.</p>
<div class="figure align-center" id="svrnonlinreg">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/svrNonLinReg011.png"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/svrNonLinReg011.png" src="https://maucher.home.hdm-stuttgart.de/Pics/svrNonLinReg011.png" style="width: 600pt;" /></a>
<p class="caption"><span class="caption-number">Fig. 15 </span><span class="caption-text">Only the training instances outside the <span class="math notranslate nohighlight">\(\epsilon\)</span>-tube contibute to the soft-error. For instances below the tube the error margin is denoted by <span class="math notranslate nohighlight">\(\widehat{\zeta}_t\)</span>, for instances above the tube the margin is denoted by <span class="math notranslate nohighlight">\(\zeta_t\)</span>.</span><a class="headerlink" href="#svrnonlinreg" title="Permalink to this image">#</a></p>
</div>
<p>Similar as in equation <a class="reference internal" href="#equation-minsofterror">(32)</a> for SVM classification, the optimisation problem, which accepts error-margins is now:</p>
<p>Minimize</p>
<div class="math notranslate nohighlight" id="equation-svrsoftopt">
<span class="eqno">(48)<a class="headerlink" href="#equation-svrsoftopt" title="Permalink to this equation">#</a></span>\[
C \sum\limits_{t=1}^N(\zeta_t+\widehat{\zeta_t}) \, + \, \frac{1}{2}||\mathbf{w}||^2
\]</div>
<p>for <span class="math notranslate nohighlight">\(\zeta_t \geq 0\)</span> and <span class="math notranslate nohighlight">\(\widehat{\zeta_t} &gt; 0\)</span> and</p>
<div class="math notranslate nohighlight">
\[\begin{split}
r_t - g(x_t) &amp; \leq &amp; \epsilon + \zeta_t  \nonumber \\
g(x_t) - r_t &amp; \leq &amp; \epsilon + \widehat{\zeta}_t. \nonumber \\
\end{split}\]</div>
<p>The dual form of this optimisation problem is :</p>
<p>Maximize</p>
<div class="math notranslate nohighlight" id="equation-dualoptsvr">
<span class="eqno">(49)<a class="headerlink" href="#equation-dualoptsvr" title="Permalink to this equation">#</a></span>\[
L_d=-\frac{1}{2}\sum\limits_{p=1}^N \sum\limits_{s=1}^N \left( (\alpha_p-\widehat{\alpha}_p) (\alpha_s -\widehat{\alpha}_s) K(\mathbf{x}_p , \mathbf{x}_s) \right)   
	- \epsilon \sum\limits_{p=1}^N (\alpha_p+\widehat{\alpha}_p) + \sum\limits_{p=1}^N (\alpha_p-\widehat{\alpha}_p)r_p,
\]</div>
<p>w.r.t. the Langrange coefficients <span class="math notranslate nohighlight">\(\alpha_p\)</span> (corresponds to error-margin <span class="math notranslate nohighlight">\(\zeta_p\)</span>) and <span class="math notranslate nohighlight">\(\widehat{\alpha}_p\)</span> (corresponds to error-margin <span class="math notranslate nohighlight">\(\widehat{\zeta}_p\)</span>) under the constraints</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  0 \leq \alpha_p \leq C \\
  0 \leq \widehat{\alpha}_p \leq C \\
  \sum\limits_{p=1}^N (\alpha_p-\widehat{\alpha}_p) = 0
\end{split}\]</div>
<p>For the kernel</p>
<div class="math notranslate nohighlight">
\[
K(\mathbf{x}_p , \mathbf{x}_s)= \Phi(\mathbf{x}_p)^T \Phi(\mathbf{x}_s)
\]</div>
<p>in equation <a class="reference internal" href="#equation-dualoptsvr">(49)</a> the same options can be applied as in the classification task, e.g. linear-, polynomial- or RBF-kernel.</p>
<p>The result of the above optimisation are the Lagrange coefficients <span class="math notranslate nohighlight">\(\alpha_p\)</span> and <span class="math notranslate nohighlight">\(\widehat{\alpha}_p\)</span>. Similar to equation <a class="reference internal" href="#equation-sumofsupporttrans">(35)</a> for SVM classification, the weights can be calculated by</p>
<div class="math notranslate nohighlight" id="equation-wreg">
<span class="eqno">(50)<a class="headerlink" href="#equation-wreg" title="Permalink to this equation">#</a></span>\[
 \mathbf{w}=\sum\limits_{p=1}^N (\alpha_p-\widehat{\alpha}_p) \Phi(\mathbf{x}_p), 
\]</div>
<p>Inserting this equation for the weights into</p>
<div class="math notranslate nohighlight">
\[
g(\mathbf{x})= \mathbf{w}^T \Phi(\mathbf{x}) +w_0
\]</div>
<p>yields</p>
<div class="math notranslate nohighlight" id="equation-svrpartw">
<span class="eqno">(51)<a class="headerlink" href="#equation-svrpartw" title="Permalink to this equation">#</a></span>\[
g(\mathbf{x})= \sum\limits_{p=1}^N (\alpha_p-\widehat{\alpha}_p) K(\mathbf{x},\mathbf{x}_p)+w_0
\]</div>
<p>The training inputs <span class="math notranslate nohighlight">\(\mathbf{x}_p\)</span> with <span class="math notranslate nohighlight">\(\alpha_p \neq 0 \vee \widehat{\alpha}_p \neq 0\)</span> are the support vectors. Only the support vectors determine the learned function. It can be shown, that all training instances <span class="math notranslate nohighlight">\((\mathbf{x}_t,r_t)\)</span>, which lie</p>
<ul class="simple">
<li><p>within the <span class="math notranslate nohighlight">\(\epsilon\)</span>-tube have <span class="math notranslate nohighlight">\(\alpha_p=\widehat{\alpha}_p=0\)</span></p></li>
<li><p>on the upper boundary of the <span class="math notranslate nohighlight">\(\epsilon\)</span>-tube have <span class="math notranslate nohighlight">\(0 &lt; \alpha_p &lt; C \quad \widehat{\alpha}_p = 0\)</span></p></li>
<li><p>above the upper boundary of the <span class="math notranslate nohighlight">\(\epsilon\)</span>-tube have <span class="math notranslate nohighlight">\(\alpha_p = C \quad \widehat{\alpha}_p = 0\)</span></p></li>
<li><p>on the lower boundary of the <span class="math notranslate nohighlight">\(\epsilon\)</span>-tube have <span class="math notranslate nohighlight">\(\alpha_p \quad 0 &lt; \widehat{\alpha}_p &lt; C\)</span></p></li>
<li><p>below the lower boundary of the <span class="math notranslate nohighlight">\(\epsilon\)</span>-tube have <span class="math notranslate nohighlight">\(\alpha_p = 0 \quad \widehat{\alpha}_p = C\)</span></p></li>
</ul>
<p>I.e. the support vectors are all input-vectors, which do not lie within the <span class="math notranslate nohighlight">\(\epsilon\)</span>-tube.</p>
<p>In equation <a class="reference internal" href="#equation-svrpartw">(51)</a> the weight <span class="math notranslate nohighlight">\(w_0\)</span> is still unknown. It can be calculated for example by choosing a training instance <span class="math notranslate nohighlight">\((\mathbf{x}_t,r_t)\)</span> for which the corresponding Lagrange coefficient fulfills <span class="math notranslate nohighlight">\(0 &lt; \alpha_p &lt; C\)</span>. This training instance lies on the upper boundary of the <span class="math notranslate nohighlight">\(\epsilon\)</span>-tube, i.e. <span class="math notranslate nohighlight">\(\zeta=0\)</span> and therefore</p>
<div class="math notranslate nohighlight">
\[
w_0 = r_p - \epsilon - \sum\limits_{m=1}^N (\alpha_m-\widehat{\alpha}_m) K(\mathbf{x_p},\mathbf{x}_m).
\]</div>
</div>
<div class="section" id="influence-of-the-hyperparameters">
<h3>Influence of the hyperparameters<a class="headerlink" href="#influence-of-the-hyperparameters" title="Permalink to this headline">#</a></h3>
<p>Besides the kernel-function the most important hyperparameters of SVRs are the radius of the tube <span class="math notranslate nohighlight">\(\epsilon\)</span> and the parameter <span class="math notranslate nohighlight">\(C\)</span>. As can be seen in equation <a class="reference internal" href="#equation-svrsoftopt">(48)</a> a higher value of <span class="math notranslate nohighlight">\(C\)</span> yields less importance on weight-minimmisation and therefore a stronger fit to the training data. The hyperparameter <span class="math notranslate nohighlight">\(\epsilon\)</span> also influences underfitting and overfitting, respectively. A large radius of the tube (large <span class="math notranslate nohighlight">\(\epsilon\)</span>) implies that many training-instances lie within the tube. Accordingly the number of support-vectors is small and it’s relatively easy to achieve a small overall soft-error. With wide tubes that easily cover all training elements weight-minimizsation gets easier and the chance for underfitting increases.</p>
<p>The influence of <span class="math notranslate nohighlight">\(\epsilon\)</span> and <span class="math notranslate nohighlight">\(C\)</span> is visualized in the images below.</p>
<div class="figure align-center" id="varepsilon">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/svrNonLinRBFvaryEps.png"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/svrNonLinRBFvaryEps.png" src="https://maucher.home.hdm-stuttgart.de/Pics/svrNonLinRBFvaryEps.png" style="width: 600pt;" /></a>
<p class="caption"><span class="caption-number">Fig. 16 </span><span class="caption-text">Learned SVR models with RBF kernels. Parameter <span class="math notranslate nohighlight">\(C=10\)</span> is fixed. Parameter <span class="math notranslate nohighlight">\(\epsilon\)</span> increases from the upper left to the lower right plot. Smaller <span class="math notranslate nohighlight">\(\epsilon\)</span> increases the chance of overfitting.</span><a class="headerlink" href="#varepsilon" title="Permalink to this image">#</a></p>
</div>
<div class="figure align-center" id="varc">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/svrNonLinRBFvaryC.png"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/svrNonLinRBFvaryC.png" src="https://maucher.home.hdm-stuttgart.de/Pics/svrNonLinRBFvaryC.png" style="width: 600pt;" /></a>
<p class="caption"><span class="caption-number">Fig. 17 </span><span class="caption-text">Learned SVR models with RBF kernels. Parameter <span class="math notranslate nohighlight">\(\epsilon=0.2\)</span> is fixed. Parameter <span class="math notranslate nohighlight">\(C\)</span> increases from the upper left to the lower right plot. Higher <span class="math notranslate nohighlight">\(C\)</span> increases the chance of overfitting.</span><a class="headerlink" href="#varc" title="Permalink to this image">#</a></p>
</div>
<p><strong>Further Reading:</strong></p>
<ul class="simple">
<li><p><span id="id2">[<a class="reference internal" href="../referenceSection.html#id59" title="E Alpaydin. Introduction to Machine Learning. MIT Press, 2 edition, 2010. ISBN 978-0-262-01243-0.">Alp10</a>]</span></p></li>
</ul>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./machinelearning"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="LinearClassification.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Linear Classification</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="gp.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Gaussian Process</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Prof. Dr. Johannes Maucher<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>