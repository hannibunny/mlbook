
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Linear Regression &#8212; Machine Learning Lecture</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Example Linear Regression" href="heartRateRegression.html" />
    <link rel="prev" title="Bayes- and Naive Bayes Classifier" href="parametricClassification1D.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/hdmlogomed.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Machine Learning Lecture</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Intro and Overview Machine Learning Lecture
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../00BasicConcepts.html">
   Basic Concepts of Data Mining and Machine Learning
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Conventional ML
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="knn.html">
   K - Nearest Neighbour Classification / Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="parametricClassification1D.html">
   Bayes- and Naive Bayes Classifier
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="heartRateRegression.html">
   Example Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="LinearClassification.html">
   Linear Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="svm.html">
   Support Vector Machines (SVM)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="gp.html">
   Gaussian Process
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="GaussianProcessRegression.html">
   Gaussian Process: Implementation in Python
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Neural Networks
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../neuralnetworks/01NeuralNets.html">
   Neural Networks Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../neuralnetworks/02RecurrentNeuralNetworks.html">
   Recurrent Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../neuralnetworks/03ConvolutionNeuralNetworks.html">
   Convolutional Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../neuralnetworks/convolutionDemos.html">
   Animations of Convolution and Deconvolution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../neuralnetworks/03KerasMLPandCNNcifar.html">
   Implementing Neural Networks with Keras
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../neuralnetworks/04KerasPretrainedClassifiers.html">
   Applying Pretrained Deep Neural Networks for Image Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../neuralnetworks/05KerasPretrainedCovid.html">
   Apply Pretrained Neural Networks on new Task
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Autoencoder
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../neuralnetworks/04VariationalAutoencoder.html">
   Variational Autoencoder (VAE) to generate handwritten digits
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  GAN
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../gan/GAN.html">
   Generative Adversarial Nets (GAN)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../gan/DCGAN.html">
   DCGAN Keras Implementation
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Reinforcement Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../rl/reinforcement.html">
   Reinforcement Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../rl/DQN.html">
   Deep Reinforcement Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../rl/QLearnFrozenLake.html">
   Example Q-Learning
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Text
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../text/01ModellingWordsAndTexts.html">
   Representations for Words and Texts
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../text/02TextClassification.html">
   Text classification with CNNs and LSTMs
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Graph Neural Networks
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../neuralnetworks/GraphNeuralNetworks.html">
   Graph Neural Networks (GNN)
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Transformer
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../transformer/attention.html">
   Sequence-To-Sequence, Attention, Transformer
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../transformer/intent_classification_with_bert.html">
   Intent Classification with BERT
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../referenceSection.html">
   References
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/machinelearning/LinReg.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#maximum-likelihood-estimation">
   Maximum-Likelihood Estimation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#generalized-linear-regression">
   Generalized Linear Regression
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#regularisation">
   Regularisation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ridge-regression">
     Ridge Regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lasso">
     Lasso
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#elastic-net">
     Elastic-Net
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Linear Regression</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#maximum-likelihood-estimation">
   Maximum-Likelihood Estimation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#generalized-linear-regression">
   Generalized Linear Regression
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#regularisation">
   Regularisation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#ridge-regression">
     Ridge Regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lasso">
     Lasso
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#elastic-net">
     Elastic-Net
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="linear-regression">
<h1>Linear Regression<a class="headerlink" href="#linear-regression" title="Permalink to this headline">#</a></h1>
<p>For a given set of labeled training data</p>
<div class="math notranslate nohighlight">
\[
T=\{\mathbf{x}_t,r_t \}_{t=1}^N, 
\]</div>
<p>where the targets <span class="math notranslate nohighlight">\(r_t\)</span> are numeric values, usually from <span class="math notranslate nohighlight">\(\mathcal{R}\)</span> and the inputs <span class="math notranslate nohighlight">\(\mathbf{x}_t=(x_{1,t}, \ldots, x_{d,t})\)</span> are numeric vectors of length <span class="math notranslate nohighlight">\(d\)</span>, the goal of regression is to learn a function, which maps the input-vectors to the target-values.</p>
<p>We usually assume that the targets <span class="math notranslate nohighlight">\(r\)</span> can be calculated as a sum of a determinisitc function <span class="math notranslate nohighlight">\(f(\mathbf{x})\)</span> and a non-determininstic noise <span class="math notranslate nohighlight">\(n\)</span></p>
<div class="math notranslate nohighlight">
\[
r=f(\mathbf{x})+n,
\]</div>
<p>and we like to find a good approximation <span class="math notranslate nohighlight">\(g(\mathbf{x}|\Theta)\)</span> for the unknown deterministic part <span class="math notranslate nohighlight">\(f(\mathbf{x})\)</span>.</p>
<p>In linear regression we assume that the approximation <span class="math notranslate nohighlight">\(g(x|\Theta)\)</span> is a linear function of type</p>
<div class="math notranslate nohighlight" id="equation-linfunction">
<span class="eqno">(8)<a class="headerlink" href="#equation-linfunction" title="Permalink to this equation">#</a></span>\[
g(\mathbf{x})=w_0 + w_1 x_1 + w_2 x_2 + \cdots + w_d x_d 
\]</div>
<p>This means, that we assume a certain type of function and we want to learn the parameters</p>
<div class="math notranslate nohighlight">
\[
\Theta = \lbrace w_0, w_1, \ldots , w_d \rbrace
\]</div>
<p>from data, such that the corresponding <span class="math notranslate nohighlight">\(g(\mathbf{x}|\Theta)\)</span> is a good estimate for <span class="math notranslate nohighlight">\(f(\mathbf{x})\)</span>.</p>
<p>Concerning the non-deterministic part <span class="math notranslate nohighlight">\(n\)</span> one often assumes that it is a Gaussian-distributed random variable of mean <span class="math notranslate nohighlight">\(\mu=0\)</span> and standard-deviation <span class="math notranslate nohighlight">\(\sigma\)</span>. In this case the posterior <span class="math notranslate nohighlight">\(p(r|\mathbf{x})\)</span> is also a Gaussian distribution with standard-deviation <span class="math notranslate nohighlight">\(\sigma\)</span> and mean <span class="math notranslate nohighlight">\(\mu=g(\mathbf{x} \mid \theta)\)</span>.</p>
<section id="maximum-likelihood-estimation">
<h2>Maximum-Likelihood Estimation<a class="headerlink" href="#maximum-likelihood-estimation" title="Permalink to this headline">#</a></h2>
<p>Maximum-Likelihood Estimation (MLE) estimates the parameters <span class="math notranslate nohighlight">\(\Theta\)</span>, such that the corresponding <span class="math notranslate nohighlight">\(g(\mathbf{x}|\Theta)\)</span> is the one, which most likely generates the given set of training data <span class="math notranslate nohighlight">\(T\)</span>.</p>
<p>Under the assumption that noise <span class="math notranslate nohighlight">\(n\)</span> is a Gaussian-distributed variable of mean <span class="math notranslate nohighlight">\(\mu=0\)</span>, one can prove that the MLE approach can be realized by minimizing the <strong>Sum of Squared Error (SSE) Loss function</strong>:</p>
<div class="math notranslate nohighlight" id="equation-sse">
<span class="eqno">(2)<a class="headerlink" href="#equation-sse" title="Permalink to this equation">#</a></span>\[
E(\Theta | T)=\frac{1}{2} \sum\limits_{t=1}^N [r_t-g(\mathbf{x}_t|\Theta)]^2 = \frac{1}{2} \sum\limits_{t=1}^N [r_t-(w_0 + w_1 x_{1,t} + \cdots + w_d x_{d,t})]^2
\]</div>
<p>This means that the learning task is: <strong>Determine the parameters <span class="math notranslate nohighlight">\(w_i \in \Theta\)</span>, which minimize the SSE (function <a class="reference internal" href="#equation-sse">(2)</a>).</strong></p>
<p>In general, in order to find the value <span class="math notranslate nohighlight">\(x\)</span>, at which a function <span class="math notranslate nohighlight">\(f(x)\)</span> is minimal, we have to calculate the first derivation <span class="math notranslate nohighlight">\(f'(x)=\frac{\partial f}{\partial x} \)</span> and determine it’s zeros.</p>
<p>Here we have an error function <span class="math notranslate nohighlight">\(E(\Theta | T)\)</span>, which depends not only on a single variable, but on all weights <span class="math notranslate nohighlight">\(w_i \in \Theta\)</span>. Hence, we have to determine all partial derivatives</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{\partial E}{\partial w_0} &amp;=&amp; \sum\limits_{t=1}^N \left[r_t-(w_0 + w_1 x_{1,t} + \cdots + w_d x_{d,t})\right] \cdot -1 \\
\frac{\partial E}{\partial w_1} &amp;=&amp; \sum\limits_{t=1}^N \left[r_t-(w_0 + w_1 x_{1,t} + \cdots + w_d x_{d,t})\right] \cdot -x_{1,t} \\
\vdots							&amp;=&amp;	\vdots \nonumber \\
\frac{\partial E}{\partial w_d} &amp;=&amp; \sum\limits_{t=1}^N \left[r_t-(w_0 + w_1 x_{1,t} + \cdots + w_d x_{d,t})\right] \cdot -x_{d,t}  
\end{split}\]</div>
<p>and set them equal to zero. This yields a system of <span class="math notranslate nohighlight">\(d+1\)</span> linear equations, which can be written as a matrix-multiplication</p>
<div class="math notranslate nohighlight" id="equation-yaw">
<span class="eqno">(3)<a class="headerlink" href="#equation-yaw" title="Permalink to this equation">#</a></span>\[
\mathbf{y}=\mathbf{A} \cdot \mathbf{w},
\]</div>
<p>and solving for <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> yields:</p>
<div class="math notranslate nohighlight" id="equation-way">
<span class="eqno">(4)<a class="headerlink" href="#equation-way" title="Permalink to this equation">#</a></span>\[
\mathbf{w} = \mathbf{A}^{-1} \mathbf{y},
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{A}= \left[ 
\begin{array}{ccccc}
	N &amp; \sum\limits_{t=1}^N x_{1,t} &amp; \sum\limits_{t=1}^N x_{2,t} &amp; \cdots &amp; \sum\limits_{t=1}^N x_{d,t}\\
	\sum\limits_{t=1}^N x_{1,t} &amp; \sum\limits_{t=1}^N x_{1,t} x_{1,t} &amp; \sum\limits_{t=1}^N x_{1,t} x_{2,t}&amp; \cdots &amp; \sum\limits_{t=1}^N x_{1,t} x_{d,t} \\
	\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
	\sum\limits_{t=1}^N x_{d,t} &amp; \sum\limits_{t=1}^N x_{d,t} x_{1,t} &amp; \sum\limits_{t=1}^N x_{d,t} x_{2,t}  &amp; \cdots &amp; \sum\limits_{t=1}^N x_{d,t} x_{d,t} \\
	\end{array}
	\right],
	\qquad 
	\mathbf{w} = \left[ \begin{array}{c}
	w_0 \\
	w_1 \\
	\vdots \\
	w_d 	
	\end{array}
	\right],
	\qquad 
	\mathbf{y} = \left[ \begin{array}{c}
	\sum\limits_{t=1}^N r_t \\
	\sum\limits_{t=1}^N r_t x_{1,t} \\	
	\vdots \\
	\sum\limits_{t=1}^N r_t x_{d,t}\\
	\end{array}
	\right]
\end{split}\]</div>
<p>For an efficient solution one usually calulates <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{A} &amp; = &amp; \mathbf{D}^T \mathbf{D} \nonumber \\
\mathbf{y} &amp; = &amp; \mathbf{D}^T \mathbf{r} \nonumber
\end{split}\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{D}= \left[ 
\begin{array}{ccccc}
1 &amp; x_{1,1} &amp; x_{2,1} &amp; \cdots &amp; x_{d,1} \\
1 &amp; x_{1,2} &amp; x_{2,2} &amp; \cdots &amp; x_{d,2} \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
1 &amp; x_{1,N} &amp; x_{2,N} &amp; \cdots &amp; x_{d,N} \\
\end{array}
\right]
	\qquad \mbox{and} \qquad
\mathbf{r} = \left[ \begin{array}{c}
r_1 \\
r_2 \\
\vdots \\
r_N \\
\end{array}
\right]
\end{split}\]</div>
<p>By expressing <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> in eauation <a class="reference internal" href="#equation-way">(4)</a> in terms of <span class="math notranslate nohighlight">\(\mathbf{D}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{r}\)</span>  the weights, which minimize the loss function are:</p>
<div class="math notranslate nohighlight" id="equation-minweights">
<span class="eqno">(5)<a class="headerlink" href="#equation-minweights" title="Permalink to this equation">#</a></span>\[
\mathbf{w}= \left( \mathbf{D}^T \mathbf{D} \right)^{-1} \mathbf{D}^T \mathbf{r}.
\]</div>
</section>
<section id="generalized-linear-regression">
<h2>Generalized Linear Regression<a class="headerlink" href="#generalized-linear-regression" title="Permalink to this headline">#</a></h2>
<p>With <strong>Linear Regression</strong> one can not only learn linear functions <span class="math notranslate nohighlight">\(g(\mathbf{x})\)</span> of type <a class="reference internal" href="#equation-linfunction">(8)</a>. Since we are free to preprocess the input vectors <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> with an arbitrary aomount <span class="math notranslate nohighlight">\(z\)</span> of preprocessing functions <span class="math notranslate nohighlight">\(\Phi_i\)</span> of arbitrary type (linear and non-linear), a <strong>Generlized Linear Regression</strong> of type</p>
<div class="math notranslate nohighlight" id="equation-genlin">
<span class="eqno">(9)<a class="headerlink" href="#equation-genlin" title="Permalink to this equation">#</a></span>\[
g(\mathbf{x})=w_0 + w_1 \Phi_1(\mathbf{x}) + w_2 \Phi_2(\mathbf{x}) + \cdots + w_z \Phi_z(\mathbf{x})
\]</div>
<p>can be learned. Note that this is still called <strong>linear</strong> regression, because we are still linear in the variable’s <span class="math notranslate nohighlight">\(w_i\)</span>.</p>
<div class="tip admonition" id="exlin1">
<p class="admonition-title">Example Generalized Linear Regression with</p>
<p>Assume that the input vectors are of dimension <span class="math notranslate nohighlight">\(d=2\)</span>, i.e. <span class="math notranslate nohighlight">\(\mathbf{x}=(x_1,x_2)\)</span> and we like to learn a quadratic function. For this we can define a set of functions <span class="math notranslate nohighlight">\(\Phi_i\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-ex1">
<span class="eqno">(7)<a class="headerlink" href="#equation-ex1" title="Permalink to this equation">#</a></span>\[\begin{split}
 \Phi_1(\mathbf{x}) &amp; = &amp; x_1 \\
 \Phi_2(\mathbf{x}) &amp; = &amp; x_2 \\
 \Phi_3(\mathbf{x})&amp; = &amp; x_1 x_2 \\
 \Phi_5(\mathbf{x}) &amp; = &amp; x_1^2 \\
 \Phi_6(\mathbf{x}) &amp;=&amp; x_2^2 
 \end{split}\]</div>
<p>The learning task is then to determine the weights <span class="math notranslate nohighlight">\(w_i\)</span> of the polynomial</p>
<div class="math notranslate nohighlight" id="equation-linfunction">
<span class="eqno">(8)<a class="headerlink" href="#equation-linfunction" title="Permalink to this equation">#</a></span>\[
 g(\mathbf{x})=w_0 + w_1 \Phi_1(\mathbf{x}) + w_2 \Phi_2(\mathbf{x} + \cdots + w_6 \Phi_6(\mathbf{x})
 \]</div>
<p>which yields the minimum loss.</p>
</div>
<p>Finding the weights <span class="math notranslate nohighlight">\(w_i\)</span>, which minimize the loss function, can be performed in the same way as described above. One just has to replace all <span class="math notranslate nohighlight">\(x_{i,t}\)</span> by <span class="math notranslate nohighlight">\(\Phi_i(\mathbf{x}_t)\)</span>. In particular <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> can be calculated as in equation <a class="reference internal" href="#equation-minweights">(5)</a>, but now matrix <span class="math notranslate nohighlight">\(D\)</span> is</p>
<div class="math notranslate nohighlight" id="equation-genlin">
<span class="eqno">(9)<a class="headerlink" href="#equation-genlin" title="Permalink to this equation">#</a></span>\[\begin{split}
\mathbf{D}= \left[ 
\begin{array}{ccccc}
1 &amp; \Phi_1(\mathbf{x}_{1}) &amp; \Phi_2(\mathbf{x}_{1})&amp; \cdots &amp; \Phi_z(\mathbf{x}_{1}) \\
1 &amp; \Phi_1(\mathbf{x}_{2}) &amp; \Phi_2(\mathbf{x}_{2})&amp; \cdots &amp; \Phi_z(\mathbf{x}_{2}) \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
1 &amp; \Phi_1(\mathbf{x}_{N}) &amp; \Phi_2(\mathbf{x}_{N})&amp; \cdots &amp; \Phi_z(\mathbf{x}_{N}) \\
\end{array}
\right]
\end{split}\]</div>
</section>
<section id="regularisation">
<h2>Regularisation<a class="headerlink" href="#regularisation" title="Permalink to this headline">#</a></h2>
<p>In Machine Learning Regularisation is a technique to avoid overfitting. With regularisation the weights are learned such that they not only minimize a loss function on training data (e.g. the Mean Squared Error) but simultaneously have as low as possible absolut values. This additional restriction - absolute values of weights shall be low - yields better generalisation because functions with lower coefficients <span class="math notranslate nohighlight">\(w_i\)</span> are smoother. However, the challenge is to find a good trade-off between minimizing the error on training data and minimizing the weights <span class="math notranslate nohighlight">\(w_i\)</span>. If too much emphasis is put on the weight-minimizsation the learned function may be to simple, i.e. it is underfitted to training data.</p>
<p>The different regularisation methods, described below, learn weights by minimizing training-error and a regularisation term simultaneously:</p>
<div class="math notranslate nohighlight" id="equation-genreg">
<span class="eqno">(10)<a class="headerlink" href="#equation-genreg" title="Permalink to this equation">#</a></span>\[
weights = argmin\left( E(w,T) + \lambda \cdot regularisationterm(w) \right)
\]</div>
<p>The different techniques described below all perform linear regression, but differ in the used <em>regularisation-term</em>. The hyperparameter <span class="math notranslate nohighlight">\(\lambda\)</span> is used to control the trade-off between error-minimisation and weight-minimisation.</p>
<figure class="align-default" id="id1">
<img alt="https://maucher.home.hdm-stuttgart.de/Pics/regularisation.png" src="https://maucher.home.hdm-stuttgart.de/Pics/regularisation.png" />
<figcaption>
<p><span class="caption-number">Fig. 1 </span><span class="caption-text">The plot on the left hand side displays a polynomial of degree 7, which has been learned from the given training data without regularisation. It can be observed, that the weights have comparatively high values and the learned function is tightly fitted to training data (overfitted). In contrast on the right hand side regularisation has been applied (Ridge regression). It can be seen, that now the learned weights are much smaller and the corresponding curve is smoother and not overfitted to training data.</span><a class="headerlink" href="#id1" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<section id="ridge-regression">
<h3>Ridge Regression<a class="headerlink" href="#ridge-regression" title="Permalink to this headline">#</a></h3>
<p>In Ridge-Regression the error-function <span class="math notranslate nohighlight">\(E(w,T)\)</span> in equation <a class="reference internal" href="#equation-genreg">(10)</a> is the MSE and the regularisation term is the squared <strong>L2-norm</strong>. I.e. Ridge-Regression minimizes</p>
<div class="math notranslate nohighlight" id="equation-ridge">
<span class="eqno">(13)<a class="headerlink" href="#equation-ridge" title="Permalink to this equation">#</a></span>\[
\mathbf{w}=argmin\left( \sum\limits_{t=1}^N [r_t-g(\mathbf{x}_t|\Theta)]^2 + \lambda \cdot ||w||_2^2 \right),
\]</div>
<p>where the <strong>p-norm</strong> <span class="math notranslate nohighlight">\(||w||_p\)</span> is defined to be</p>
<div class="math notranslate nohighlight" id="equation-pnorm">
<span class="eqno">(12)<a class="headerlink" href="#equation-pnorm" title="Permalink to this equation">#</a></span>\[
||w||_p = \left(\sum\limits_{i} |w_i|^p \right)^{\frac{1}{p}}
\]</div>
<p>For Ridge Regression, <em>scikit-learn</em> provides the class <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge">Ridge</a>.</p>
</section>
<section id="lasso">
<h3>Lasso<a class="headerlink" href="#lasso" title="Permalink to this headline">#</a></h3>
<p>Lasso regularisation provides sparse weights. This means that many weights are zero or very close to zero and only the weights which belong to the most important features are non-zero. This sparsity can be achieved by applying the <strong>L1-Norm</strong> as regularisation term.</p>
<div class="math notranslate nohighlight" id="equation-ridge">
<span class="eqno">(13)<a class="headerlink" href="#equation-ridge" title="Permalink to this equation">#</a></span>\[
\mathbf{w}=argmin\left( \sum\limits_{t=1}^N [r_t-g(\mathbf{x}_t|\Theta)]^2 + \lambda \cdot ||w||_1 \right),
\]</div>
<p>For Lasso Regression, <em>scikit-learn</em> provides the class <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso">Lasso</a>.</p>
</section>
<section id="elastic-net">
<h3>Elastic-Net<a class="headerlink" href="#elastic-net" title="Permalink to this headline">#</a></h3>
<p>Elastic-Net regularisation applies a regularisation term, which is a weighted sum of <strong>L1-</strong> and <strong>L2-norm</strong>.</p>
<div class="math notranslate nohighlight" id="equation-elasticnet">
<span class="eqno">(14)<a class="headerlink" href="#equation-elasticnet" title="Permalink to this equation">#</a></span>\[
\mathbf{w}=argmin\left( \sum\limits_{t=1}^N [r_t-g(\mathbf{x}_t|\Theta)]^2 + \lambda \rho ||w||_1  +  \frac{\lambda (1-\rho)}{2} ||w||_2^2 \right),
\]</div>
<p>For Elastic-Net Regression, <em>scikit-learn</em> provides the class <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html#sklearn.linear_model.ElasticNet">ElasticNet</a>.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./machinelearning"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="parametricClassification1D.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Bayes- and Naive Bayes Classifier</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="heartRateRegression.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Example Linear Regression</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Prof. Dr. Johannes Maucher<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>