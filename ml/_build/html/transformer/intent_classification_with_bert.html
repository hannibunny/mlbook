
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Intent Classification with BERT &#8212; Machine Learning Lecture</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.5f77b4aec8189eecf79907ce328c390d.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="References" href="../referenceSection.html" />
    <link rel="prev" title="Sequence-To-Sequence, Attention, Transformer" href="attention.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      <img src="../_static/hdmlogomed.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Machine Learning Lecture</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Intro and Overview Machine Learning Lecture
  </a>
 </li>
</ul>
<p>
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../00BasicConcepts.html">
   Basic Concepts of Data Mining and Machine Learning
  </a>
 </li>
</ul>
<p>
 <span class="caption-text">
  Conventional ML
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../machinelearning/knn.html">
   K - Nearest Neighbour Classification / Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machinelearning/parametricClassification1D.html">
   Bayes- and Naive Bayes Classifier
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machinelearning/LinReg.html">
   Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machinelearning/heartRateRegression.html">
   Example Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machinelearning/LinearClassification.html">
   Linear Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machinelearning/svm.html">
   Support Vector Machines (SVM)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machinelearning/gp.html">
   Gaussian Process
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machinelearning/GaussianProcessRegression.html">
   Gaussian Process: Implementation in Python
  </a>
 </li>
</ul>
<p>
 <span class="caption-text">
  Neural Networks
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../neuralnetworks/01NeuralNets.html">
   Neural Networks Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../neuralnetworks/02RecurrentNeuralNetworks.html">
   Recurrent Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../neuralnetworks/03ConvolutionNeuralNetworks.html">
   Convolutional Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../neuralnetworks/convolutionDemos.html">
   Animations of Convolution and Deconvolution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../neuralnetworks/03KerasMLPandCNNcifar.html">
   Implementing Neural Networks with Keras
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../neuralnetworks/04KerasPretrainedClassifiers.html">
   Applying Pretrained Deep Neural Networks for Image Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../neuralnetworks/05KerasPretrainedCovid.html">
   Apply Pretrained Neural Networks on new Task
  </a>
 </li>
</ul>
<p>
 <span class="caption-text">
  Autoencoder
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../neuralnetworks/04VariationalAutoencoder.html">
   Variational Autoencoder (VAE) to generate handwritten digits
  </a>
 </li>
</ul>
<p>
 <span class="caption-text">
  GAN
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../gan/GAN.html">
   Generative Adversarial Nets (GAN)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../gan/DCGAN.html">
   DCGAN Keras Implementation
  </a>
 </li>
</ul>
<p>
 <span class="caption-text">
  Reinforcement Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../rl/reinforcement.html">
   Reinforcement Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../rl/DQN.html">
   Deep Reinforcement Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../rl/QLearnFrozenLake.html">
   Example Q-Learning
  </a>
 </li>
</ul>
<p>
 <span class="caption-text">
  Text
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../text/01ModellingWordsAndTexts.html">
   Representations for Words and Texts
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../text/02TextClassification.html">
   Text classification with CNNs and LSTMs
  </a>
 </li>
</ul>
<p>
 <span class="caption-text">
  Transformer
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="attention.html">
   Sequence-To-Sequence, Attention, Transformer
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Intent Classification with BERT
  </a>
 </li>
</ul>
<p>
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../referenceSection.html">
   References
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/transformer/intent_classification_with_bert.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/transformer/intent_classification_with_bert.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-you-will-learn">
   What you will learn
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#about-bert">
   About BERT
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#setup">
   Setup
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#data-access">
   Data Access
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#loading-models-from-tensorflow-hub">
   Loading models from TensorFlow Hub
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-preprocessing-model">
   The preprocessing model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#using-the-bert-model">
   Using the BERT model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#define-your-model">
   Define your model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-training">
   Model training
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#loading-the-bert-model-and-training">
     Loading the BERT model and training
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#evaluate-the-model">
     Evaluate the model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#plot-the-accuracy-and-loss-over-time">
     Plot the accuracy and loss over time
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <section id="intent-classification-with-bert">
<h1>Intent Classification with BERT<a class="headerlink" href="#intent-classification-with-bert" title="Permalink to this headline">¶</a></h1>
<p>This notebook demonstrates the fine-tuning of BERT to perform intent classification.
Intent classification tries to map given instructions (sentence in natural language) to a set of predefined intents.</p>
<section id="what-you-will-learn">
<h2>What you will learn<a class="headerlink" href="#what-you-will-learn" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Load data from csv and preprocess it for training and test</p></li>
<li><p>Load a BERT model from TensorFlow Hub</p></li>
<li><p>Build your own model by combining BERT with a classifier</p></li>
<li><p>Train your own model, fine-tuning BERT as part of that</p></li>
<li><p>Save your model and use it to recognize the intend of instructions</p></li>
</ul>
</section>
<section id="about-bert">
<h2>About BERT<a class="headerlink" href="#about-bert" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://arxiv.org/abs/1810.04805">BERT</a> and other Transformer encoder architectures have been shown to be successful on a variety of tasks in NLP (natural language processing). They compute vector-space representations of natural language that are suitable for use in deep learning models. The BERT family of models uses the Transformer encoder architecture to process each token of input text in the full context of all tokens before and after, hence the name: Bidirectional Encoder Representations from Transformers.</p>
<p>BERT models are usually pre-trained on a large corpus of text, then fine-tuned for specific tasks.</p>
</section>
<section id="setup">
<h2>Setup<a class="headerlink" href="#setup" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Required to preprocess text for BERT inputs</span>
<span class="o">!</span>pip install -q tensorflow-text<span class="o">==</span><span class="m">2</span>.6.0
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="c1">#import shutil</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">tensorflow_hub</span> <span class="k">as</span> <span class="nn">hub</span>
<span class="kn">import</span> <span class="nn">tensorflow_text</span> <span class="k">as</span> <span class="nn">text</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">pylab</span> <span class="kn">import</span> <span class="n">rcParams</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">tf</span><span class="o">.</span><span class="n">get_logger</span><span class="p">()</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="s1">&#39;ERROR&#39;</span><span class="p">)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="s1">&#39;whitegrid&#39;</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="s1">&#39;muted&#39;</span><span class="p">,</span> <span class="n">font_scale</span><span class="o">=</span><span class="mf">1.2</span><span class="p">)</span>
<span class="n">HAPPY_COLORS_PALETTE</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;#01BEFE&quot;</span><span class="p">,</span> <span class="s2">&quot;#FFDD00&quot;</span><span class="p">,</span> <span class="s2">&quot;#FF7D00&quot;</span><span class="p">,</span> <span class="s2">&quot;#FF006D&quot;</span><span class="p">,</span> <span class="s2">&quot;#ADFF02&quot;</span><span class="p">,</span> <span class="s2">&quot;#8F00FF&quot;</span><span class="p">]</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_palette</span><span class="p">(</span><span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">(</span><span class="n">HAPPY_COLORS_PALETTE</span><span class="p">))</span>
<span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">8</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ImportError</span><span class="g g-Whitespace">                               </span>Traceback (most recent call last)
<span class="nn">Input In [2],</span> in <span class="ni">&lt;cell line: 5&gt;</span><span class="nt">()</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="c1">#import shutil</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> <span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="ne">----&gt; </span><span class="mi">5</span> <span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="g g-Whitespace">      </span><span class="mi">6</span> <span class="kn">import</span> <span class="nn">tensorflow_hub</span> <span class="k">as</span> <span class="nn">hub</span>
<span class="g g-Whitespace">      </span><span class="mi">7</span> <span class="kn">import</span> <span class="nn">tensorflow_text</span> <span class="k">as</span> <span class="nn">text</span>

<span class="n">File</span> <span class="o">~/</span><span class="n">opt</span><span class="o">/</span><span class="n">anaconda3</span><span class="o">/</span><span class="n">envs</span><span class="o">/</span><span class="n">books</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">python3</span><span class="mf">.8</span><span class="o">/</span><span class="n">site</span><span class="o">-</span><span class="n">packages</span><span class="o">/</span><span class="n">tensorflow</span><span class="o">/</span><span class="fm">__init__</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">41</span><span class="p">,</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="g g-Whitespace">     </span><span class="mi">38</span> <span class="kn">import</span> <span class="nn">six</span> <span class="k">as</span> <span class="nn">_six</span>
<span class="g g-Whitespace">     </span><span class="mi">39</span> <span class="kn">import</span> <span class="nn">sys</span> <span class="k">as</span> <span class="nn">_sys</span>
<span class="ne">---&gt; </span><span class="mi">41</span> <span class="kn">from</span> <span class="nn">tensorflow.python.tools</span> <span class="kn">import</span> <span class="n">module_util</span> <span class="k">as</span> <span class="n">_module_util</span>
<span class="g g-Whitespace">     </span><span class="mi">42</span> <span class="kn">from</span> <span class="nn">tensorflow.python.util.lazy_loader</span> <span class="kn">import</span> <span class="n">LazyLoader</span> <span class="k">as</span> <span class="n">_LazyLoader</span>
<span class="g g-Whitespace">     </span><span class="mi">44</span> <span class="c1"># Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.</span>

<span class="n">File</span> <span class="o">~/</span><span class="n">opt</span><span class="o">/</span><span class="n">anaconda3</span><span class="o">/</span><span class="n">envs</span><span class="o">/</span><span class="n">books</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">python3</span><span class="mf">.8</span><span class="o">/</span><span class="n">site</span><span class="o">-</span><span class="n">packages</span><span class="o">/</span><span class="n">tensorflow</span><span class="o">/</span><span class="n">python</span><span class="o">/</span><span class="fm">__init__</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">40</span><span class="p">,</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="g g-Whitespace">     </span><span class="mi">31</span> <span class="kn">import</span> <span class="nn">traceback</span>
<span class="g g-Whitespace">     </span><span class="mi">33</span> <span class="c1"># We aim to keep this file minimal and ideally remove completely.</span>
<span class="g g-Whitespace">     </span><span class="mi">34</span> <span class="c1"># If you are adding a new file with @tf_export decorators,</span>
<span class="g g-Whitespace">     </span><span class="mi">35</span> <span class="c1"># import it in modules_with_exports.py instead.</span>
<span class="g g-Whitespace">     </span><span class="mi">36</span> 
<span class="g g-Whitespace">     </span><span class="mi">37</span> <span class="c1"># go/tf-wildcard-import</span>
<span class="g g-Whitespace">     </span><span class="mi">38</span> <span class="c1"># pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top</span>
<span class="ne">---&gt; </span><span class="mi">40</span> <span class="kn">from</span> <span class="nn">tensorflow.python.eager</span> <span class="kn">import</span> <span class="n">context</span>
<span class="g g-Whitespace">     </span><span class="mi">41</span> <span class="kn">from</span> <span class="nn">tensorflow.python</span> <span class="kn">import</span> <span class="n">pywrap_tensorflow</span> <span class="k">as</span> <span class="n">_pywrap_tensorflow</span>
<span class="g g-Whitespace">     </span><span class="mi">43</span> <span class="c1"># pylint: enable=wildcard-import</span>
<span class="g g-Whitespace">     </span><span class="mi">44</span> 
<span class="g g-Whitespace">     </span><span class="mi">45</span> <span class="c1"># Bring in subpackages.</span>

<span class="n">File</span> <span class="o">~/</span><span class="n">opt</span><span class="o">/</span><span class="n">anaconda3</span><span class="o">/</span><span class="n">envs</span><span class="o">/</span><span class="n">books</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">python3</span><span class="mf">.8</span><span class="o">/</span><span class="n">site</span><span class="o">-</span><span class="n">packages</span><span class="o">/</span><span class="n">tensorflow</span><span class="o">/</span><span class="n">python</span><span class="o">/</span><span class="n">eager</span><span class="o">/</span><span class="n">context</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">35</span><span class="p">,</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="g g-Whitespace">     </span><span class="mi">33</span> <span class="kn">from</span> <span class="nn">tensorflow.core.protobuf</span> <span class="kn">import</span> <span class="n">config_pb2</span>
<span class="g g-Whitespace">     </span><span class="mi">34</span> <span class="kn">from</span> <span class="nn">tensorflow.core.protobuf</span> <span class="kn">import</span> <span class="n">rewriter_config_pb2</span>
<span class="ne">---&gt; </span><span class="mi">35</span> <span class="kn">from</span> <span class="nn">tensorflow.python</span> <span class="kn">import</span> <span class="n">pywrap_tfe</span>
<span class="g g-Whitespace">     </span><span class="mi">36</span> <span class="kn">from</span> <span class="nn">tensorflow.python</span> <span class="kn">import</span> <span class="n">tf2</span>
<span class="g g-Whitespace">     </span><span class="mi">37</span> <span class="kn">from</span> <span class="nn">tensorflow.python.client</span> <span class="kn">import</span> <span class="n">pywrap_tf_session</span>

<span class="n">File</span> <span class="o">~/</span><span class="n">opt</span><span class="o">/</span><span class="n">anaconda3</span><span class="o">/</span><span class="n">envs</span><span class="o">/</span><span class="n">books</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">python3</span><span class="mf">.8</span><span class="o">/</span><span class="n">site</span><span class="o">-</span><span class="n">packages</span><span class="o">/</span><span class="n">tensorflow</span><span class="o">/</span><span class="n">python</span><span class="o">/</span><span class="n">pywrap_tfe</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">29</span><span class="p">,</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="g g-Whitespace">     </span><span class="mi">27</span> <span class="c1"># pylint: disable=invalid-import-order,g-bad-import-order, wildcard-import, unused-import</span>
<span class="g g-Whitespace">     </span><span class="mi">28</span> <span class="kn">from</span> <span class="nn">tensorflow.python</span> <span class="kn">import</span> <span class="n">pywrap_tensorflow</span>
<span class="ne">---&gt; </span><span class="mi">29</span> <span class="kn">from</span> <span class="nn">tensorflow.python._pywrap_tfe</span> <span class="kn">import</span> <span class="o">*</span>

<span class="ne">ImportError</span>: dlopen(/Users/johannes/opt/anaconda3/envs/books/lib/python3.8/site-packages/tensorflow/python/_pywrap_tfe.so, 0x0002): Library not loaded: @rpath/_pywrap_tensorflow_internal.so
  <span class="n">Referenced</span> <span class="n">from</span><span class="p">:</span> <span class="o">/</span><span class="n">Users</span><span class="o">/</span><span class="n">johannes</span><span class="o">/</span><span class="n">opt</span><span class="o">/</span><span class="n">anaconda3</span><span class="o">/</span><span class="n">envs</span><span class="o">/</span><span class="n">books</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">python3</span><span class="mf">.8</span><span class="o">/</span><span class="n">site</span><span class="o">-</span><span class="n">packages</span><span class="o">/</span><span class="n">tensorflow</span><span class="o">/</span><span class="n">python</span><span class="o">/</span><span class="n">_pywrap_tfe</span><span class="o">.</span><span class="n">so</span>
  <span class="n">Reason</span><span class="p">:</span> <span class="n">tried</span><span class="p">:</span> <span class="s1">&#39;/Users/johannes/opt/anaconda3/envs/books/lib/python3.8/site-packages/tensorflow/python/../../_solib_darwin_x86_64/_U_S_Stensorflow_Spython_C_Upywrap_Utfe.so___Utensorflow/_pywrap_tensorflow_internal.so&#39;</span> <span class="p">(</span><span class="n">no</span> <span class="n">such</span> <span class="n">file</span><span class="p">),</span> <span class="s1">&#39;/Users/johannes/opt/anaconda3/envs/books/lib/python3.8/site-packages/tensorflow/python/../../_solib_darwin_x86_64/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal_Umacos___Utensorflow_Spython/_pywrap_tensorflow_internal.so&#39;</span> <span class="p">(</span><span class="n">no</span> <span class="n">such</span> <span class="n">file</span><span class="p">),</span> <span class="s1">&#39;/Users/johannes/opt/anaconda3/envs/books/lib/python3.8/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so&#39;</span> <span class="p">(</span><span class="n">no</span> <span class="n">such</span> <span class="n">file</span><span class="p">),</span> <span class="s1">&#39;/Users/johannes/opt/anaconda3/envs/books/lib/python3.8/site-packages/tensorflow/python/../_pywrap_tensorflow_internal.so&#39;</span> <span class="p">(</span><span class="n">no</span> <span class="n">such</span> <span class="n">file</span><span class="p">),</span> <span class="s1">&#39;/Users/johannes/opt/anaconda3/envs/books/lib/python3.8/site-packages/tensorflow/python/../../_solib_darwin_x86_64/_U_S_Stensorflow_Spython_C_Upywrap_Utfe.so___Utensorflow/_pywrap_tensorflow_internal.so&#39;</span> <span class="p">(</span><span class="n">no</span> <span class="n">such</span> <span class="n">file</span><span class="p">),</span> <span class="s1">&#39;/Users/johannes/opt/anaconda3/envs/books/lib/python3.8/site-packages/tensorflow/python/../../_solib_darwin_x86_64/_U_S_Stensorflow_Spython_C_Upywrap_Utensorflow_Uinternal_Umacos___Utensorflow_Spython/_pywrap_tensorflow_internal.so&#39;</span> <span class="p">(</span><span class="n">no</span> <span class="n">such</span> <span class="n">file</span><span class="p">),</span> <span class="s1">&#39;/Users/johannes/opt/anaconda3/envs/books/lib/python3.8/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so&#39;</span> <span class="p">(</span><span class="n">no</span> <span class="n">such</span> <span class="n">file</span><span class="p">),</span> <span class="s1">&#39;/Users/johannes/opt/anaconda3/envs/books/lib/python3.8/site-packages/tensorflow/python/../_pywrap_tensorflow_internal.so&#39;</span> <span class="p">(</span><span class="n">no</span> <span class="n">such</span> <span class="n">file</span><span class="p">),</span> <span class="s1">&#39;/Users/johannes/opt/anaconda3/envs/books/lib/_pywrap_tensorflow_internal.so&#39;</span> <span class="p">(</span><span class="n">no</span> <span class="n">such</span> <span class="n">file</span><span class="p">),</span> <span class="s1">&#39;/Users/johannes/opt/anaconda3/envs/books/bin/../lib/_pywrap_tensorflow_internal.so&#39;</span> <span class="p">(</span><span class="n">no</span> <span class="n">such</span> <span class="n">file</span><span class="p">),</span> <span class="s1">&#39;/Users/johannes/opt/anaconda3/envs/books/lib/_pywrap_tensorflow_internal.so&#39;</span> <span class="p">(</span><span class="n">no</span> <span class="n">such</span> <span class="n">file</span><span class="p">),</span> <span class="s1">&#39;/Users/johannes/opt/anaconda3/envs/books/bin/../lib/_pywrap_tensorflow_internal.so&#39;</span> <span class="p">(</span><span class="n">no</span> <span class="n">such</span> <span class="n">file</span><span class="p">),</span> <span class="s1">&#39;/usr/local/lib/_pywrap_tensorflow_internal.so&#39;</span> <span class="p">(</span><span class="n">no</span> <span class="n">such</span> <span class="n">file</span><span class="p">),</span> <span class="s1">&#39;/usr/lib/_pywrap_tensorflow_internal.so&#39;</span> <span class="p">(</span><span class="n">no</span> <span class="n">such</span> <span class="n">file</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="data-access">
<h2>Data Access<a class="headerlink" href="#data-access" title="Permalink to this headline">¶</a></h2>
<p>The data contains various user queries categorized into seven intents. It is hosted on <a class="reference external" href="https://github.com/snipsco/nlu-benchmark/tree/master/2017-06-custom-intent-engines">GitHub</a> and is first presented in <a class="reference external" href="https://arxiv.org/abs/1805.10190">this paper</a>. In the list below the classes and an example for each class is given:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">class</span></code>: SearchCreativeWork - <code class="docutils literal notranslate"><span class="pre">example</span></code>:<em>play hell house song</em></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">class</span></code>: GetWeather - <code class="docutils literal notranslate"><span class="pre">example</span></code>: <em>is it windy in boston, mb right now</em></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">class</span></code>: BookRestaurant - <code class="docutils literal notranslate"><span class="pre">example</span></code>: <em>book a restaurant for eight people in six years</em></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">class</span></code>: PlayMusic - <code class="docutils literal notranslate"><span class="pre">example</span></code>: <em>play the song little robin redbreast</em></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">class</span></code>: AddToPlaylist - <code class="docutils literal notranslate"><span class="pre">example</span></code>: <em>add step to me to the 50 clásicos playlist</em></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">class</span></code>: RateBook - <code class="docutils literal notranslate"><span class="pre">example</span></code>: <em>give 6 stars to of mice and men</em></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">class</span></code>: SearchScreeningEvent - <code class="docutils literal notranslate"><span class="pre">example</span></code> : <em>find fish story</em></p></li>
</ul>
<p>Data can be downloaded from a Google Drive by applying <a class="reference external" href="https://pypi.org/project/gdown/">gdown</a>. In the following code cells the download is invoked only if the corresponding file, does not yet exist at the corresponding location.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">datafolder</span><span class="o">=</span><span class="s2">&quot;/Users/johannes/DataSets/IntentClassification/&quot;</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">trainfile</span><span class="o">=</span><span class="n">datafolder</span><span class="o">+</span><span class="s2">&quot;train.csv&quot;</span>
<span class="n">testfile</span><span class="o">=</span><span class="n">datafolder</span><span class="o">+</span><span class="s2">&quot;test.csv&quot;</span>
<span class="n">validfile</span><span class="o">=</span><span class="n">datafolder</span><span class="o">+</span><span class="s2">&quot;valid.csv&quot;</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="ch">#!pip install gdown</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">trainfile</span><span class="p">):</span>
    <span class="o">!</span>gdown --id 1OlcvGWReJMuyYQuOZm149vHWwPtlboR6 --output /Users/johannes/DataSets/IntentClassification/train.csv
<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">validfile</span><span class="p">):</span>
    <span class="o">!</span>gdown --id 1Oi5cRlTybuIF2Fl5Bfsr-KkqrXrdt77w --output /Users/johannes/DataSets/IntentClassification/valid.csv
<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">testfile</span><span class="p">):</span>
    <span class="o">!</span>gdown --id 1ep9H6-HvhB4utJRLVcLzieWNUSG3P_uF --output /Users/johannes/DataSets/IntentClassification/test.csv
</pre></div>
</div>
</div>
</div>
<p>Next, the downloaded .csv-files for training, validation and test are imported into pandas dataframes:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">traindf</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">trainfile</span><span class="p">)</span>
<span class="n">validdf</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">validfile</span><span class="p">)</span>
<span class="n">testdf</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">testfile</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">traindf</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>intent</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>listen to westbam alumb allergic on google music</td>
      <td>PlayMusic</td>
    </tr>
    <tr>
      <th>1</th>
      <td>add step to me to the 50 clásicos playlist</td>
      <td>AddToPlaylist</td>
    </tr>
    <tr>
      <th>2</th>
      <td>i give this current textbook a rating value of...</td>
      <td>RateBook</td>
    </tr>
    <tr>
      <th>3</th>
      <td>play the song little robin redbreast</td>
      <td>PlayMusic</td>
    </tr>
    <tr>
      <th>4</th>
      <td>please add iris dement to my playlist this is ...</td>
      <td>AddToPlaylist</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Training data contains 13084 instructions:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">traindf</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(13084, 2)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">trainfeatures</span><span class="o">=</span><span class="n">traindf</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">trainlabels</span><span class="o">=</span><span class="n">trainfeatures</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;intent&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">trainfeatures</span><span class="o">=</span><span class="n">trainfeatures</span><span class="o">.</span><span class="n">values</span>
</pre></div>
</div>
</div>
</div>
<p>Distribution of class-labels in training-data:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">chart</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">countplot</span><span class="p">(</span><span class="n">trainlabels</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="n">HAPPY_COLORS_PALETTE</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Number of texts per intent&quot;</span><span class="p">)</span>
<span class="n">chart</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">chart</span><span class="o">.</span><span class="n">get_xticklabels</span><span class="p">(),</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">horizontalalignment</span><span class="o">=</span><span class="s1">&#39;right&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/intent_classification_with_bert_19_0.png" src="../_images/intent_classification_with_bert_19_0.png" />
</div>
</div>
<p>One-Hot-Encoding of class-labels:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelBinarizer</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">binarizer</span><span class="o">=</span><span class="n">LabelBinarizer</span><span class="p">()</span>
<span class="n">trainlabels</span><span class="o">=</span><span class="n">binarizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">trainlabels</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">trainlabels</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(13084, 7)
</pre></div>
</div>
</div>
</div>
<p>Preprocess test- and validation data in the same way as it has been done for training-data:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">testfeatures</span><span class="o">=</span><span class="n">testdf</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">testlabels</span><span class="o">=</span><span class="n">testfeatures</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;intent&quot;</span><span class="p">)</span>
<span class="n">validfeatures</span><span class="o">=</span><span class="n">validdf</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">validlabels</span><span class="o">=</span><span class="n">validfeatures</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;intent&quot;</span><span class="p">)</span>

<span class="n">testfeatures</span><span class="o">=</span><span class="n">testfeatures</span><span class="o">.</span><span class="n">values</span>
<span class="n">validfeatures</span><span class="o">=</span><span class="n">validfeatures</span><span class="o">.</span><span class="n">values</span>

<span class="n">testlabels</span><span class="o">=</span><span class="n">binarizer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">testlabels</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">validlabels</span><span class="o">=</span><span class="n">binarizer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">validlabels</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="loading-models-from-tensorflow-hub">
<h2>Loading models from TensorFlow Hub<a class="headerlink" href="#loading-models-from-tensorflow-hub" title="Permalink to this headline">¶</a></h2>
<p>Here you can choose which BERT model you will load from TensorFlow Hub and fine-tune. There are multiple BERT models available.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3">BERT-Base</a>, <a class="reference external" href="https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3">Uncased</a> and <a class="reference external" href="https://tfhub.dev/google/collections/bert/1">seven more models</a> with trained weights released by the original BERT authors.</p></li>
<li><p><a class="reference external" href="https://tfhub.dev/google/collections/bert/1">Small BERTs</a> have the same general architecture but fewer and/or smaller Transformer blocks, which lets you explore tradeoffs between speed, size and quality.</p></li>
<li><p><a class="reference external" href="https://tfhub.dev/google/collections/albert/1">ALBERT</a>: four different sizes of “A Lite BERT” that reduces model size (but not computation time) by sharing parameters between layers.</p></li>
<li><p><a class="reference external" href="https://tfhub.dev/google/collections/experts/bert/1">BERT Experts</a>: eight models that all have the BERT-base architecture but offer a choice between different pre-training domains, to align more closely with the target task.</p></li>
<li><p><a class="reference external" href="https://tfhub.dev/google/collections/electra/1">Electra</a> has the same architecture as BERT (in three different sizes), but gets pre-trained as a discriminator in a set-up that resembles a Generative Adversarial Network (GAN).</p></li>
<li><p>BERT with Talking-Heads Attention and Gated GELU [<a class="reference external" href="https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1">base</a>, <a class="reference external" href="https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_large/1">large</a>] has two improvements to the core of the Transformer architecture.</p></li>
</ul>
<p>The model documentation on TensorFlow Hub has more details and references to the
research literature. Follow the links above, or click on the <a class="reference external" href="http://tfhub.dev"><code class="docutils literal notranslate"><span class="pre">tfhub.dev</span></code></a> URL
printed after the next cell execution.</p>
<p>The suggestion is to start with a Small BERT (with fewer parameters) since they are faster to fine-tune. If you like a small model but with higher accuracy, ALBERT might be your next option. If you want even better accuracy, choose
one of the classic BERT sizes or their recent refinements like Electra, Talking Heads, or a BERT Expert.</p>
<p>Aside from the models available below, there are <a class="reference external" href="https://tfhub.dev/google/collections/transformer_encoders_text/1">multiple versions</a> of the models that are larger and can yield even better accuracy but they are too big to be fine-tuned on a single GPU. You will be able to do that on the <a class="reference external" href="https://www.tensorflow.org/tutorials/text/solve_glue_tasks_using_bert_on_tpu">Solve GLUE tasks using BERT on a TPU colab</a>.</p>
<p>You’ll see in the code below that switching the tfhub.dev URL is enough to try any of these models, because all the differences between them are encapsulated in the SavedModels from TF Hub.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bert_model_name</span> <span class="o">=</span> <span class="s1">&#39;small_bert/bert_en_uncased_L-8_H-512_A-8&#39;</span> 
<span class="n">map_name_to_handle</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;bert_en_uncased_L-12_H-768_A-12&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3&#39;</span><span class="p">,</span>
    <span class="s1">&#39;bert_en_cased_L-12_H-768_A-12&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3&#39;</span><span class="p">,</span>
    <span class="s1">&#39;bert_multi_cased_L-12_H-768_A-12&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3&#39;</span><span class="p">,</span>
    <span class="s1">&#39;small_bert/bert_en_uncased_L-2_H-128_A-2&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1&#39;</span><span class="p">,</span>
    <span class="s1">&#39;small_bert/bert_en_uncased_L-2_H-256_A-4&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1&#39;</span><span class="p">,</span>
    <span class="s1">&#39;small_bert/bert_en_uncased_L-2_H-512_A-8&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1&#39;</span><span class="p">,</span>
    <span class="s1">&#39;small_bert/bert_en_uncased_L-2_H-768_A-12&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1&#39;</span><span class="p">,</span>
    <span class="s1">&#39;small_bert/bert_en_uncased_L-4_H-128_A-2&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1&#39;</span><span class="p">,</span>
    <span class="s1">&#39;small_bert/bert_en_uncased_L-4_H-256_A-4&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1&#39;</span><span class="p">,</span>
    <span class="s1">&#39;small_bert/bert_en_uncased_L-4_H-512_A-8&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1&#39;</span><span class="p">,</span>
    <span class="s1">&#39;small_bert/bert_en_uncased_L-4_H-768_A-12&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1&#39;</span><span class="p">,</span>
    <span class="s1">&#39;small_bert/bert_en_uncased_L-6_H-128_A-2&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1&#39;</span><span class="p">,</span>
    <span class="s1">&#39;small_bert/bert_en_uncased_L-6_H-256_A-4&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1&#39;</span><span class="p">,</span>
    <span class="s1">&#39;small_bert/bert_en_uncased_L-6_H-512_A-8&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1&#39;</span><span class="p">,</span>
    <span class="s1">&#39;small_bert/bert_en_uncased_L-6_H-768_A-12&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1&#39;</span><span class="p">,</span>
    <span class="s1">&#39;small_bert/bert_en_uncased_L-8_H-128_A-2&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1&#39;</span><span class="p">,</span>
    <span class="s1">&#39;small_bert/bert_en_uncased_L-8_H-256_A-4&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1&#39;</span><span class="p">,</span>
    <span class="s1">&#39;small_bert/bert_en_uncased_L-8_H-512_A-8&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1&#39;</span><span class="p">,</span>
    <span class="s1">&#39;small_bert/bert_en_uncased_L-8_H-768_A-12&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1&#39;</span><span class="p">,</span>
    <span class="s1">&#39;small_bert/bert_en_uncased_L-10_H-128_A-2&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1&#39;</span><span class="p">,</span>
    <span class="s1">&#39;small_bert/bert_en_uncased_L-10_H-256_A-4&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1&#39;</span><span class="p">,</span>
    <span class="s1">&#39;small_bert/bert_en_uncased_L-10_H-512_A-8&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1&#39;</span><span class="p">,</span>
    <span class="s1">&#39;small_bert/bert_en_uncased_L-10_H-768_A-12&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1&#39;</span><span class="p">,</span>
    <span class="s1">&#39;small_bert/bert_en_uncased_L-12_H-128_A-2&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1&#39;</span><span class="p">,</span>
    <span class="s1">&#39;small_bert/bert_en_uncased_L-12_H-256_A-4&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1&#39;</span><span class="p">,</span>
    <span class="s1">&#39;small_bert/bert_en_uncased_L-12_H-512_A-8&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1&#39;</span><span class="p">,</span>
    <span class="s1">&#39;small_bert/bert_en_uncased_L-12_H-768_A-12&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1&#39;</span><span class="p">,</span>
    <span class="s1">&#39;albert_en_base&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/albert_en_base/2&#39;</span><span class="p">,</span>
    <span class="s1">&#39;electra_small&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/google/electra_small/2&#39;</span><span class="p">,</span>
    <span class="s1">&#39;electra_base&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/google/electra_base/2&#39;</span><span class="p">,</span>
    <span class="s1">&#39;experts_pubmed&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/google/experts/bert/pubmed/2&#39;</span><span class="p">,</span>
    <span class="s1">&#39;experts_wiki_books&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/google/experts/bert/wiki_books/2&#39;</span><span class="p">,</span>
    <span class="s1">&#39;talking-heads_base&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1&#39;</span><span class="p">,</span>
<span class="p">}</span>

<span class="n">map_model_to_preprocess</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;bert_en_uncased_L-12_H-768_A-12&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2&#39;</span><span class="p">,</span>
    <span class="s1">&#39;bert_en_cased_L-12_H-768_A-12&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/bert_en_cased_preprocess/2&#39;</span><span class="p">,</span>
    <span class="s1">&#39;small_bert/bert_en_uncased_L-2_H-128_A-2&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2&#39;</span><span class="p">,</span>
    <span class="s1">&#39;small_bert/bert_en_uncased_L-2_H-256_A-4&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2&#39;</span><span class="p">,</span>
    <span class="s1">&#39;small_bert/bert_en_uncased_L-2_H-512_A-8&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2&#39;</span><span class="p">,</span>
    <span class="s1">&#39;small_bert/bert_en_uncased_L-2_H-768_A-12&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2&#39;</span><span class="p">,</span>
    <span class="s1">&#39;small_bert/bert_en_uncased_L-4_H-128_A-2&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2&#39;</span><span class="p">,</span>
    <span class="s1">&#39;small_bert/bert_en_uncased_L-4_H-256_A-4&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2&#39;</span><span class="p">,</span>
    <span class="s1">&#39;small_bert/bert_en_uncased_L-4_H-512_A-8&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2&#39;</span><span class="p">,</span>
    <span class="s1">&#39;small_bert/bert_en_uncased_L-4_H-768_A-12&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2&#39;</span><span class="p">,</span>
    <span class="s1">&#39;small_bert/bert_en_uncased_L-6_H-128_A-2&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2&#39;</span><span class="p">,</span>
    <span class="s1">&#39;small_bert/bert_en_uncased_L-6_H-256_A-4&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2&#39;</span><span class="p">,</span>
    <span class="s1">&#39;small_bert/bert_en_uncased_L-6_H-512_A-8&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2&#39;</span><span class="p">,</span>
    <span class="s1">&#39;small_bert/bert_en_uncased_L-6_H-768_A-12&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2&#39;</span><span class="p">,</span>
    <span class="s1">&#39;small_bert/bert_en_uncased_L-8_H-128_A-2&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2&#39;</span><span class="p">,</span>
    <span class="s1">&#39;small_bert/bert_en_uncased_L-8_H-256_A-4&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2&#39;</span><span class="p">,</span>
    <span class="s1">&#39;small_bert/bert_en_uncased_L-8_H-512_A-8&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2&#39;</span><span class="p">,</span>
    <span class="s1">&#39;small_bert/bert_en_uncased_L-8_H-768_A-12&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2&#39;</span><span class="p">,</span>
    <span class="s1">&#39;small_bert/bert_en_uncased_L-10_H-128_A-2&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2&#39;</span><span class="p">,</span>
    <span class="s1">&#39;small_bert/bert_en_uncased_L-10_H-256_A-4&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2&#39;</span><span class="p">,</span>
    <span class="s1">&#39;small_bert/bert_en_uncased_L-10_H-512_A-8&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2&#39;</span><span class="p">,</span>
    <span class="s1">&#39;small_bert/bert_en_uncased_L-10_H-768_A-12&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2&#39;</span><span class="p">,</span>
    <span class="s1">&#39;small_bert/bert_en_uncased_L-12_H-128_A-2&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2&#39;</span><span class="p">,</span>
    <span class="s1">&#39;small_bert/bert_en_uncased_L-12_H-256_A-4&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2&#39;</span><span class="p">,</span>
    <span class="s1">&#39;small_bert/bert_en_uncased_L-12_H-512_A-8&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2&#39;</span><span class="p">,</span>
    <span class="s1">&#39;small_bert/bert_en_uncased_L-12_H-768_A-12&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2&#39;</span><span class="p">,</span>
    <span class="s1">&#39;bert_multi_cased_L-12_H-768_A-12&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/2&#39;</span><span class="p">,</span>
    <span class="s1">&#39;albert_en_base&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/albert_en_preprocess/2&#39;</span><span class="p">,</span>
    <span class="s1">&#39;electra_small&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2&#39;</span><span class="p">,</span>
    <span class="s1">&#39;electra_base&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2&#39;</span><span class="p">,</span>
    <span class="s1">&#39;experts_pubmed&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2&#39;</span><span class="p">,</span>
    <span class="s1">&#39;experts_wiki_books&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2&#39;</span><span class="p">,</span>
    <span class="s1">&#39;talking-heads_base&#39;</span><span class="p">:</span>
        <span class="s1">&#39;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2&#39;</span><span class="p">,</span>
<span class="p">}</span>

<span class="n">tfhub_handle_encoder</span> <span class="o">=</span> <span class="n">map_name_to_handle</span><span class="p">[</span><span class="n">bert_model_name</span><span class="p">]</span>
<span class="n">tfhub_handle_preprocess</span> <span class="o">=</span> <span class="n">map_model_to_preprocess</span><span class="p">[</span><span class="n">bert_model_name</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;BERT model selected           : </span><span class="si">{</span><span class="n">tfhub_handle_encoder</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Preprocess model auto-selected: </span><span class="si">{</span><span class="n">tfhub_handle_preprocess</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>BERT model selected           : https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1
Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2
</pre></div>
</div>
</div>
</div>
</section>
<section id="the-preprocessing-model">
<h2>The preprocessing model<a class="headerlink" href="#the-preprocessing-model" title="Permalink to this headline">¶</a></h2>
<p>Text inputs need to be transformed to numeric token ids and arranged in several Tensors before being input to BERT. TensorFlow Hub provides a matching preprocessing model for each of the BERT models discussed above, which implements this transformation using TF ops from the TF.text library. It is not necessary to run pure Python code outside your TensorFlow model to preprocess text.</p>
<p>The preprocessing model must be the one referenced by the documentation of the BERT model, which you can read at the URL printed above. For BERT models from the drop-down above, the preprocessing model is selected automatically.</p>
<p>Note: You will load the preprocessing model into a <a class="reference external" href="https://www.tensorflow.org/hub/api_docs/python/hub/KerasLayer">hub.KerasLayer</a> to compose your fine-tuned model. This is the preferred API to load a TF2-style SavedModel from TF Hub into a Keras model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bert_preprocess_model</span> <span class="o">=</span> <span class="n">hub</span><span class="o">.</span><span class="n">KerasLayer</span><span class="p">(</span><span class="n">tfhub_handle_preprocess</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s try the preprocessing model on some text and see the output:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">trainfeatures</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;listen to westbam alumb allergic on google music&#39;], dtype=object)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">text_test</span> <span class="o">=</span> <span class="n">trainfeatures</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">text_preprocessed</span> <span class="o">=</span> <span class="n">bert_preprocess_model</span><span class="p">(</span><span class="n">text_test</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Keys       : </span><span class="si">{</span><span class="nb">list</span><span class="p">(</span><span class="n">text_preprocessed</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Shape      : </span><span class="si">{</span><span class="n">text_preprocessed</span><span class="p">[</span><span class="s2">&quot;input_word_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Word Ids   : </span><span class="si">{</span><span class="n">text_preprocessed</span><span class="p">[</span><span class="s2">&quot;input_word_ids&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">,</span> <span class="p">:</span><span class="mi">12</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Input Mask : </span><span class="si">{</span><span class="n">text_preprocessed</span><span class="p">[</span><span class="s2">&quot;input_mask&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">,</span> <span class="p">:</span><span class="mi">12</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Type Ids   : </span><span class="si">{</span><span class="n">text_preprocessed</span><span class="p">[</span><span class="s2">&quot;input_type_ids&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">,</span> <span class="p">:</span><span class="mi">12</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Keys       : [&#39;input_word_ids&#39;, &#39;input_mask&#39;, &#39;input_type_ids&#39;]
Shape      : (1, 128)
Word Ids   : [  101  4952  2000  2225  3676  2213  2632 25438 27395  2006  8224  2189]
Input Mask : [1 1 1 1 1 1 1 1 1 1 1 1]
Type Ids   : [0 0 0 0 0 0 0 0 0 0 0 0]
</pre></div>
</div>
</div>
</div>
<p>As can be seen, there are 3 outputs from the preprocessing that a BERT model would use (<code class="docutils literal notranslate"><span class="pre">input_words_id</span></code>, <code class="docutils literal notranslate"><span class="pre">input_mask</span></code> and <code class="docutils literal notranslate"><span class="pre">input_type_ids</span></code>).</p>
<p>Some other important points:</p>
<ul class="simple">
<li><p>The input is truncated to 128 tokens. The number of tokens can be customized and you can see more details on the <a class="reference external" href="https://www.tensorflow.org/tutorials/text/solve_glue_tasks_using_bert_on_tpu">Solve GLUE tasks using BERT on a TPU colab</a>.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">input_type_ids</span></code> only have one value (0) because this is a single sentence input. For a multiple sentence input, it would have one number for each input.</p></li>
</ul>
<p>Since this text preprocessor is a TensorFlow model, It can be included in your model directly.</p>
</section>
<section id="using-the-bert-model">
<h2>Using the BERT model<a class="headerlink" href="#using-the-bert-model" title="Permalink to this headline">¶</a></h2>
<p>Before putting BERT into an own model, let’s take a look at its outputs. You will load it from TF Hub and see the returned values.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bert_model</span> <span class="o">=</span> <span class="n">hub</span><span class="o">.</span><span class="n">KerasLayer</span><span class="p">(</span><span class="n">tfhub_handle_encoder</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bert_results</span> <span class="o">=</span> <span class="n">bert_model</span><span class="p">(</span><span class="n">text_preprocessed</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Loaded BERT: </span><span class="si">{</span><span class="n">tfhub_handle_encoder</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Pooled Outputs Shape:</span><span class="si">{</span><span class="n">bert_results</span><span class="p">[</span><span class="s2">&quot;pooled_output&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Pooled Outputs Values:</span><span class="si">{</span><span class="n">bert_results</span><span class="p">[</span><span class="s2">&quot;pooled_output&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">,</span> <span class="p">:</span><span class="mi">12</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Sequence Outputs Shape:</span><span class="si">{</span><span class="n">bert_results</span><span class="p">[</span><span class="s2">&quot;sequence_output&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Sequence Outputs Values:</span><span class="si">{</span><span class="n">bert_results</span><span class="p">[</span><span class="s2">&quot;sequence_output&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">,</span> <span class="p">:</span><span class="mi">12</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Loaded BERT: https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1
Pooled Outputs Shape:(1, 512)
Pooled Outputs Values:[-0.0496942  -0.16525196 -0.9980706  -0.9327927  -0.6145213  -0.22613096
 -0.9558851  -0.50678325  0.29122883  0.2631647   0.7982282   0.49405974]
Sequence Outputs Shape:(1, 128, 512)
Sequence Outputs Values:[[-0.10247643  0.2220486   0.5988388  ... -0.25584024  0.61985415
  -0.0182253 ]
 [ 0.45503587 -0.5723831   0.55420965 ... -0.28608802  1.3628978
   0.91311985]
 [ 0.42473745  0.29045242  0.82692915 ...  0.28371722  1.7948042
  -0.36674204]
 ...
 [-0.46153072  0.0282942   0.5167359  ... -0.1503543   1.4651562
   0.6449581 ]
 [ 0.71108186  1.0848472   0.66065186 ...  0.47941187  0.7233063
  -0.08312234]
 [ 0.3555895  -0.38904855  0.51018417 ...  0.19970977  0.86474466
   0.12226923]]
</pre></div>
</div>
</div>
</div>
<p>The BERT models return a map with 3 important keys: <code class="docutils literal notranslate"><span class="pre">pooled_output</span></code>, <code class="docutils literal notranslate"><span class="pre">sequence_output</span></code>, <code class="docutils literal notranslate"><span class="pre">encoder_outputs</span></code>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">pooled_output</span></code> to represent each input sequence as a whole. The shape is <code class="docutils literal notranslate"><span class="pre">[batch_size,</span> <span class="pre">H]</span></code>. You can think of this as an embedding for the entire movie review.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sequence_output</span></code> represents each input token in the context. The shape is <code class="docutils literal notranslate"><span class="pre">[batch_size,</span> <span class="pre">seq_length,</span> <span class="pre">H]</span></code>. You can think of this as a contextual embedding for every token in the movie review.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">encoder_outputs</span></code> are the intermediate activations of the <code class="docutils literal notranslate"><span class="pre">L</span></code> Transformer blocks. <code class="docutils literal notranslate"><span class="pre">outputs[&quot;encoder_outputs&quot;][i]</span></code> is a Tensor of shape <code class="docutils literal notranslate"><span class="pre">[batch_size,</span> <span class="pre">seq_length,</span> <span class="pre">1024]</span></code> with the outputs of the i-th Transformer block, for <code class="docutils literal notranslate"><span class="pre">0</span> <span class="pre">&lt;=</span> <span class="pre">i</span> <span class="pre">&lt;</span> <span class="pre">L</span></code>. The last value of the list is equal to <code class="docutils literal notranslate"><span class="pre">sequence_output</span></code>.</p></li>
</ul>
<p>For the fine-tuning you are going to use the <code class="docutils literal notranslate"><span class="pre">pooled_output</span></code> array.</p>
</section>
<section id="define-your-model">
<h2>Define your model<a class="headerlink" href="#define-your-model" title="Permalink to this headline">¶</a></h2>
<p>You will create a very simple fine-tuned model, with the preprocessing model, the selected BERT model, one Dense and a Dropout layer.</p>
<p>Note: for more information about the base model’s input and output you can use just follow the model’s url for documentation. Here specifically you don’t need to worry about it because the preprocessing model will take care of that for you.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">build_classifier_model</span><span class="p">():</span>
  <span class="n">text_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">string</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;text&#39;</span><span class="p">)</span>
  <span class="n">preprocessing_layer</span> <span class="o">=</span> <span class="n">hub</span><span class="o">.</span><span class="n">KerasLayer</span><span class="p">(</span><span class="n">tfhub_handle_preprocess</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;preprocessing&#39;</span><span class="p">)</span>
  <span class="n">encoder_inputs</span> <span class="o">=</span> <span class="n">preprocessing_layer</span><span class="p">(</span><span class="n">text_input</span><span class="p">)</span>
  <span class="n">encoder</span> <span class="o">=</span> <span class="n">hub</span><span class="o">.</span><span class="n">KerasLayer</span><span class="p">(</span><span class="n">tfhub_handle_encoder</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;BERT_encoder&#39;</span><span class="p">)</span>
  <span class="n">outputs</span> <span class="o">=</span> <span class="n">encoder</span><span class="p">(</span><span class="n">encoder_inputs</span><span class="p">)</span>
  <span class="n">net</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="s1">&#39;pooled_output&#39;</span><span class="p">]</span>
  <span class="n">net</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)(</span><span class="n">net</span><span class="p">)</span>
  <span class="n">net</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;classifier&#39;</span><span class="p">)(</span><span class="n">net</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">text_input</span><span class="p">,</span> <span class="n">net</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s check that the model runs with the output of the preprocessing model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">classifier_model</span> <span class="o">=</span> <span class="n">build_classifier_model</span><span class="p">()</span>
<span class="n">bert_raw_result</span> <span class="o">=</span> <span class="n">classifier_model</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">trainfeatures</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">activations</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">bert_raw_result</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tf.Tensor(
[[0.51704866 0.09932686 0.23939744 0.0246855  0.02893808 0.04027716
  0.05032637]], shape=(1, 7), dtype=float32)
</pre></div>
</div>
</div>
</div>
<p>The output is meaningless, of course, because the model has not been trained yet.</p>
<p>Let’s take a look at the model’s structure.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">classifier_model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;model_1&quot;
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
text (InputLayer)               [(None,)]            0                                            
__________________________________________________________________________________________________
preprocessing (KerasLayer)      {&#39;input_word_ids&#39;: ( 0           text[0][0]                       
__________________________________________________________________________________________________
BERT_encoder (KerasLayer)       {&#39;default&#39;: (None, 5 41373185    preprocessing[0][0]              
                                                                 preprocessing[0][1]              
                                                                 preprocessing[0][2]              
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 512)          0           BERT_encoder[0][9]               
__________________________________________________________________________________________________
classifier (Dense)              (None, 7)            3591        dropout_1[0][0]                  
==================================================================================================
Total params: 41,376,776
Trainable params: 41,376,775
Non-trainable params: 1
__________________________________________________________________________________________________
</pre></div>
</div>
</div>
</div>
</section>
<section id="model-training">
<h2>Model training<a class="headerlink" href="#model-training" title="Permalink to this headline">¶</a></h2>
<p>You now have all the pieces to train a model, including the preprocessing module, BERT encoder, data, and classifier.</p>
<p>Since this is a non-binary classification problem and the model outputs probabilities, you’ll use <code class="docutils literal notranslate"><span class="pre">losses.CategoricalCrossentropy</span></code> loss function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">CategoricalCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">metrics</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">CategoricalAccuracy</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<section id="loading-the-bert-model-and-training">
<h3>Loading the BERT model and training<a class="headerlink" href="#loading-the-bert-model-and-training" title="Permalink to this headline">¶</a></h3>
<p>Using the <code class="docutils literal notranslate"><span class="pre">classifier_model</span></code> you created earlier, you can compile the model with the loss, metric and optimizer.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">epochs</span><span class="o">=</span><span class="mi">5</span>
<span class="n">optimizer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="mf">1e-5</span><span class="p">)</span>
<span class="n">classifier_model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
                         <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
                         <span class="n">metrics</span><span class="o">=</span><span class="n">metrics</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Note: training time will vary depending on the complexity of the BERT model you have selected.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Training model with </span><span class="si">{</span><span class="n">tfhub_handle_encoder</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">classifier_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">trainfeatures</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="n">trainlabels</span><span class="p">,</span>
                               <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">validfeatures</span><span class="p">,</span><span class="n">validlabels</span><span class="p">),</span>
                               <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                               <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training model with https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1
Epoch 1/5
409/409 [==============================] - 1866s 5s/step - loss: 0.3508 - categorical_accuracy: 0.8992 - val_loss: 0.0508 - val_categorical_accuracy: 0.9857
Epoch 2/5
409/409 [==============================] - 1813s 4s/step - loss: 0.0547 - categorical_accuracy: 0.9860 - val_loss: 0.0331 - val_categorical_accuracy: 0.9857
Epoch 3/5
409/409 [==============================] - 1812s 4s/step - loss: 0.0342 - categorical_accuracy: 0.9906 - val_loss: 0.0365 - val_categorical_accuracy: 0.9886
Epoch 4/5
409/409 [==============================] - 1780s 4s/step - loss: 0.0247 - categorical_accuracy: 0.9928 - val_loss: 0.0438 - val_categorical_accuracy: 0.9871
Epoch 5/5
409/409 [==============================] - 1811s 4s/step - loss: 0.0177 - categorical_accuracy: 0.9944 - val_loss: 0.0408 - val_categorical_accuracy: 0.9857
</pre></div>
</div>
</div>
</div>
</section>
<section id="evaluate-the-model">
<h3>Evaluate the model<a class="headerlink" href="#evaluate-the-model" title="Permalink to this headline">¶</a></h3>
<p>Let’s see how the model performs. Two values will be returned. Loss (a number which represents the error, lower values are better), and accuracy.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">loss</span><span class="p">,</span> <span class="n">accuracy</span> <span class="o">=</span> <span class="n">classifier_model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">testfeatures</span><span class="p">,</span><span class="n">testlabels</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Accuracy: </span><span class="si">{</span><span class="n">accuracy</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>22/22 [==============================] - 34s 1s/step - loss: 0.0856 - categorical_accuracy: 0.9743
Loss: 0.08561959117650986
Accuracy: 0.9742857217788696
</pre></div>
</div>
</div>
</div>
</section>
<section id="plot-the-accuracy-and-loss-over-time">
<h3>Plot the accuracy and loss over time<a class="headerlink" href="#plot-the-accuracy-and-loss-over-time" title="Permalink to this headline">¶</a></h3>
<p>Based on the <code class="docutils literal notranslate"><span class="pre">History</span></code> object returned by <code class="docutils literal notranslate"><span class="pre">model.fit()</span></code>. You can plot the training and validation loss for comparison, as well as the training and validation accuracy:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">history_dict</span> <span class="o">=</span> <span class="n">history</span><span class="o">.</span><span class="n">history</span>
<span class="nb">print</span><span class="p">(</span><span class="n">history_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>

<span class="n">acc</span> <span class="o">=</span> <span class="n">history_dict</span><span class="p">[</span><span class="s1">&#39;categorical_accuracy&#39;</span><span class="p">]</span>
<span class="n">val_acc</span> <span class="o">=</span> <span class="n">history_dict</span><span class="p">[</span><span class="s1">&#39;val_categorical_accuracy&#39;</span><span class="p">]</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">history_dict</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">]</span>
<span class="n">val_loss</span> <span class="o">=</span> <span class="n">history_dict</span><span class="p">[</span><span class="s1">&#39;val_loss&#39;</span><span class="p">]</span>

<span class="n">epochs</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">acc</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="c1"># &quot;bo&quot; is for &quot;blue dot&quot;</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training loss&#39;</span><span class="p">)</span>
<span class="c1"># b is for &quot;solid blue line&quot;</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">val_loss</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Validation loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Training and validation loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># plt.xlabel(&#39;Epochs&#39;)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">acc</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training acc&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">val_acc</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Validation acc&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Training and validation accuracy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Epochs&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Accuracy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;lower right&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>dict_keys([&#39;loss&#39;, &#39;categorical_accuracy&#39;, &#39;val_loss&#39;, &#39;val_categorical_accuracy&#39;])
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x7fca48e9a790&gt;
</pre></div>
</div>
<img alt="../_images/intent_classification_with_bert_54_2.png" src="../_images/intent_classification_with_bert_54_2.png" />
</div>
</div>
<p>In this plot, the red lines represents the training loss and accuracy, and the blue lines are the validation loss and accuracy.</p>
<p>Classifying arbitrary instructions:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">print_my_examples</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">results</span><span class="p">):</span>
  <span class="n">result_for_printing</span> <span class="o">=</span> \
    <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;input: </span><span class="si">{</span><span class="n">inputs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">:</span><span class="s1">&lt;30</span><span class="si">}</span><span class="s1"> : estimated intent: </span><span class="si">{</span><span class="n">results</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span>
                         <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">))]</span>
  <span class="nb">print</span><span class="p">(</span><span class="o">*</span><span class="n">result_for_printing</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">()</span>


<span class="n">examples</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s1">&#39;play a song from U2&#39;</span><span class="p">,</span>  <span class="c1"># this is the same sentence tried earlier</span>
    <span class="s1">&#39;Will it rain tomorrow&#39;</span><span class="p">,</span>
    <span class="s1">&#39;I like to hear greatist hits from beastie boys&#39;</span><span class="p">,</span>
    <span class="s1">&#39;I like to book a table for 3 persons&#39;</span><span class="p">,</span>
    <span class="s1">&#39;5 stars for machines like me&#39;</span>
<span class="p">]</span>

<span class="n">results</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">classifier_model</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">examples</span><span class="p">)))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">binarizer</span><span class="o">.</span><span class="n">classes_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;AddToPlaylist&#39;, &#39;BookRestaurant&#39;, &#39;GetWeather&#39;, &#39;PlayMusic&#39;,
       &#39;RateBook&#39;, &#39;SearchCreativeWork&#39;, &#39;SearchScreeningEvent&#39;],
      dtype=&#39;&lt;U20&#39;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">intents</span><span class="o">=</span><span class="n">binarizer</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">print_my_examples</span><span class="p">(</span><span class="n">examples</span><span class="p">,</span> <span class="n">intents</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>input: play a song from U2            : estimated intent: PlayMusic
input: Will it rain tomorrow          : estimated intent: GetWeather
input: I like to hear greatist hits from beastie boys : estimated intent: PlayMusic
input: I like to book a table for 3 persons : estimated intent: BookRestaurant
input: 5 stars for machines like me   : estimated intent: RateBook
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./transformer"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="attention.html" title="previous page">Sequence-To-Sequence, Attention, Transformer</a>
    <a class='right-next' id="next-link" href="../referenceSection.html" title="next page">References</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Prof. Dr. Johannes Maucher<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>