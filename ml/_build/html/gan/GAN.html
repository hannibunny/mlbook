
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Generative Adversarial Nets (GAN) &#8212; Machine Learning Lecture</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.5f77b4aec8189eecf79907ce328c390d.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="DCGAN Keras Implementation" href="DCGAN.html" />
    <link rel="prev" title="Variational Autoencoder (VAE) to generate handwritten digits" href="../neuralnetworks/04VariationalAutoencoder.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      <img src="../_static/hdmlogomed.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Machine Learning Lecture</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Intro and Overview Machine Learning Lecture
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../00BasicConcepts.html">
   Basic Concepts of Data Mining and Machine Learning
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Conventional ML
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../machinelearning/knn.html">
   K - Nearest Neighbour Classification / Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machinelearning/parametricClassification1D.html">
   Bayes- and Naive Bayes Classifier
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machinelearning/LinReg.html">
   Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machinelearning/heartRateRegression.html">
   Example Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machinelearning/LinearClassification.html">
   Linear Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machinelearning/svm.html">
   Support Vector Machines (SVM)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machinelearning/gp.html">
   Gaussian Process
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machinelearning/GaussianProcessRegression.html">
   Gaussian Process: Implementation in Python
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Neural Networks
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../neuralnetworks/01NeuralNets.html">
   Neural Networks Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../neuralnetworks/02RecurrentNeuralNetworks.html">
   Recurrent Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../neuralnetworks/03ConvolutionNeuralNetworks.html">
   Convolutional Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../neuralnetworks/03KerasMLPandCNNcifar.html">
   Implementing Neural Networks with Keras
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../neuralnetworks/04KerasPretrainedClassifiers.html">
   Applying Pretrained Deep Neural Networks for Image Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../neuralnetworks/05KerasPretrainedCovid.html">
   Apply Pretrained Neural Networks on new Task
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Autoencoder
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../neuralnetworks/04VariationalAutoencoder.html">
   Variational Autoencoder (VAE) to generate handwritten digits
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  GAN
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Generative Adversarial Nets (GAN)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="DCGAN.html">
   DCGAN Keras Implementation
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Reinforcement Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../rl/reinforcement.html">
   Reinforcement Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../rl/DQN.html">
   Deep Reinforcement Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../rl/QLearnFrozenLake.html">
   Example Q-Learning
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Text
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../text/01ModellingWordsAndTexts.html">
   Representations for Words and Texts
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../text/02TextClassification.html">
   Text classification with CNNs and LSTMs
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Transformer
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../transformer/attention.html">
   Sequence-To-Sequence, Attention, Transformer
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../transformer/intent_classification_with_bert.html">
   Intent Classification with BERT
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../referenceSection.html">
   References
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/gan/GAN.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#idea-gans">
   Idea GANs
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#architecture-and-training">
   Architecture and Training
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conditional-gan">
   Conditional GAN
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#cycle-gan-transform-from-one-domain-to-another">
   Cycle GAN: Transform from one domain to another
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#star-gan-multi-domain-transformations">
   Star GAN: Multi-domain transformations
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="generative-adversarial-nets-gan">
<h1>Generative Adversarial Nets (GAN)<a class="headerlink" href="#generative-adversarial-nets-gan" title="Permalink to this headline">¶</a></h1>
<p>In 2014 GANs have been introduced by <em>Ian Goodfellow</em> in <span id="id1">[<a class="reference internal" href="../referenceSection.html#id16">GPAM+14</a>]</span> (<a class="reference external" href="https://arxiv.org/pdf/1406.2661.pdf">Ian Goodfellow et al</a>). Since then they are one of the hottest topics in deeplearning research. GANs are able to generate synthetic data that looks similar to data of a given trainingset. In this way artifical images, paintings, texts, audio or handwritten digits can be generated.</p>
<div class="section" id="idea-gans">
<h2>Idea GANs<a class="headerlink" href="#idea-gans" title="Permalink to this headline">¶</a></h2>
<p>On an abstract level the idea of GANs can be described as follows: A counterfeiter produces fake banknotes. The police is able to discriminate the fake banknotes from real ones and it provides feedback to the counterfeiter on why the banknotes can be detected as fake. This feedback is used by the counterfeiter in order to produce fake, which is less distinguishable from real bankotes. After some iterations of producing better but not sufficiently good fake the counterfeiter is able to produce fake, which can not be discriminated from real banknotes.</p>
<p>A GAN consists of two models:</p>
<ol class="simple">
<li><p>The <strong>discriminator</strong> is the police. It learns to discriminate real data from artificially generated fake data.</p></li>
<li><p>The <strong>generator</strong> is the counterfeiter, which learns to generate data, that is indistinguishable from real data.</p></li>
</ol>
<div class="figure align-center" id="ganidea">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/GAN.png"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/GAN.png" src="https://maucher.home.hdm-stuttgart.de/Pics/GAN.png" style="width: 600pt;" /></a>
<p class="caption"><span class="caption-number">Fig. 29 </span><span class="caption-text">GAN: Generator-Network tries to produce data, such that the Discriminator-Network can not distinguish this <em>fake</em>-data from <em>real</em>-data.</span><a class="headerlink" href="#ganidea" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="architecture-and-training">
<h2>Architecture and Training<a class="headerlink" href="#architecture-and-training" title="Permalink to this headline">¶</a></h2>
<p>As depicted in image <a class="reference internal" href="#ganidea"><span class="std std-ref">GAN overall picture</span></a> the overall GAN consists of a Generator and a Discriminator. Both of them are usually neural networks of any type. In the initial work <a class="reference internal" href="#ganidea"><span class="std std-ref">GAN overall picture</span></a> MLPs has been applied for both components. Later, it has been shown in <span id="id2">[<a class="reference internal" href="../referenceSection.html#id54">RMC16</a>]</span>, that GANs, which apply CNNs for both components are much easier to configure and bahve in a more stable manner. The CNN, which has been applied as Generator in <span id="id3">[<a class="reference internal" href="../referenceSection.html#id54">RMC16</a>]</span>, is depicted below:</p>
<div class="figure align-center" id="dcgangen">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/DCGANgenerator.png"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/DCGANgenerator.png" src="https://maucher.home.hdm-stuttgart.de/Pics/DCGANgenerator.png" style="width: 600pt;" /></a>
<p class="caption"><span class="caption-number">Fig. 30 </span><span class="caption-text">Generator of the DCGAN. Image Source: <span id="id4">[<a class="reference internal" href="../referenceSection.html#id54">RMC16</a>]</span></span><a class="headerlink" href="#dcgangen" title="Permalink to this image">¶</a></p>
</div>
<p>The task of the GAN is to generate fake-data at the output of the Generator. This fake data shall be indistinguishable from the real-data <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, which is passed to the Discriminator input. The input to the Generator is usually a random noise vector <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>, drawn e.g. from a multivariate Gaussian distribution. The Generator must be designed such that it’s output <span class="math notranslate nohighlight">\(G(\mathbf{z})\)</span> has the same format as the real-data at the input of the discriminator. The Discriminator is a binary classification network, which receives real-data <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and fake-data <span class="math notranslate nohighlight">\(G(\mathbf{z})\)</span>, provided by the Generator. The Discriminator is trained such, that it can discriminate fake-data from real-data. The Generator is trained such, that it’s output <span class="math notranslate nohighlight">\(G(\mathbf{z})\)</span> is not distinguishable from real-data, i.e. both networks have adversarial training goals.</p>
<p>The adversarial training process is depcited in <a class="reference internal" href="#gantraining"><span class="std std-ref">the flow-chart below</span></a>. In each epoch</p>
<ol>
<li><p>first the discriminator is trained with a batch of real- and fake-data. For this</p>
<ul class="simple">
<li><p>real-data <span class="math notranslate nohighlight">\(x\)</span> is labeled by <span class="math notranslate nohighlight">\(1\)</span></p></li>
<li><p>fake-data <span class="math notranslate nohighlight">\(G(z)\)</span> is labeled by <span class="math notranslate nohighlight">\(0\)</span></p></li>
</ul>
<p>In this phase the weight in the discriminator are adapted such that the Minmax-Loss</p>
<div class="math notranslate nohighlight" id="equation-minmax">
<span class="eqno">(71)<a class="headerlink" href="#equation-minmax" title="Permalink to this equation">¶</a></span>\[
	E_x \left[ \log(D(x)) \right] +  E_z \left[ log(1-D(G(z)))  \right]
	\]</div>
<p><strong>is maximized</strong>. In this equation</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(D(x)\)</span> is the Discriminator’s estimate of the probability that real data instance <span class="math notranslate nohighlight">\(x\)</span> is real.</p></li>
<li><p><span class="math notranslate nohighlight">\(E_x\)</span> is the Expected value over all real data instances.</p></li>
<li><p><span class="math notranslate nohighlight">\(G(z)\)</span> is the Generator’s output when given noise <span class="math notranslate nohighlight">\(z\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(D(G(z))\)</span> is the Discriminator’s estimate of the probability that a fake instance is real.</p></li>
<li><p><span class="math notranslate nohighlight">\(E_z\)</span> is the Expected value over all random inputs to the generator.</p></li>
</ul>
<p>For a minibatch of <span class="math notranslate nohighlight">\(m\)</span> real-data samples <span class="math notranslate nohighlight">\(\lbrace x^{(1)}, \ldots, x^{(m)} \rbrace\)</span>  and <span class="math notranslate nohighlight">\(m\)</span> random samples <span class="math notranslate nohighlight">\(\lbrace z^{(1)}, \ldots, z^{(m)} \rbrace\)</span>, the expected values in equation <a class="reference internal" href="#equation-minmax">(71)</a>are calculated by</p>
<div class="math notranslate nohighlight" id="equation-minmaxdisc">
<span class="eqno">(72)<a class="headerlink" href="#equation-minmaxdisc" title="Permalink to this equation">¶</a></span>\[
	\frac{1}{m} \sum\limits_{i=1}^m \left[ \log(D(x^{(i)})) + log(1-D(G(z^{(i)})))  \right]
	\]</div>
</li>
<li><p>the weights of the Discrimator are being frozen and the Generator is trained. For this a a minibatch of <span class="math notranslate nohighlight">\(m\)</span> random-noise vectors <span class="math notranslate nohighlight">\(\lbrace z^{(1)}, \ldots, z^{(m)} \rbrace\)</span> is sampled and passed to the generator. The Generator outputs <span class="math notranslate nohighlight">\(G(z^{(i)})\)</span> are now labeled by 1 and passed to the Discriminator. In this phase only the weights of the Generator are adapted. They are adapted such that the Minmax-loss of equation <a class="reference internal" href="#equation-minmax">(71)</a> is <strong>minimized</strong>. For a minibatch of <span class="math notranslate nohighlight">\(m\)</span> random samples <span class="math notranslate nohighlight">\(\lbrace z^{(1)}, \ldots, z^{(m)} \rbrace\)</span>, the following Loss is <strong>minimized</strong> during Generator training:</p>
<div class="math notranslate nohighlight" id="equation-minmaxgen">
<span class="eqno">(73)<a class="headerlink" href="#equation-minmaxgen" title="Permalink to this equation">¶</a></span>\[
	\frac{1}{m} \sum\limits_{i=1}^m log(1-D(G(z^{(i)}))).
	\]</div>
</li>
</ol>
<div class="figure align-center" id="gantraining">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/GANtrainingProcess2cols.png"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/GANtrainingProcess2cols.png" src="https://maucher.home.hdm-stuttgart.de/Pics/GANtrainingProcess2cols.png" style="width: 600pt;" /></a>
<p class="caption"><span class="caption-number">Fig. 31 </span><span class="caption-text">GAN training process.</span><a class="headerlink" href="#gantraining" title="Permalink to this image">¶</a></p>
</div>
<p>Even if CNNs are applied as Generator and Discriminator, the configuration of GANs remains challenging. The authors of <span id="id5">[<a class="reference internal" href="../referenceSection.html#id54">RMC16</a>]</span> recommend the guidelines given below:</p>
<div class="figure align-center" id="dcganguide">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/DCGANguidelines.png"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/DCGANguidelines.png" src="https://maucher.home.hdm-stuttgart.de/Pics/DCGANguidelines.png" style="width: 600pt;" /></a>
<p class="caption"><span class="caption-number">Fig. 32 </span><span class="caption-text">Source: <span id="id6">[<a class="reference internal" href="../referenceSection.html#id54">RMC16</a>]</span></span><a class="headerlink" href="#dcganguide" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="conditional-gan">
<h2>Conditional GAN<a class="headerlink" href="#conditional-gan" title="Permalink to this headline">¶</a></h2>
<p>In <span id="id7">[<a class="reference internal" href="../referenceSection.html#id52">MO14</a>]</span> conditional GANs (cGAN) has been introduced. cGANs allow to control different different variants of the generated fake-data. For example if MNIST-like handwritten digits shall be generated by the GAN one can control which concrete digit shall be generated. The control is realized by passing additional information <span class="math notranslate nohighlight">\(y\)</span> to both, the input of the Generator and the input of the Discriminator.</p>
<div class="figure align-center" id="cgan">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/ConditionalGAN.png"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/ConditionalGAN.png" src="https://maucher.home.hdm-stuttgart.de/Pics/ConditionalGAN.png" style="width: 400pt;" /></a>
<p class="caption"><span class="caption-number">Fig. 33 </span><span class="caption-text">Conditional GAN: The condition <span class="math notranslate nohighlight">\(y\)</span> is added to the Generator- and Discriminator input. Depending on the condition different fake-data <span class="math notranslate nohighlight">\(G(z)\)</span> can be generated. Source: <span id="id8">[<a class="reference internal" href="../referenceSection.html#id52">MO14</a>]</span></span><a class="headerlink" href="#cgan" title="Permalink to this image">¶</a></p>
</div>
<p>For conditional GANs the Minmax-Loss Function, which is maximized by the Discriminator- and minimized by the Generator-training is</p>
<div class="math notranslate nohighlight" id="equation-minmaxcgan">
<span class="eqno">(74)<a class="headerlink" href="#equation-minmaxcgan" title="Permalink to this equation">¶</a></span>\[
E_x \left[ \log(D(x \mid y)) \right] +  E_z \left[ log(1-D(G(z  \mid y)))  \right]
\]</div>
<p>In the MNIST-example <span class="math notranslate nohighlight">\(y\)</span> represents the concrete number, e.g. it is the one-hot-encoding of the digit that shall be generated. In general, <span class="math notranslate nohighlight">\(y\)</span> can be any kind of auxillary information, such as a class-label or data from other modalities, e.g. word-vectors.</p>
<div class="figure align-center" id="cganmnist">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/GANconditionalMNIST.png"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/GANconditionalMNIST.png" src="https://maucher.home.hdm-stuttgart.de/Pics/GANconditionalMNIST.png" style="width: 500pt;" /></a>
<p class="caption"><span class="caption-number">Fig. 34 </span><span class="caption-text">Source: <span id="id9">[<a class="reference internal" href="../referenceSection.html#id52">MO14</a>]</span></span><a class="headerlink" href="#cganmnist" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="cycle-gan-transform-from-one-domain-to-another">
<h2>Cycle GAN: Transform from one domain to another<a class="headerlink" href="#cycle-gan-transform-from-one-domain-to-another" title="Permalink to this headline">¶</a></h2>
<p>Image-to-image translation (see picture below for examples) is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available <span id="id10">[<a class="reference internal" href="../referenceSection.html#id32">ZPIE17</a>]</span>.</p>
<div class="figure align-center" id="cycleganexamples">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/CycleGANexamples.png"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/CycleGANexamples.png" src="https://maucher.home.hdm-stuttgart.de/Pics/CycleGANexamples.png" style="width: 600pt;" /></a>
<p class="caption"><span class="caption-number">Fig. 35 </span><span class="caption-text">Source: <span id="id11">[<a class="reference internal" href="../referenceSection.html#id32">ZPIE17</a>]</span>.</span><a class="headerlink" href="#cycleganexamples" title="Permalink to this image">¶</a></p>
</div>
<p>Cycle GAN, as introduced in <span id="id12">[<a class="reference internal" href="../referenceSection.html#id32">ZPIE17</a>]</span>, is an approach for learning to translate an image from a <strong>source domain <span class="math notranslate nohighlight">\(X\)</span></strong> to a <strong>target domain <span class="math notranslate nohighlight">\(Y\)</span></strong> without requiring paired images for training - two sets of unpaired images are sufficient.</p>
<div class="figure align-center" id="paired">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/pairedVsUnpairedStyleTransfer.png"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/pairedVsUnpairedStyleTransfer.png" src="https://maucher.home.hdm-stuttgart.de/Pics/pairedVsUnpairedStyleTransfer.png" style="width: 400pt;" /></a>
<p class="caption"><span class="caption-number">Fig. 36 </span><span class="caption-text">Paired vs. unpaired training data. For Cycle GAN unpaired data is sufficient. Source: <span id="id13">[<a class="reference internal" href="../referenceSection.html#id32">ZPIE17</a>]</span>.</span><a class="headerlink" href="#paired" title="Permalink to this image">¶</a></p>
</div>
<p>In Cycle GAN a mapping</p>
<div class="math notranslate nohighlight">
\[
G : X \rightarrow Y 
\]</div>
<p>is learned, such that the distribution of images from <span class="math notranslate nohighlight">\(G(X)\)</span> is indistinguishable from the distribution <span class="math notranslate nohighlight">\(Y\)</span>. Moreover, this mapping <span class="math notranslate nohighlight">\(G\)</span> is coupled with an inverse mapping</p>
<div class="math notranslate nohighlight">
\[
F : Y \rightarrow  X
\]</div>
<p>Both mappings, <span class="math notranslate nohighlight">\(G\)</span> and <span class="math notranslate nohighlight">\(F\)</span> are learned using a <strong>adversarial loss</strong>.</p>
<p>Moreover, a <strong>cycle consistency loss</strong> to enforce</p>
<div class="math notranslate nohighlight">
\[
F(G(X)) \approx X
\]</div>
<p>(and vice versa) is applied.</p>
<div class="figure align-center" id="id15">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/CycleGAN.png"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/CycleGAN.png" src="https://maucher.home.hdm-stuttgart.de/Pics/CycleGAN.png" style="width: 600pt;" /></a>
<p class="caption"><span class="caption-number">Fig. 37 </span><span class="caption-text">Learned mappings between source domain <span class="math notranslate nohighlight">\(X\)</span> and target domain <span class="math notranslate nohighlight">\(Y\)</span>. Source: <span id="id14">[<a class="reference internal" href="../referenceSection.html#id32">ZPIE17</a>]</span>.</span><a class="headerlink" href="#id15" title="Permalink to this image">¶</a></p>
</div>
<p><strong>Adversarial Loss:</strong></p>
<div class="math notranslate nohighlight">
\[\begin{split}
L_{adv}(G,D_y,X) &amp; = &amp; \frac{1}{m} \sum\limits_{i=1}^m (1-D_y(G(x_i)))^2 \\
L_{adv}(F,D_x,Y) &amp; = &amp; \frac{1}{m} \sum\limits_{i=1}^m (1-D_x(F(y_i)))^2 
\end{split}\]</div>
<ul class="simple">
<li><p>The Generator <span class="math notranslate nohighlight">\(G\)</span> is trained, such that it converts <span class="math notranslate nohighlight">\(x\)</span> to something that the Discriminator <span class="math notranslate nohighlight">\(D_y\)</span> can not distinguish from <span class="math notranslate nohighlight">\(y\)</span>.</p></li>
<li><p>The Generator <span class="math notranslate nohighlight">\(F\)</span> is trained, such that it converts <span class="math notranslate nohighlight">\(y\)</span> to something that the Discriminator <span class="math notranslate nohighlight">\(D_x\)</span> can not distinguish from <span class="math notranslate nohighlight">\(x\)</span>.</p></li>
</ul>
<p>The adversarial loss-functions above are not derived from <em>binary-crossentropy</em> but from <em>least-square-loss</em>. In <span id="id16">[<a class="reference internal" href="../referenceSection.html#id34">MLX+16</a>]</span> it has been shown, that this-loss is better in the context of the <a class="reference external" href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem">vanishing gradient problem</a>.</p>
<p><strong>Cycle Consitency Loss:</strong></p>
<p>The <em>cycle consistency loss</em> enforces the requirement, that if an input-image is converted to the other domain and back again, by feeding it through both generators, the result should be similar to the input-image.</p>
<p>The minimization of <span class="math notranslate nohighlight">\(L_{cyc}(G,F,X,Y)\)</span>  enforces that <span class="math notranslate nohighlight">\(F(G(x)) \approx x\)</span> and <span class="math notranslate nohighlight">\(G(F(y)) \approx y\)</span>.</p>
<div class="math notranslate nohighlight">
\[
L_{cyc}(G,F,X,Y) =  \frac{1}{m} \sum\limits_{i=1}^m \left[ F(G(x_i))-x_i \right] + \left[ G(F(y_i))-y_i)   \right].
\]</div>
<p><strong>Overall Loss-Function:</strong></p>
<div class="math notranslate nohighlight">
\[
L_{full} = L_{adv}(G,D_y,X) + L_{adv}(F,D_x,Y) + L_{cyc}(G,F,X,Y)
\]</div>
<p><strong>Architecture of the Generators <span class="math notranslate nohighlight">\(G\)</span> and <span class="math notranslate nohighlight">\(F\)</span>:</strong></p>
<div class="figure align-center" id="cyclegangen">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/CycleGANgenerator.png"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/CycleGANgenerator.png" src="https://maucher.home.hdm-stuttgart.de/Pics/CycleGANgenerator.png" style="width: 400pt;" /></a>
<p class="caption"><span class="caption-number">Fig. 38 </span><span class="caption-text">Source: <a class="reference external" href="https://towardsdatascience.com/cyclegan-learning-to-translate-images-without-paired-training-data-5b4e93862c8d%7D%7Bhttps://towardsdatascience.com/cyclegan-learning-to-translate-images-without-paired-training-data-5b4e93862c8d">towards data science</a>.</span><a class="headerlink" href="#cyclegangen" title="Permalink to this image">¶</a></p>
</div>
<p><strong>Architecture of the Discriminators <span class="math notranslate nohighlight">\(D_x\)</span> and <span class="math notranslate nohighlight">\(D_y\)</span>:</strong></p>
<div class="figure align-center" id="cyclegandisc">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/CycleGANdiscriminator.png"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/CycleGANdiscriminator.png" src="https://maucher.home.hdm-stuttgart.de/Pics/CycleGANdiscriminator.png" style="width: 400pt;" /></a>
<p class="caption"><span class="caption-number">Fig. 39 </span><span class="caption-text">Source: <a class="reference external" href="https://towardsdatascience.com/cyclegan-learning-to-translate-images-without-paired-training-data-5b4e93862c8d%7D%7Bhttps://towardsdatascience.com/cyclegan-learning-to-translate-images-without-paired-training-data-5b4e93862c8d">towards data science</a>.</span><a class="headerlink" href="#cyclegandisc" title="Permalink to this image">¶</a></p>
</div>
<p><strong>Cycle GAN Strengths and Limitations:</strong></p>
<p>Cycle GAN works well on tasks that involve color or texture changes, e.g. day-to-night translations, photo-to-painting transformation or style transfer. This can be seen e.g. in this <a class="reference external" href="https://youtu.be/lCR9sT9mbis">video on Cityscape to GTA5-Style transfer</a> or this <a class="reference external" href="https://youtu.be/N7KbfWodXJE">video on day- to night-drive transformation</a>.</p>
<p>However, Cycle GAN often fails for tasks, that require substantial geometric changes, as can be seen in the image below.</p>
<div class="figure align-center" id="badexample">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/CycleGANBadExample.png"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/CycleGANBadExample.png" src="https://maucher.home.hdm-stuttgart.de/Pics/CycleGANBadExample.png" style="width: 400pt;" /></a>
<p class="caption"><span class="caption-number">Fig. 40 </span><span class="caption-text">Source: <span id="id17">[<a class="reference internal" href="../referenceSection.html#id32">ZPIE17</a>]</span>.</span><a class="headerlink" href="#badexample" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="star-gan-multi-domain-transformations">
<h2>Star GAN: Multi-domain transformations<a class="headerlink" href="#star-gan-multi-domain-transformations" title="Permalink to this headline">¶</a></h2>
<div class="figure align-center" id="starganceleb">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/StarGANcelebA.png"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/StarGANcelebA.png" src="https://maucher.home.hdm-stuttgart.de/Pics/StarGANcelebA.png" style="width: 400pt;" /></a>
<p class="caption"><span class="caption-number">Fig. 41 </span><span class="caption-text">Example: Transformations between multiple domains. Source: <span id="id18">[<a class="reference internal" href="../referenceSection.html#id2">CCK+18</a>]</span>.</span><a class="headerlink" href="#starganceleb" title="Permalink to this image">¶</a></p>
</div>
<p>In Cycle GAN transformations from a source domain <span class="math notranslate nohighlight">\(X\)</span> to a target domain <span class="math notranslate nohighlight">\(Y\)</span> has been addressed. For transformations between multiple domains, Cycle GAN can also be applied, as shown in image <a class="reference internal" href="#stargancross"><span class="std std-ref">Comparison between Cycle- and Star-GAN for multi-domain transformations</span></a>. However, in this case for each possible pair of domains an individual Generator pair has to be trained. With Star GAN transformations between multiple domains can be realized using only a single Generator.</p>
<div class="figure align-center" id="stargancross">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/starGANcrossdomain.png"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/starGANcrossdomain.png" src="https://maucher.home.hdm-stuttgart.de/Pics/starGANcrossdomain.png" style="width: 400pt;" /></a>
<p class="caption"><span class="caption-number">Fig. 42 </span><span class="caption-text">Source: <span id="id19">[<a class="reference internal" href="../referenceSection.html#id2">CCK+18</a>]</span>.</span><a class="headerlink" href="#stargancross" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-center" id="stargantrain">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/starGANtraining.png"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/starGANtraining.png" src="https://maucher.home.hdm-stuttgart.de/Pics/starGANtraining.png" style="width: 600pt;" /></a>
<p class="caption"><span class="caption-number">Fig. 43 </span><span class="caption-text">Source: <span id="id20">[<a class="reference internal" href="../referenceSection.html#id2">CCK+18</a>]</span>.</span><a class="headerlink" href="#stargantrain" title="Permalink to this image">¶</a></p>
</div>
<p><strong>Adversarial Loss:</strong></p>
<p>Discriminator <span class="math notranslate nohighlight">\(D\)</span> tries to maximize <span class="math notranslate nohighlight">\(L_{adv}\)</span>, whereas the Generator <span class="math notranslate nohighlight">\(G\)</span> tries to it.</p>
<div class="math notranslate nohighlight">
\[
L_{adv}= E_x \left[ \log(D_{src}(x)) \right] +  E_{x,c} \left[ log(1-D_{src}(G(x,c)))  \right],
\]</div>
<p>with</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(x\)</span> real image</p></li>
<li><p><span class="math notranslate nohighlight">\(c\)</span> label of target domain</p></li>
<li><p><span class="math notranslate nohighlight">\(D_{src}\)</span> the part of the discriminator which distinguishes real from fake</p></li>
</ul>
<p><strong>Domain Classification Loss:</strong></p>
<p>The Domain classification loss of <strong>real images</strong> to be minimized by <span class="math notranslate nohighlight">\(D\)</span> is:</p>
<div class="math notranslate nohighlight">
\[
L_{cls}^r= E_{x,c'} \left[ - \log(D_{cls}(c'|x)) \right]. 
\]</div>
<p>By minimizing this objective, <span class="math notranslate nohighlight">\(D\)</span> learns to classify a real image <span class="math notranslate nohighlight">\(x\)</span> to its corresponding original domain <span class="math notranslate nohighlight">\(c′\)</span>.</p>
<p>The domain classification loss of <strong>fake images</strong> to be minimized by <span class="math notranslate nohighlight">\(G\)</span> is:</p>
<div class="math notranslate nohighlight">
\[
L_{cls}^f= E_{x,c} \left[ - \log(D_{cls}(c|G(x,c))) \right], 
\]</div>
<p><span class="math notranslate nohighlight">\(G\)</span> tries to minimize this objective to generate images that can be classified as the target domain <span class="math notranslate nohighlight">\(c\)</span>.
with</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(x\)</span> real image</p></li>
<li><p><span class="math notranslate nohighlight">\(c\)</span> label of target domain</p></li>
<li><p><span class="math notranslate nohighlight">\(c'\)</span> label of original domain</p></li>
<li><p><span class="math notranslate nohighlight">\(D_{cls}\)</span> the part of the discriminator which assigns images to corresponding labels.</p></li>
</ul>
<p><strong>Reconstruction Loss:</strong></p>
<p>By minimizing the adversarial and classification losses, <span class="math notranslate nohighlight">\(G\)</span> is trained to generate images that are realistic and classified to its correct target domain. However, minimizing the losses does not guarantee that translated images preserve the content of its input images while changing only the domain-related part of the inputs. This problem is alleviated by the reconstruction loss, which is similar to the cycle consistency loss in Cycle GAN.</p>
<div class="math notranslate nohighlight">
\[
L_{rec}= E_{x,c',c} \left[ || x - G(G(x,c),c') ||_1  \right],
\]</div>
<p>with</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(x\)</span> real image</p></li>
<li><p><span class="math notranslate nohighlight">\(c\)</span> label of target domain</p></li>
<li><p><span class="math notranslate nohighlight">\(c'\)</span> label of original domain</p></li>
</ul>
<p><strong>Overall Loss:</strong></p>
<p>Discriminator minimizes</p>
<div class="math notranslate nohighlight">
\[
L_D=-L_{adv}+\lambda_{cls} L_{cls}^r.
\]</div>
<p>Generator minimizes</p>
<div class="math notranslate nohighlight">
\[
L_G=L_{adv}+\lambda_{cls} L_{cls}^f + \lambda_{rec} L_{rec}.
\]</div>
<div class="figure align-center" id="stargantrainceleb">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/StarGANtrainingCelebRafd.png"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/StarGANtrainingCelebRafd.png" src="https://maucher.home.hdm-stuttgart.de/Pics/StarGANtrainingCelebRafd.png" style="width: 600pt;" /></a>
<p class="caption"><span class="caption-number">Fig. 44 </span><span class="caption-text">Source: <span id="id21">[<a class="reference internal" href="../referenceSection.html#id2">CCK+18</a>]</span>.</span><a class="headerlink" href="#stargantrainceleb" title="Permalink to this image">¶</a></p>
</div>
<p><strong>Architecture of Star GAN Generator:</strong></p>
<p>The Generator archictecture is similar to the one used in Cycle GAN:</p>
<div class="figure align-center" id="stargangenar">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/StarGANgenerator.png"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/StarGANgenerator.png" src="https://maucher.home.hdm-stuttgart.de/Pics/StarGANgenerator.png" style="width: 600pt;" /></a>
<p class="caption"><span class="caption-number">Fig. 45 </span><span class="caption-text">Source: <span id="id22">[<a class="reference internal" href="../referenceSection.html#id2">CCK+18</a>]</span>.</span><a class="headerlink" href="#stargangenar" title="Permalink to this image">¶</a></p>
</div>
<p><strong>Architecture of Star GAN Discriminator:</strong></p>
<p>The Discriminator <span class="math notranslate nohighlight">\(D\)</span> classifies per patch. For this a Fully Convolutional Neural Network, like in PatchGAN is applied:</p>
<div class="figure align-center" id="stargandiscar">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/StarGANdiscriminator.png"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/StarGANdiscriminator.png" src="https://maucher.home.hdm-stuttgart.de/Pics/StarGANdiscriminator.png" style="width: 600pt;" /></a>
<p class="caption"><span class="caption-number">Fig. 46 </span><span class="caption-text">Source: <span id="id23">[<a class="reference internal" href="../referenceSection.html#id2">CCK+18</a>]</span>.</span><a class="headerlink" href="#stargandiscar" title="Permalink to this image">¶</a></p>
</div>
<p><strong>Final Remark on Fully Convolutional Network:</strong></p>
<p>In a Fully Convolutional Network (FCNN) dense layers at the output are replaced by convolutional layers, by rearranging the neurons in the dense layer into the channel-dimension of a convolutional layer. In this way images of variable size can be passed to the network and the corresponding output has also a variable size. Each output neuron belongs to one region in the image.</p>
<div class="figure align-center" id="fcn">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/fcn.PNG"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/fcn.PNG" src="https://maucher.home.hdm-stuttgart.de/Pics/fcn.PNG" style="width: 600pt;" /></a>
<p class="caption"><span class="caption-number">Fig. 47 </span><span class="caption-text">Neurons of the dense layer are rearranged into channels of a convolutional layer. Since a convolutional layer always has the same number of weights, independent of the size of it’s input, it can manage different sizes of input. In dense layers the number of weights depends on the size of the input. Therefore, dense layers can not cope with variable-size input.</span><a class="headerlink" href="#fcn" title="Permalink to this image">¶</a></p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./gan"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="../neuralnetworks/04VariationalAutoencoder.html" title="previous page">Variational Autoencoder (VAE) to generate handwritten digits</a>
    <a class='right-next' id="next-link" href="DCGAN.html" title="next page">DCGAN Keras Implementation</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Prof. Dr. Johannes Maucher<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>