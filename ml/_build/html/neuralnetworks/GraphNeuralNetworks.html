
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Graph Neural Networks (GNN) &#8212; Machine Learning Lecture</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.5f77b4aec8189eecf79907ce328c390d.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Sequence-To-Sequence, Attention, Transformer" href="../transformer/attention.html" />
    <link rel="prev" title="Text classification with CNNs and LSTMs" href="../text/02TextClassification.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      <img src="../_static/hdmlogomed.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Machine Learning Lecture</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Intro and Overview Machine Learning Lecture
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../00BasicConcepts.html">
   Basic Concepts of Data Mining and Machine Learning
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Conventional ML
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../machinelearning/knn.html">
   K - Nearest Neighbour Classification / Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machinelearning/parametricClassification1D.html">
   Bayes- and Naive Bayes Classifier
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machinelearning/LinReg.html">
   Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machinelearning/heartRateRegression.html">
   Example Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machinelearning/LinearClassification.html">
   Linear Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machinelearning/svm.html">
   Support Vector Machines (SVM)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machinelearning/gp.html">
   Gaussian Process
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machinelearning/GaussianProcessRegression.html">
   Gaussian Process: Implementation in Python
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Neural Networks
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="01NeuralNets.html">
   Neural Networks Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02RecurrentNeuralNetworks.html">
   Recurrent Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03ConvolutionNeuralNetworks.html">
   Convolutional Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="convolutionDemos.html">
   Animations of Convolution and Deconvolution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03KerasMLPandCNNcifar.html">
   Implementing Neural Networks with Keras
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="04KerasPretrainedClassifiers.html">
   Applying Pretrained Deep Neural Networks for Image Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="05KerasPretrainedCovid.html">
   Apply Pretrained Neural Networks on new Task
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Autoencoder
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="04VariationalAutoencoder.html">
   Variational Autoencoder (VAE) to generate handwritten digits
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  GAN
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../gan/GAN.html">
   Generative Adversarial Nets (GAN)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../gan/DCGAN.html">
   DCGAN Keras Implementation
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Reinforcement Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../rl/reinforcement.html">
   Reinforcement Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../rl/DQN.html">
   Deep Reinforcement Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../rl/QLearnFrozenLake.html">
   Example Q-Learning
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Text
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../text/01ModellingWordsAndTexts.html">
   Representations for Words and Texts
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../text/02TextClassification.html">
   Text classification with CNNs and LSTMs
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Graph Neural Networks
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Graph Neural Networks (GNN)
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Transformer
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../transformer/attention.html">
   Sequence-To-Sequence, Attention, Transformer
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../transformer/intent_classification_with_bert.html">
   Intent Classification with BERT
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../referenceSection.html">
   References
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/neuralnetworks/GraphNeuralNetworks.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/neuralnetworks/GraphNeuralNetworks.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Graph Neural Networks (GNN)
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#graph">
     Graph
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#graph-neural-networks">
     Graph Neural Networks
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#message-passing">
       Message Passing
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#graph-convolutional-network-gcn">
       Graph Convolutional Network (GCN)
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#graph-convolutional-network-for-node-classification">
       Graph Convolutional Network for Node Classification
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#graph-convolutional-network-for-graph-classification">
       Graph Convolutional Network for Graph Classification
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introductory-example-node-classification-with-graph-neural-networks">
   Introductory Example: Node Classification with Graph Neural Networks
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#introduction">
     Introduction
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#setup">
     Setup
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#prepare-the-dataset">
     Prepare the Dataset
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#download-the-dataset">
       Download the dataset
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#process-and-visualize-the-dataset">
       Process and visualize the dataset
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#split-the-dataset-into-stratified-train-and-test-sets">
       Split the dataset into stratified train and test sets
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#implement-train-and-evaluate-experiment">
     Implement, Train and Evaluate Experiment
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#implement-feedforward-network-ffn-module">
     Implement Feedforward Network (FFN) Module
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#build-a-baseline-neural-network-model">
     Build a Baseline Neural Network Model
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#prepare-the-data-for-the-baseline-model">
       Prepare the data for the baseline model
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#implement-a-baseline-classifier">
       Implement a baseline classifier
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#train-the-baseline-classifier">
       Train the baseline classifier
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#examine-the-baseline-model-predictions">
       Examine the baseline model predictions
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#build-a-graph-neural-network-model">
     Build a Graph Neural Network Model
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#prepare-the-data-for-the-graph-model">
       Prepare the data for the graph model
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#implement-a-graph-convolution-layer">
       Implement a graph convolution layer
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#implement-a-graph-neural-network-node-classifier">
       Implement a graph neural network node classifier
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#train-the-gnn-model">
       Train the GNN model
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#examine-the-gnn-model-predictions">
       Examine the GNN model predictions
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="graph-neural-networks-gnn">
<h1>Graph Neural Networks (GNN)<a class="headerlink" href="#graph-neural-networks-gnn" title="Permalink to this headline">¶</a></h1>
<p>Graph based deep learning is currently one of the hottest topics in Machine Learning Research. In the NeurIPS 2020 conference GNNs constituted the most prominent topic, as can be seen in this <a class="reference external" href="https://github.com/naganandy/graph-based-deep-learning-literature/blob/master/conference-publications/folders/publications_neurips20/README.md">list of conference papers</a>.</p>
<p>However, GNNs are not only subject of research. They have already found their way into a wide range of <a class="reference external" href="https://medium.com/criteo-engineering/top-applications-of-graph-neural-networks-2021-c06ec82bfc18">applications</a>.</p>
<p>Graph Neural Networks are suitable for Machine Learning tasks on data with structural
relation between the individual data-points. Examples are e.g. social and communication networks analysis, traffic prediction, fraud detection, etc. <a class="reference external" href="https://www.cs.mcgill.ca/~wlh/grl_book/">Graph Representation Learning</a>
aims to build and train models for graph datasets to be used for a variety of ML tasks.</p>
<p>This lecture shall provide an understanding of the basic concepts of Graph Neural Networks and their application categories.</p>
<div class="section" id="graph">
<h2>Graph<a class="headerlink" href="#graph" title="Permalink to this headline">¶</a></h2>
<p>A graph consists of a set of nodes, which are partially connected by edges. <strong>Nodes</strong> can represent things of different categories such as persons, locations, companies, etc. Nodes may also represent things of the same categorie but different subcategories. <strong>Edges</strong> can be directed or undirected. They also may have weights, which somehow define the <em>strength</em> of the connection. Edges can also be of different type, e.g.</p>
<ul class="simple">
<li><p>an edge between a node of type <em>person</em> and a node of type <em>paper</em> may have the meaning <em>isAuthorOf</em></p></li>
<li><p>a directed edge between two nodes of type <em>paper</em> may mean that the paper of the source node refers to the paper of the target node.</p></li>
</ul>
<figure align="center">
<img src="https://maucher.home.hdm-stuttgart.de/Pics/graph.png" width="500px" align="center">
<figcaption>
Figure 1: Example graph with undirected edges (no arrows) and different node types (colours)
</figcaption>
</figure></div>
<div class="section" id="graph-neural-networks">
<h2>Graph Neural Networks<a class="headerlink" href="#graph-neural-networks" title="Permalink to this headline">¶</a></h2>
<p>In the context of GNNs each node is described by a set of features (a numeric vector). For example, if the nodes represent papers, then the descriptor vector of the nodes may be the Bag-of-Words vector (BoW) of the paper.</p>
<figure align="center">
<img src="https://maucher.home.hdm-stuttgart.de/Pics/graphNodeDescriptors.png" width="550px" align="center">
<figcaption>
Figure 2: To each node a descriptor (numeric vector) is assigned.
</figcaption>
</figure>
<p>The BoW of a paper depends only on the words, which appear in the paper. Information about the neighbouring nodes is totally ignored. On the other hand, in some applications such information from neighbouring nodes may be helpful. For example, it may be easier to determine the subject of a paper, if not only the words in the paper itself are known, but also the contents of the papers in it’s immediate neighbourhood. This is actually what GNNs provide: <strong>GNNs calculate node representations (embeddings), which depend not only on the contents of the node itself, but also on the neighbouring nodes.</strong> The definition of <em>neighbourhood</em> depends on the task and the context. For example the immediate neighbours of a scientific paper may be all papers, which are referenced in this paper.</p>
<div class="section" id="message-passing">
<h3>Message Passing<a class="headerlink" href="#message-passing" title="Permalink to this headline">¶</a></h3>
<p>In a GNN the node representation <span class="math notranslate nohighlight">\(h_i^k\)</span> of each node <span class="math notranslate nohighlight">\(i\)</span> are adapted over many iteration (k indicates the iteration). At the very beginning to each node <span class="math notranslate nohighlight">\(i\)</span> an initial vector <span class="math notranslate nohighlight">\(h_i^0\)</span> is assigned, e.g. the BoW vector of a document. Then in each iteration <span class="math notranslate nohighlight">\(k&gt;0\)</span> these representations are updated by information from the neighbouring nodes. The image below sketches the update of node 0 in iteration <span class="math notranslate nohighlight">\(k\)</span>. Let <span class="math notranslate nohighlight">\(N(i)\)</span> denote the set of direct neighbours of node <span class="math notranslate nohighlight">\(i\)</span>, then</p>
<ol class="simple">
<li><p>For each node <span class="math notranslate nohighlight">\(u\)</span> in <span class="math notranslate nohighlight">\(N(i)\)</span> pass the current descriptor <span class="math notranslate nohighlight">\(h_u^k\)</span> to a Feed Forward Neural Network (for example a single fully connected layer). The output of this FFN is called the message <span class="math notranslate nohighlight">\(m_u^k\)</span> from node <span class="math notranslate nohighlight">\(u\)</span>.</p></li>
<li><p>All messages <span class="math notranslate nohighlight">\(m_u^k\)</span> and the current node representation <span class="math notranslate nohighlight">\(h_i^{k}\)</span> are <strong>aggregated</strong> by some function which outputs the new representation <span class="math notranslate nohighlight">\(h_i^{k+1}\)</span> of node <span class="math notranslate nohighlight">\(i\)</span>. The aggregation function can be a simple sum-, mean-, or max-operation. However, it can also be implemented by an arbitrary neural layer type as e.g. a fully connected layer or a recurrent layer.</p></li>
</ol>
<p>This process is performed, in parallel, on all nodes <span class="math notranslate nohighlight">\(i\)</span> in the graph as node-embeddings in iteration <span class="math notranslate nohighlight">\(k+1\)</span> depend on embeddings in iteration <span class="math notranslate nohighlight">\(k\)</span>. In this way in each iteration each node <em>grabs</em> more information from it’s surrounding nodes. Finally this <em>better representations</em> can be applied for different tasks, such as node-classification, node-clustering, sub-graph-identification, sub-graph-clustering etc. Depending on the task, individual node embeddings of type <span class="math notranslate nohighlight">\(h_i^{k}\)</span>, or representations of sub-graphs or the entire graph may be required. A simple representation of a subgraph or graph can be obtained by just summing up the node-embeddings of all nodes in the graph or subgraph, respectively.</p>
<figure align="center">
<img src="https://maucher.home.hdm-stuttgart.de/Pics/graphMessagePassing.png" width="800px" align="center">
<figcaption>
Figure 3: Calculation of new descriptor at node 0.
</figcaption>
</figure></div>
<div class="section" id="graph-convolutional-network-gcn">
<h3>Graph Convolutional Network (GCN)<a class="headerlink" href="#graph-convolutional-network-gcn" title="Permalink to this headline">¶</a></h3>
<p>Next, we will consider how the concept of message passing, as introduced in the previous subsection, is implemented in Graph Neural Networks (GNNs)?</p>
<p>As explained in the previous subsections, in each iteration each node’s descriptor vector is updated by the messages from the neighbouring nodes. In a Graph Convolutional Network (GCN) each iteration is implemented by a GCN-layer. I.e. the first update is calculated in the first GCN-layer, the second update is calculated in the second layer and so on. Thus the number of updates and therefore the range of the neighbourhood that is considered in the final node representation is given by the number of GCN layers. Such a stack of GCN-layers is depicted in the right hand side of Figure 4. In this Figure the abbreviation GNN is used for Graph Neural Net (GNN). GNN is more general as GCN, but here we always assume that the GNN is a GCN. The short-cut connections around the GNN layers are necessary, because the new representation of a node is the aggregation of the messages from the neighboring nodes and the old representation of this node.</p>
<p>As depicted in the right hand side of Figure 4, the stack of GCN layers is preceeded by one or more preprocessing layers, which calculate the initial node descriptors. The initial node descriptors do not depend on the neighbouring codes. Moreover, the output of the GCN-layer stack is passed to one or more post-processing layers. These post-processing layers depend on the task. E.g. for the task of node-classification, the post-processing layers assign to each node descriptor at the output of the GCN-layer stack the category of the node.</p>
<p>In Figure 3, it was assumed, that the message <span class="math notranslate nohighlight">\(m_u^k\)</span> coming from node <span class="math notranslate nohighlight">\(u\)</span> in iteration <span class="math notranslate nohighlight">\(k+1\)</span> is the output of a dense layer (linear layer), whose input is the node descriptor <span class="math notranslate nohighlight">\(h_u^k\)</span>. In general, as depicted in the left hand-side of the image below, the dense layer can apply Batch-Normalization, Dropout and an arbitrary Activation function.</p>
<figure align="center">
<img src="https://maucher.home.hdm-stuttgart.de/Pics/GNNDesign.png" width="600px" align="center">
<figcaption>
Figure 4: Image Source: <a href="https://arxiv.org/abs/2011.08843">Design Space for Graph Neural Networks</a> 
</figcaption>
</figure>
<p>Where does the name <em>Graph Convolutional Layer</em> come from?</p>
<p>For this take a look to the left hand side of Figure 5. In a <em>Convolutional Layer</em> at each position a new value is calculated as the scalar-product of the filter coefficients and the receptive field of the current position (neuron). Each position (neuron) can be considered as a node. And each node has an edge to all other nodes (positions), which belong to the local receptive field of the current position (neuron). Actually, at each position not only one value is calculated, but one value for each feature map.</p>
<p>With this notion in mind, the difference of a graph convolution layer w.r.t. a convolution layer is that in a GCN each node can have an arbitrary number of neighbouring nodes, whereas in a convolution layer the number of neighbouring nodes is fixed and given by the size of the filter.</p>
<figure align="center">
<img src="https://maucher.home.hdm-stuttgart.de/Pics/gcnVScnn.png" width="600px" align="center">
<figcaption>
    Figure 5: Image Source: <a href="https://arxiv.org/pdf/1901.00596.pdf">Wu et al.: A Comprehensive Survey on Graph Neural Networks</a>
</figcaption>
</figure>
</div>
<div class="section" id="graph-convolutional-network-for-node-classification">
<h3>Graph Convolutional Network for Node Classification<a class="headerlink" href="#graph-convolutional-network-for-node-classification" title="Permalink to this headline">¶</a></h3>
<figure align="center">
<img src="https://maucher.home.hdm-stuttgart.de/Pics/gcnNodeClassification.png" width="600px" align="center">
<figcaption>
    Image Source: <a href="https://arxiv.org/pdf/1901.00596.pdf">Wu et al.: A Comprehensive Survey on Graph Neural Networks</a>
</figcaption>
</figure>
</div>
<div class="section" id="graph-convolutional-network-for-graph-classification">
<h3>Graph Convolutional Network for Graph Classification<a class="headerlink" href="#graph-convolutional-network-for-graph-classification" title="Permalink to this headline">¶</a></h3>
<figure align="center">
<img src="https://maucher.home.hdm-stuttgart.de/Pics/gcnGraphClassification.png" width="600px"align="center">
<figcaption>
    Image Source: <a href="https://arxiv.org/pdf/1901.00596.pdf">Wu et al.: A Comprehensive Survey on Graph Neural Networks</a>
</figcaption>
</figure>
<p><a id='intro'></a></p>
</div>
</div>
</div>
<div class="section" id="introductory-example-node-classification-with-graph-neural-networks">
<h1>Introductory Example: Node Classification with Graph Neural Networks<a class="headerlink" href="#introductory-example-node-classification-with-graph-neural-networks" title="Permalink to this headline">¶</a></h1>
<p>After learning the basic concepts of Graph Neural Networks, we continue with the implementation of Graph Convolution Networks (GCN). This part is an adaptation of the <a class="reference external" href="https://keras.io/examples/graph/gnn_citations/">Keras tutorial</a>. You don’t have to implement the GCN layer and the GCN Network by yourself, but you should study this part carefully, such that you are able</p>
<ul class="simple">
<li><p>to answer the questions at the end of this section</p></li>
<li><p>to implement the GCN network on the task of music genre prediction in the following section.</p></li>
</ul>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>This example demonstrates a simple implementation of a <a class="reference external" href="https://arxiv.org/pdf/1901.00596.pdf">Graph Neural Network</a>
(GNN) model. The model is used for a node prediction task on the <a class="reference external" href="https://relational.fit.cvut.cz/dataset/CORA">Cora dataset</a>
to predict the subject of a paper given its words and citations network.</p>
<p>Note that, <strong>we implement a Graph Convolution Layer from scratch</strong> to provide a better
understanding of how they work. However, there is a number of specialized TensorFlow-based
libraries that provide rich GNN APIs, such as <a class="reference external" href="https://graphneural.network/">Spectral</a>,
<a class="reference external" href="https://stellargraph.readthedocs.io/en/stable/README.html">StellarGraph</a>, and
<a class="reference external" href="https://github.com/deepmind/graph_nets">GraphNets</a>.</p>
</div>
<div class="section" id="setup">
<h2>Setup<a class="headerlink" href="#setup" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">networkx</span> <span class="k">as</span> <span class="nn">nx</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow</span> <span class="kn">import</span> <span class="n">keras</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">layers</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">suppress</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="prepare-the-dataset">
<h2>Prepare the Dataset<a class="headerlink" href="#prepare-the-dataset" title="Permalink to this headline">¶</a></h2>
<p>The Cora dataset consists of 2,708 scientific papers classified into one of seven classes.
The citation network consists of 5,429 links. Each paper has a binary word vector of size
1,433, indicating the presence of a corresponding word.</p>
<div class="section" id="download-the-dataset">
<h3>Download the dataset<a class="headerlink" href="#download-the-dataset" title="Permalink to this headline">¶</a></h3>
<p>The dataset has two tap-separated files: <code class="docutils literal notranslate"><span class="pre">cora.cites</span></code> and <code class="docutils literal notranslate"><span class="pre">cora.content</span></code>.</p>
<ol class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">cora.cites</span></code> includes the citation records with two columns:
<code class="docutils literal notranslate"><span class="pre">cited_paper_id</span></code> (target) and <code class="docutils literal notranslate"><span class="pre">citing_paper_id</span></code> (source).</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">cora.content</span></code> includes the paper content records with 1,435 columns:
<code class="docutils literal notranslate"><span class="pre">paper_id</span></code>, <code class="docutils literal notranslate"><span class="pre">subject</span></code>, and 1,433 binary features.</p></li>
</ol>
<p>First the Cora-dataset is downloaded and extracted:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">zip_file</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">get_file</span><span class="p">(</span>
    <span class="n">fname</span><span class="o">=</span><span class="s2">&quot;cora.tgz&quot;</span><span class="p">,</span>
    <span class="n">origin</span><span class="o">=</span><span class="s2">&quot;https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz&quot;</span><span class="p">,</span>
    <span class="n">extract</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">data_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">zip_file</span><span class="p">),</span> <span class="s2">&quot;cora&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">zip_file</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;/Users/johannes/.keras/datasets/cora.tgz&#39;
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="process-and-visualize-the-dataset">
<h3>Process and visualize the dataset<a class="headerlink" href="#process-and-visualize-the-dataset" title="Permalink to this headline">¶</a></h3>
<p>Then we load the citations data into a Pandas DataFrame.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">citations</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span>
    <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="s2">&quot;cora.cites&quot;</span><span class="p">),</span>
    <span class="n">sep</span><span class="o">=</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">&quot;</span><span class="p">,</span>
    <span class="n">header</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;target&quot;</span><span class="p">,</span> <span class="s2">&quot;source&quot;</span><span class="p">],</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Citations shape:&quot;</span><span class="p">,</span> <span class="n">citations</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Citations shape: (5429, 2)
</pre></div>
</div>
</div>
</div>
<p>Now we display a sample of the <code class="docutils literal notranslate"><span class="pre">citations</span></code> DataFrame.
The <code class="docutils literal notranslate"><span class="pre">target</span></code> column includes the paper ids cited by the paper ids in the <code class="docutils literal notranslate"><span class="pre">source</span></code> column.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">citations</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>target</th>
      <th>source</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>35</td>
      <td>1033</td>
    </tr>
    <tr>
      <th>1</th>
      <td>35</td>
      <td>103482</td>
    </tr>
    <tr>
      <th>2</th>
      <td>35</td>
      <td>103515</td>
    </tr>
    <tr>
      <th>3</th>
      <td>35</td>
      <td>1050679</td>
    </tr>
    <tr>
      <th>4</th>
      <td>35</td>
      <td>1103960</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>As can be seen in the output of the next-code cell, there are some papers (e.g. paper 35), which are referenced by many others. Other papers are referenced only once or even not at all.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">citations</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>35        166
6213       76
1365       74
3229       61
114        42
         ... 
264347      1
346243      1
221302      1
143476      1
851968      1
Name: target, Length: 1565, dtype: int64
</pre></div>
</div>
</div>
</div>
<p>Now let’s load the papers data into a Pandas DataFrame.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">column_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;paper_id&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;term_</span><span class="si">{</span><span class="n">idx</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1433</span><span class="p">)]</span> <span class="o">+</span> <span class="p">[</span><span class="s2">&quot;subject&quot;</span><span class="p">]</span>
<span class="n">papers</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span>
    <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="s2">&quot;cora.content&quot;</span><span class="p">),</span> <span class="n">sep</span><span class="o">=</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="n">column_names</span><span class="p">,</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Papers shape:&quot;</span><span class="p">,</span> <span class="n">papers</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Papers shape: (2708, 1435)
</pre></div>
</div>
</div>
</div>
<p>Now we display a sample of the <code class="docutils literal notranslate"><span class="pre">papers</span></code> DataFrame. The DataFrame includes the <code class="docutils literal notranslate"><span class="pre">paper_id</span></code>
and the <code class="docutils literal notranslate"><span class="pre">subject</span></code> columns, as well as 1,433 binary column representing whether a term exists
in the paper or not.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">papers</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>paper_id</th>
      <th>term_0</th>
      <th>term_1</th>
      <th>term_2</th>
      <th>term_3</th>
      <th>term_4</th>
      <th>term_5</th>
      <th>term_6</th>
      <th>term_7</th>
      <th>term_8</th>
      <th>...</th>
      <th>term_1424</th>
      <th>term_1425</th>
      <th>term_1426</th>
      <th>term_1427</th>
      <th>term_1428</th>
      <th>term_1429</th>
      <th>term_1430</th>
      <th>term_1431</th>
      <th>term_1432</th>
      <th>subject</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>31336</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>Neural_Networks</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1061127</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>Rule_Learning</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1106406</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>Reinforcement_Learning</td>
    </tr>
    <tr>
      <th>3</th>
      <td>13195</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>Reinforcement_Learning</td>
    </tr>
    <tr>
      <th>4</th>
      <td>37879</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>Probabilistic_Methods</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 1435 columns</p>
</div></div></div>
</div>
<p>Let’s display the count of the papers in each subject.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">papers</span><span class="o">.</span><span class="n">subject</span><span class="o">.</span><span class="n">value_counts</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Neural_Networks           818
Probabilistic_Methods     426
Genetic_Algorithms        418
Theory                    351
Case_Based                298
Reinforcement_Learning    217
Rule_Learning             180
Name: subject, dtype: int64
</pre></div>
</div>
</div>
</div>
<p>We convert the paper ids and the subjects into zero-based indices.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">class_values</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">papers</span><span class="p">[</span><span class="s2">&quot;subject&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">())</span>
<span class="n">class_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">name</span><span class="p">:</span> <span class="nb">id</span> <span class="k">for</span> <span class="nb">id</span><span class="p">,</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">class_values</span><span class="p">)}</span>
<span class="n">paper_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">name</span><span class="p">:</span> <span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="n">papers</span><span class="p">[</span><span class="s2">&quot;paper_id&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">()))}</span>

<span class="n">papers</span><span class="p">[</span><span class="s2">&quot;paper_id&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">papers</span><span class="p">[</span><span class="s2">&quot;paper_id&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">name</span><span class="p">:</span> <span class="n">paper_idx</span><span class="p">[</span><span class="n">name</span><span class="p">])</span>
<span class="n">citations</span><span class="p">[</span><span class="s2">&quot;source&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">citations</span><span class="p">[</span><span class="s2">&quot;source&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">name</span><span class="p">:</span> <span class="n">paper_idx</span><span class="p">[</span><span class="n">name</span><span class="p">])</span>
<span class="n">citations</span><span class="p">[</span><span class="s2">&quot;target&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">citations</span><span class="p">[</span><span class="s2">&quot;target&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">name</span><span class="p">:</span> <span class="n">paper_idx</span><span class="p">[</span><span class="n">name</span><span class="p">])</span>
<span class="n">papers</span><span class="p">[</span><span class="s2">&quot;subject&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">papers</span><span class="p">[</span><span class="s2">&quot;subject&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">value</span><span class="p">:</span> <span class="n">class_idx</span><span class="p">[</span><span class="n">value</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">class_values</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">papers</span><span class="p">[</span><span class="s2">&quot;subject&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">())</span>
<span class="n">class_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">name</span><span class="p">:</span> <span class="nb">id</span> <span class="k">for</span> <span class="nb">id</span><span class="p">,</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">class_values</span><span class="p">)}</span>
<span class="n">paper_idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">name</span><span class="p">:</span> <span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="n">papers</span><span class="p">[</span><span class="s2">&quot;paper_id&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">()))}</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">class_idx</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">paper_idx</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{0: 0,
 1: 1,
 2: 2,
 3: 3,
 4: 4,
 5: 5,
 6: 6,
 7: 7,
 8: 8,
 9: 9,
 10: 10,
 11: 11,
 12: 12,
 13: 13,
 14: 14,
 15: 15,
 16: 16,
 17: 17,
 18: 18,
 19: 19,
 20: 20,
 21: 21,
 22: 22,
 23: 23,
 24: 24,
 25: 25,
 26: 26,
 27: 27,
 28: 28,
 29: 29,
 30: 30,
 31: 31,
 32: 32,
 33: 33,
 34: 34,
 35: 35,
 36: 36,
 37: 37,
 38: 38,
 39: 39,
 40: 40,
 41: 41,
 42: 42,
 43: 43,
 44: 44,
 45: 45,
 46: 46,
 47: 47,
 48: 48,
 49: 49,
 50: 50,
 51: 51,
 52: 52,
 53: 53,
 54: 54,
 55: 55,
 56: 56,
 57: 57,
 58: 58,
 59: 59,
 60: 60,
 61: 61,
 62: 62,
 63: 63,
 64: 64,
 65: 65,
 66: 66,
 67: 67,
 68: 68,
 69: 69,
 70: 70,
 71: 71,
 72: 72,
 73: 73,
 74: 74,
 75: 75,
 76: 76,
 77: 77,
 78: 78,
 79: 79,
 80: 80,
 81: 81,
 82: 82,
 83: 83,
 84: 84,
 85: 85,
 86: 86,
 87: 87,
 88: 88,
 89: 89,
 90: 90,
 91: 91,
 92: 92,
 93: 93,
 94: 94,
 95: 95,
 96: 96,
 97: 97,
 98: 98,
 99: 99,
 100: 100,
 101: 101,
 102: 102,
 103: 103,
 104: 104,
 105: 105,
 106: 106,
 107: 107,
 108: 108,
 109: 109,
 110: 110,
 111: 111,
 112: 112,
 113: 113,
 114: 114,
 115: 115,
 116: 116,
 117: 117,
 118: 118,
 119: 119,
 120: 120,
 121: 121,
 122: 122,
 123: 123,
 124: 124,
 125: 125,
 126: 126,
 127: 127,
 128: 128,
 129: 129,
 130: 130,
 131: 131,
 132: 132,
 133: 133,
 134: 134,
 135: 135,
 136: 136,
 137: 137,
 138: 138,
 139: 139,
 140: 140,
 141: 141,
 142: 142,
 143: 143,
 144: 144,
 145: 145,
 146: 146,
 147: 147,
 148: 148,
 149: 149,
 150: 150,
 151: 151,
 152: 152,
 153: 153,
 154: 154,
 155: 155,
 156: 156,
 157: 157,
 158: 158,
 159: 159,
 160: 160,
 161: 161,
 162: 162,
 163: 163,
 164: 164,
 165: 165,
 166: 166,
 167: 167,
 168: 168,
 169: 169,
 170: 170,
 171: 171,
 172: 172,
 173: 173,
 174: 174,
 175: 175,
 176: 176,
 177: 177,
 178: 178,
 179: 179,
 180: 180,
 181: 181,
 182: 182,
 183: 183,
 184: 184,
 185: 185,
 186: 186,
 187: 187,
 188: 188,
 189: 189,
 190: 190,
 191: 191,
 192: 192,
 193: 193,
 194: 194,
 195: 195,
 196: 196,
 197: 197,
 198: 198,
 199: 199,
 200: 200,
 201: 201,
 202: 202,
 203: 203,
 204: 204,
 205: 205,
 206: 206,
 207: 207,
 208: 208,
 209: 209,
 210: 210,
 211: 211,
 212: 212,
 213: 213,
 214: 214,
 215: 215,
 216: 216,
 217: 217,
 218: 218,
 219: 219,
 220: 220,
 221: 221,
 222: 222,
 223: 223,
 224: 224,
 225: 225,
 226: 226,
 227: 227,
 228: 228,
 229: 229,
 230: 230,
 231: 231,
 232: 232,
 233: 233,
 234: 234,
 235: 235,
 236: 236,
 237: 237,
 238: 238,
 239: 239,
 240: 240,
 241: 241,
 242: 242,
 243: 243,
 244: 244,
 245: 245,
 246: 246,
 247: 247,
 248: 248,
 249: 249,
 250: 250,
 251: 251,
 252: 252,
 253: 253,
 254: 254,
 255: 255,
 256: 256,
 257: 257,
 258: 258,
 259: 259,
 260: 260,
 261: 261,
 262: 262,
 263: 263,
 264: 264,
 265: 265,
 266: 266,
 267: 267,
 268: 268,
 269: 269,
 270: 270,
 271: 271,
 272: 272,
 273: 273,
 274: 274,
 275: 275,
 276: 276,
 277: 277,
 278: 278,
 279: 279,
 280: 280,
 281: 281,
 282: 282,
 283: 283,
 284: 284,
 285: 285,
 286: 286,
 287: 287,
 288: 288,
 289: 289,
 290: 290,
 291: 291,
 292: 292,
 293: 293,
 294: 294,
 295: 295,
 296: 296,
 297: 297,
 298: 298,
 299: 299,
 300: 300,
 301: 301,
 302: 302,
 303: 303,
 304: 304,
 305: 305,
 306: 306,
 307: 307,
 308: 308,
 309: 309,
 310: 310,
 311: 311,
 312: 312,
 313: 313,
 314: 314,
 315: 315,
 316: 316,
 317: 317,
 318: 318,
 319: 319,
 320: 320,
 321: 321,
 322: 322,
 323: 323,
 324: 324,
 325: 325,
 326: 326,
 327: 327,
 328: 328,
 329: 329,
 330: 330,
 331: 331,
 332: 332,
 333: 333,
 334: 334,
 335: 335,
 336: 336,
 337: 337,
 338: 338,
 339: 339,
 340: 340,
 341: 341,
 342: 342,
 343: 343,
 344: 344,
 345: 345,
 346: 346,
 347: 347,
 348: 348,
 349: 349,
 350: 350,
 351: 351,
 352: 352,
 353: 353,
 354: 354,
 355: 355,
 356: 356,
 357: 357,
 358: 358,
 359: 359,
 360: 360,
 361: 361,
 362: 362,
 363: 363,
 364: 364,
 365: 365,
 366: 366,
 367: 367,
 368: 368,
 369: 369,
 370: 370,
 371: 371,
 372: 372,
 373: 373,
 374: 374,
 375: 375,
 376: 376,
 377: 377,
 378: 378,
 379: 379,
 380: 380,
 381: 381,
 382: 382,
 383: 383,
 384: 384,
 385: 385,
 386: 386,
 387: 387,
 388: 388,
 389: 389,
 390: 390,
 391: 391,
 392: 392,
 393: 393,
 394: 394,
 395: 395,
 396: 396,
 397: 397,
 398: 398,
 399: 399,
 400: 400,
 401: 401,
 402: 402,
 403: 403,
 404: 404,
 405: 405,
 406: 406,
 407: 407,
 408: 408,
 409: 409,
 410: 410,
 411: 411,
 412: 412,
 413: 413,
 414: 414,
 415: 415,
 416: 416,
 417: 417,
 418: 418,
 419: 419,
 420: 420,
 421: 421,
 422: 422,
 423: 423,
 424: 424,
 425: 425,
 426: 426,
 427: 427,
 428: 428,
 429: 429,
 430: 430,
 431: 431,
 432: 432,
 433: 433,
 434: 434,
 435: 435,
 436: 436,
 437: 437,
 438: 438,
 439: 439,
 440: 440,
 441: 441,
 442: 442,
 443: 443,
 444: 444,
 445: 445,
 446: 446,
 447: 447,
 448: 448,
 449: 449,
 450: 450,
 451: 451,
 452: 452,
 453: 453,
 454: 454,
 455: 455,
 456: 456,
 457: 457,
 458: 458,
 459: 459,
 460: 460,
 461: 461,
 462: 462,
 463: 463,
 464: 464,
 465: 465,
 466: 466,
 467: 467,
 468: 468,
 469: 469,
 470: 470,
 471: 471,
 472: 472,
 473: 473,
 474: 474,
 475: 475,
 476: 476,
 477: 477,
 478: 478,
 479: 479,
 480: 480,
 481: 481,
 482: 482,
 483: 483,
 484: 484,
 485: 485,
 486: 486,
 487: 487,
 488: 488,
 489: 489,
 490: 490,
 491: 491,
 492: 492,
 493: 493,
 494: 494,
 495: 495,
 496: 496,
 497: 497,
 498: 498,
 499: 499,
 500: 500,
 501: 501,
 502: 502,
 503: 503,
 504: 504,
 505: 505,
 506: 506,
 507: 507,
 508: 508,
 509: 509,
 510: 510,
 511: 511,
 512: 512,
 513: 513,
 514: 514,
 515: 515,
 516: 516,
 517: 517,
 518: 518,
 519: 519,
 520: 520,
 521: 521,
 522: 522,
 523: 523,
 524: 524,
 525: 525,
 526: 526,
 527: 527,
 528: 528,
 529: 529,
 530: 530,
 531: 531,
 532: 532,
 533: 533,
 534: 534,
 535: 535,
 536: 536,
 537: 537,
 538: 538,
 539: 539,
 540: 540,
 541: 541,
 542: 542,
 543: 543,
 544: 544,
 545: 545,
 546: 546,
 547: 547,
 548: 548,
 549: 549,
 550: 550,
 551: 551,
 552: 552,
 553: 553,
 554: 554,
 555: 555,
 556: 556,
 557: 557,
 558: 558,
 559: 559,
 560: 560,
 561: 561,
 562: 562,
 563: 563,
 564: 564,
 565: 565,
 566: 566,
 567: 567,
 568: 568,
 569: 569,
 570: 570,
 571: 571,
 572: 572,
 573: 573,
 574: 574,
 575: 575,
 576: 576,
 577: 577,
 578: 578,
 579: 579,
 580: 580,
 581: 581,
 582: 582,
 583: 583,
 584: 584,
 585: 585,
 586: 586,
 587: 587,
 588: 588,
 589: 589,
 590: 590,
 591: 591,
 592: 592,
 593: 593,
 594: 594,
 595: 595,
 596: 596,
 597: 597,
 598: 598,
 599: 599,
 600: 600,
 601: 601,
 602: 602,
 603: 603,
 604: 604,
 605: 605,
 606: 606,
 607: 607,
 608: 608,
 609: 609,
 610: 610,
 611: 611,
 612: 612,
 613: 613,
 614: 614,
 615: 615,
 616: 616,
 617: 617,
 618: 618,
 619: 619,
 620: 620,
 621: 621,
 622: 622,
 623: 623,
 624: 624,
 625: 625,
 626: 626,
 627: 627,
 628: 628,
 629: 629,
 630: 630,
 631: 631,
 632: 632,
 633: 633,
 634: 634,
 635: 635,
 636: 636,
 637: 637,
 638: 638,
 639: 639,
 640: 640,
 641: 641,
 642: 642,
 643: 643,
 644: 644,
 645: 645,
 646: 646,
 647: 647,
 648: 648,
 649: 649,
 650: 650,
 651: 651,
 652: 652,
 653: 653,
 654: 654,
 655: 655,
 656: 656,
 657: 657,
 658: 658,
 659: 659,
 660: 660,
 661: 661,
 662: 662,
 663: 663,
 664: 664,
 665: 665,
 666: 666,
 667: 667,
 668: 668,
 669: 669,
 670: 670,
 671: 671,
 672: 672,
 673: 673,
 674: 674,
 675: 675,
 676: 676,
 677: 677,
 678: 678,
 679: 679,
 680: 680,
 681: 681,
 682: 682,
 683: 683,
 684: 684,
 685: 685,
 686: 686,
 687: 687,
 688: 688,
 689: 689,
 690: 690,
 691: 691,
 692: 692,
 693: 693,
 694: 694,
 695: 695,
 696: 696,
 697: 697,
 698: 698,
 699: 699,
 700: 700,
 701: 701,
 702: 702,
 703: 703,
 704: 704,
 705: 705,
 706: 706,
 707: 707,
 708: 708,
 709: 709,
 710: 710,
 711: 711,
 712: 712,
 713: 713,
 714: 714,
 715: 715,
 716: 716,
 717: 717,
 718: 718,
 719: 719,
 720: 720,
 721: 721,
 722: 722,
 723: 723,
 724: 724,
 725: 725,
 726: 726,
 727: 727,
 728: 728,
 729: 729,
 730: 730,
 731: 731,
 732: 732,
 733: 733,
 734: 734,
 735: 735,
 736: 736,
 737: 737,
 738: 738,
 739: 739,
 740: 740,
 741: 741,
 742: 742,
 743: 743,
 744: 744,
 745: 745,
 746: 746,
 747: 747,
 748: 748,
 749: 749,
 750: 750,
 751: 751,
 752: 752,
 753: 753,
 754: 754,
 755: 755,
 756: 756,
 757: 757,
 758: 758,
 759: 759,
 760: 760,
 761: 761,
 762: 762,
 763: 763,
 764: 764,
 765: 765,
 766: 766,
 767: 767,
 768: 768,
 769: 769,
 770: 770,
 771: 771,
 772: 772,
 773: 773,
 774: 774,
 775: 775,
 776: 776,
 777: 777,
 778: 778,
 779: 779,
 780: 780,
 781: 781,
 782: 782,
 783: 783,
 784: 784,
 785: 785,
 786: 786,
 787: 787,
 788: 788,
 789: 789,
 790: 790,
 791: 791,
 792: 792,
 793: 793,
 794: 794,
 795: 795,
 796: 796,
 797: 797,
 798: 798,
 799: 799,
 800: 800,
 801: 801,
 802: 802,
 803: 803,
 804: 804,
 805: 805,
 806: 806,
 807: 807,
 808: 808,
 809: 809,
 810: 810,
 811: 811,
 812: 812,
 813: 813,
 814: 814,
 815: 815,
 816: 816,
 817: 817,
 818: 818,
 819: 819,
 820: 820,
 821: 821,
 822: 822,
 823: 823,
 824: 824,
 825: 825,
 826: 826,
 827: 827,
 828: 828,
 829: 829,
 830: 830,
 831: 831,
 832: 832,
 833: 833,
 834: 834,
 835: 835,
 836: 836,
 837: 837,
 838: 838,
 839: 839,
 840: 840,
 841: 841,
 842: 842,
 843: 843,
 844: 844,
 845: 845,
 846: 846,
 847: 847,
 848: 848,
 849: 849,
 850: 850,
 851: 851,
 852: 852,
 853: 853,
 854: 854,
 855: 855,
 856: 856,
 857: 857,
 858: 858,
 859: 859,
 860: 860,
 861: 861,
 862: 862,
 863: 863,
 864: 864,
 865: 865,
 866: 866,
 867: 867,
 868: 868,
 869: 869,
 870: 870,
 871: 871,
 872: 872,
 873: 873,
 874: 874,
 875: 875,
 876: 876,
 877: 877,
 878: 878,
 879: 879,
 880: 880,
 881: 881,
 882: 882,
 883: 883,
 884: 884,
 885: 885,
 886: 886,
 887: 887,
 888: 888,
 889: 889,
 890: 890,
 891: 891,
 892: 892,
 893: 893,
 894: 894,
 895: 895,
 896: 896,
 897: 897,
 898: 898,
 899: 899,
 900: 900,
 901: 901,
 902: 902,
 903: 903,
 904: 904,
 905: 905,
 906: 906,
 907: 907,
 908: 908,
 909: 909,
 910: 910,
 911: 911,
 912: 912,
 913: 913,
 914: 914,
 915: 915,
 916: 916,
 917: 917,
 918: 918,
 919: 919,
 920: 920,
 921: 921,
 922: 922,
 923: 923,
 924: 924,
 925: 925,
 926: 926,
 927: 927,
 928: 928,
 929: 929,
 930: 930,
 931: 931,
 932: 932,
 933: 933,
 934: 934,
 935: 935,
 936: 936,
 937: 937,
 938: 938,
 939: 939,
 940: 940,
 941: 941,
 942: 942,
 943: 943,
 944: 944,
 945: 945,
 946: 946,
 947: 947,
 948: 948,
 949: 949,
 950: 950,
 951: 951,
 952: 952,
 953: 953,
 954: 954,
 955: 955,
 956: 956,
 957: 957,
 958: 958,
 959: 959,
 960: 960,
 961: 961,
 962: 962,
 963: 963,
 964: 964,
 965: 965,
 966: 966,
 967: 967,
 968: 968,
 969: 969,
 970: 970,
 971: 971,
 972: 972,
 973: 973,
 974: 974,
 975: 975,
 976: 976,
 977: 977,
 978: 978,
 979: 979,
 980: 980,
 981: 981,
 982: 982,
 983: 983,
 984: 984,
 985: 985,
 986: 986,
 987: 987,
 988: 988,
 989: 989,
 990: 990,
 991: 991,
 992: 992,
 993: 993,
 994: 994,
 995: 995,
 996: 996,
 997: 997,
 998: 998,
 999: 999,
 ...}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">papers</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(2708, 1435)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">citations</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>target</th>
      <th>source</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>21</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>905</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>906</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>1909</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>1940</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Now let’s visualize the citation graph. Each node in the graph represents a paper,
and the color of the node corresponds to its subject. Note that we only show a sample of
the papers in the dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">colors</span> <span class="o">=</span> <span class="n">papers</span><span class="p">[</span><span class="s2">&quot;subject&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="n">cora_graph</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">from_pandas_edgelist</span><span class="p">(</span><span class="n">citations</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">1500</span><span class="p">))</span>
<span class="n">subjects</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">papers</span><span class="p">[</span><span class="n">papers</span><span class="p">[</span><span class="s2">&quot;paper_id&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">isin</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">cora_graph</span><span class="o">.</span><span class="n">nodes</span><span class="p">))][</span><span class="s2">&quot;subject&quot;</span><span class="p">])</span>
<span class="n">nx</span><span class="o">.</span><span class="n">draw_spring</span><span class="p">(</span><span class="n">cora_graph</span><span class="p">,</span> <span class="n">node_size</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="n">subjects</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/GraphNeuralNetworks_34_0.png" src="../_images/GraphNeuralNetworks_34_0.png" />
</div>
</div>
</div>
<div class="section" id="split-the-dataset-into-stratified-train-and-test-sets">
<h3>Split the dataset into stratified train and test sets<a class="headerlink" href="#split-the-dataset-into-stratified-train-and-test-sets" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_data</span><span class="p">,</span> <span class="n">test_data</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">group_data</span> <span class="ow">in</span> <span class="n">papers</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;subject&quot;</span><span class="p">):</span>
    <span class="c1"># Select around 85% of the dataset for training and validation, and 15% for test</span>
    <span class="n">random_selection</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">group_data</span><span class="o">.</span><span class="n">index</span><span class="p">))</span> <span class="o">&lt;=</span> <span class="mf">0.5</span>
    <span class="n">train_data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">group_data</span><span class="p">[</span><span class="n">random_selection</span><span class="p">])</span>
    <span class="n">test_data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">group_data</span><span class="p">[</span><span class="o">~</span><span class="n">random_selection</span><span class="p">])</span>

<span class="n">train_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">frac</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">test_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">test_data</span><span class="p">)</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">frac</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Train data shape:&quot;</span><span class="p">,</span> <span class="n">train_data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test data shape:&quot;</span><span class="p">,</span> <span class="n">test_data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Train data shape: (1353, 1435)
Test data shape: (1355, 1435)
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="implement-train-and-evaluate-experiment">
<h2>Implement, Train and Evaluate Experiment<a class="headerlink" href="#implement-train-and-evaluate-experiment" title="Permalink to this headline">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">hidden_units</span> <span class="o">=</span> <span class="p">[</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">]</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">dropout_rate</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">300</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">256</span>
</pre></div>
</div>
</div>
</div>
<p>This function compiles and trains an input model using the given training data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">run_experiment</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">):</span>
    <span class="c1"># Compile the model.</span>
    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
        <span class="n">optimizer</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">),</span>
        <span class="n">loss</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">keras</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">SparseCategoricalAccuracy</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;acc&quot;</span><span class="p">)],</span>
    <span class="p">)</span>
    <span class="c1"># Create an early stopping callback.</span>
    <span class="n">early_stopping</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">EarlyStopping</span><span class="p">(</span>
        <span class="n">monitor</span><span class="o">=</span><span class="s2">&quot;val_acc&quot;</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">restore_best_weights</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="c1"># Fit the model.</span>
    <span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
        <span class="n">x</span><span class="o">=</span><span class="n">x_train</span><span class="p">,</span>
        <span class="n">y</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span>
        <span class="n">epochs</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
        <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">early_stopping</span><span class="p">],</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">history</span>
</pre></div>
</div>
</div>
</div>
<p>This function displays the loss and accuracy curves of the model during training.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">display_learning_curves</span><span class="p">(</span><span class="n">history</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

    <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">])</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s2">&quot;val_loss&quot;</span><span class="p">])</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="s2">&quot;test&quot;</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper right&quot;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Epochs&quot;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Loss&quot;</span><span class="p">)</span>

    <span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s2">&quot;acc&quot;</span><span class="p">])</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s2">&quot;val_acc&quot;</span><span class="p">])</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="s2">&quot;test&quot;</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper right&quot;</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Epochs&quot;</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Accuracy&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="implement-feedforward-network-ffn-module">
<h2>Implement Feedforward Network (FFN) Module<a class="headerlink" href="#implement-feedforward-network-ffn-module" title="Permalink to this headline">¶</a></h2>
<p>We will use this module in the baseline and the GNN models.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">create_ffn</span><span class="p">(</span><span class="n">hidden_units</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">fnn_layers</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">units</span> <span class="ow">in</span> <span class="n">hidden_units</span><span class="p">:</span>
        <span class="n">fnn_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">BatchNormalization</span><span class="p">())</span>
        <span class="n">fnn_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">))</span>
        <span class="n">fnn_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">gelu</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">fnn_layers</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="build-a-baseline-neural-network-model">
<h2>Build a Baseline Neural Network Model<a class="headerlink" href="#build-a-baseline-neural-network-model" title="Permalink to this headline">¶</a></h2>
<div class="section" id="prepare-the-data-for-the-baseline-model">
<h3>Prepare the data for the baseline model<a class="headerlink" href="#prepare-the-data-for-the-baseline-model" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">feature_names</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">papers</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span> <span class="o">-</span> <span class="p">{</span><span class="s2">&quot;paper_id&quot;</span><span class="p">,</span> <span class="s2">&quot;subject&quot;</span><span class="p">}</span>
<span class="n">num_features</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">feature_names</span><span class="p">)</span>
<span class="n">num_classes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">class_idx</span><span class="p">)</span>

<span class="c1"># Create train and test features as a numpy array.</span>
<span class="n">x_train</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">[</span><span class="n">feature_names</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">test_data</span><span class="p">[</span><span class="n">feature_names</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="c1"># Create train and test targets as a numpy array.</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">[</span><span class="s2">&quot;subject&quot;</span><span class="p">]</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">test_data</span><span class="p">[</span><span class="s2">&quot;subject&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">num_classes</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>7
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="implement-a-baseline-classifier">
<h3>Implement a baseline classifier<a class="headerlink" href="#implement-a-baseline-classifier" title="Permalink to this headline">¶</a></h3>
<p>We add five FFN blocks with skip connections, so that we generate a baseline model with
roughly the same number of parameters as the GNN models to be built later.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">create_baseline_model</span><span class="p">(</span><span class="n">hidden_units</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">num_features</span><span class="p">,),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;input_features&quot;</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">create_ffn</span><span class="p">(</span><span class="n">hidden_units</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;ffn_block1&quot;</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">block_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
        <span class="c1"># Create an FFN block.</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="n">create_ffn</span><span class="p">(</span><span class="n">hidden_units</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;ffn_block</span><span class="si">{</span><span class="n">block_idx</span> <span class="o">+</span> <span class="mi">2</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># Add skip connection.</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Add</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;skip_connection</span><span class="si">{</span><span class="n">block_idx</span> <span class="o">+</span> <span class="mi">2</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)([</span><span class="n">x</span><span class="p">,</span> <span class="n">x1</span><span class="p">])</span>
    <span class="c1"># Compute logits.</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;logits&quot;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="c1"># Create the model.</span>
    <span class="k">return</span> <span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;baseline&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">baseline_model</span> <span class="o">=</span> <span class="n">create_baseline_model</span><span class="p">(</span><span class="n">hidden_units</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">)</span>
<span class="n">baseline_model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;baseline&quot;
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_features (InputLayer)     [(None, 1433)]       0                                            
__________________________________________________________________________________________________
ffn_block1 (Sequential)         (None, 32)           52804       input_features[0][0]             
__________________________________________________________________________________________________
ffn_block2 (Sequential)         (None, 32)           2368        ffn_block1[0][0]                 
__________________________________________________________________________________________________
skip_connection2 (Add)          (None, 32)           0           ffn_block1[0][0]                 
                                                                 ffn_block2[0][0]                 
__________________________________________________________________________________________________
ffn_block3 (Sequential)         (None, 32)           2368        skip_connection2[0][0]           
__________________________________________________________________________________________________
skip_connection3 (Add)          (None, 32)           0           skip_connection2[0][0]           
                                                                 ffn_block3[0][0]                 
__________________________________________________________________________________________________
ffn_block4 (Sequential)         (None, 32)           2368        skip_connection3[0][0]           
__________________________________________________________________________________________________
skip_connection4 (Add)          (None, 32)           0           skip_connection3[0][0]           
                                                                 ffn_block4[0][0]                 
__________________________________________________________________________________________________
ffn_block5 (Sequential)         (None, 32)           2368        skip_connection4[0][0]           
__________________________________________________________________________________________________
skip_connection5 (Add)          (None, 32)           0           skip_connection4[0][0]           
                                                                 ffn_block5[0][0]                 
__________________________________________________________________________________________________
logits (Dense)                  (None, 7)            231         skip_connection5[0][0]           
==================================================================================================
Total params: 62,507
Trainable params: 59,065
Non-trainable params: 3,442
__________________________________________________________________________________________________
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="train-the-baseline-classifier">
<h3>Train the baseline classifier<a class="headerlink" href="#train-the-baseline-classifier" title="Permalink to this headline">¶</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">history</span> <span class="o">=</span> <span class="n">run_experiment</span><span class="p">(</span><span class="n">baseline_model</span><span class="p">,</span> <span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/300
4/4 [==============================] - 4s 300ms/step - loss: 5.3522 - acc: 0.1780 - val_loss: 1.8969 - val_acc: 0.2980
Epoch 2/300
4/4 [==============================] - 0s 24ms/step - loss: 2.7805 - acc: 0.2749 - val_loss: 1.8857 - val_acc: 0.2291
Epoch 3/300
4/4 [==============================] - 0s 23ms/step - loss: 2.6535 - acc: 0.2432 - val_loss: 1.8783 - val_acc: 0.1921
Epoch 4/300
4/4 [==============================] - 0s 23ms/step - loss: 2.4708 - acc: 0.2424 - val_loss: 1.8801 - val_acc: 0.2044
Epoch 5/300
4/4 [==============================] - 0s 24ms/step - loss: 2.1091 - acc: 0.2907 - val_loss: 1.8651 - val_acc: 0.2266
Epoch 6/300
4/4 [==============================] - 0s 24ms/step - loss: 2.0275 - acc: 0.3066 - val_loss: 1.8430 - val_acc: 0.3399
Epoch 7/300
4/4 [==============================] - 0s 25ms/step - loss: 1.8997 - acc: 0.3226 - val_loss: 1.8275 - val_acc: 0.3744
Epoch 8/300
4/4 [==============================] - 0s 24ms/step - loss: 1.7804 - acc: 0.3469 - val_loss: 1.8191 - val_acc: 0.3941
Epoch 9/300
4/4 [==============================] - 0s 25ms/step - loss: 1.7199 - acc: 0.3798 - val_loss: 1.7960 - val_acc: 0.3424
Epoch 10/300
4/4 [==============================] - 0s 24ms/step - loss: 1.6581 - acc: 0.3958 - val_loss: 1.7562 - val_acc: 0.4163
Epoch 11/300
4/4 [==============================] - 0s 24ms/step - loss: 1.5872 - acc: 0.4011 - val_loss: 1.7238 - val_acc: 0.4089
Epoch 12/300
4/4 [==============================] - 0s 24ms/step - loss: 1.5654 - acc: 0.4064 - val_loss: 1.6928 - val_acc: 0.4089
Epoch 13/300
4/4 [==============================] - 0s 24ms/step - loss: 1.4920 - acc: 0.4409 - val_loss: 1.6575 - val_acc: 0.4286
Epoch 14/300
4/4 [==============================] - 0s 23ms/step - loss: 1.4198 - acc: 0.4782 - val_loss: 1.6232 - val_acc: 0.4606
Epoch 15/300
4/4 [==============================] - 0s 22ms/step - loss: 1.3824 - acc: 0.4998 - val_loss: 1.5750 - val_acc: 0.4852
Epoch 16/300
4/4 [==============================] - 0s 22ms/step - loss: 1.3342 - acc: 0.5211 - val_loss: 1.5366 - val_acc: 0.4951
Epoch 17/300
4/4 [==============================] - 0s 21ms/step - loss: 1.2409 - acc: 0.5408 - val_loss: 1.4992 - val_acc: 0.5099
Epoch 18/300
4/4 [==============================] - 0s 20ms/step - loss: 1.1949 - acc: 0.5822 - val_loss: 1.4678 - val_acc: 0.5148
Epoch 19/300
4/4 [==============================] - 0s 20ms/step - loss: 1.1651 - acc: 0.5965 - val_loss: 1.4304 - val_acc: 0.5246
Epoch 20/300
4/4 [==============================] - 0s 19ms/step - loss: 1.0727 - acc: 0.6189 - val_loss: 1.3719 - val_acc: 0.5296
Epoch 21/300
4/4 [==============================] - 0s 20ms/step - loss: 1.0518 - acc: 0.6357 - val_loss: 1.3284 - val_acc: 0.5246
Epoch 22/300
4/4 [==============================] - 0s 19ms/step - loss: 0.9438 - acc: 0.6635 - val_loss: 1.3104 - val_acc: 0.5246
Epoch 23/300
4/4 [==============================] - 0s 20ms/step - loss: 0.9122 - acc: 0.6902 - val_loss: 1.3057 - val_acc: 0.5148
Epoch 24/300
4/4 [==============================] - 0s 20ms/step - loss: 0.9141 - acc: 0.6766 - val_loss: 1.3074 - val_acc: 0.5148
Epoch 25/300
4/4 [==============================] - 0s 20ms/step - loss: 0.8288 - acc: 0.7181 - val_loss: 1.2484 - val_acc: 0.5517
Epoch 26/300
4/4 [==============================] - 0s 18ms/step - loss: 0.8149 - acc: 0.7163 - val_loss: 1.2105 - val_acc: 0.5542
Epoch 27/300
4/4 [==============================] - 0s 20ms/step - loss: 0.7968 - acc: 0.7314 - val_loss: 1.1684 - val_acc: 0.5690
Epoch 28/300
4/4 [==============================] - 0s 22ms/step - loss: 0.7300 - acc: 0.7606 - val_loss: 1.1168 - val_acc: 0.5911
Epoch 29/300
4/4 [==============================] - 0s 20ms/step - loss: 0.7636 - acc: 0.7336 - val_loss: 1.1093 - val_acc: 0.5862
Epoch 30/300
4/4 [==============================] - 0s 22ms/step - loss: 0.6324 - acc: 0.7836 - val_loss: 1.1045 - val_acc: 0.5739
Epoch 31/300
4/4 [==============================] - 0s 24ms/step - loss: 0.6194 - acc: 0.7838 - val_loss: 1.1131 - val_acc: 0.5714
Epoch 32/300
4/4 [==============================] - 0s 24ms/step - loss: 0.5732 - acc: 0.8029 - val_loss: 1.1018 - val_acc: 0.5887
Epoch 33/300
4/4 [==============================] - 0s 24ms/step - loss: 0.6436 - acc: 0.7705 - val_loss: 1.1398 - val_acc: 0.5788
Epoch 34/300
4/4 [==============================] - 0s 25ms/step - loss: 0.5409 - acc: 0.8183 - val_loss: 1.1217 - val_acc: 0.5788
Epoch 35/300
4/4 [==============================] - 0s 24ms/step - loss: 0.6190 - acc: 0.7852 - val_loss: 1.1396 - val_acc: 0.5739
Epoch 36/300
4/4 [==============================] - 0s 22ms/step - loss: 0.6111 - acc: 0.7813 - val_loss: 1.1102 - val_acc: 0.5788
Epoch 37/300
4/4 [==============================] - 0s 24ms/step - loss: 0.5327 - acc: 0.8173 - val_loss: 1.0313 - val_acc: 0.6108
Epoch 38/300
4/4 [==============================] - 0s 24ms/step - loss: 0.5404 - acc: 0.8221 - val_loss: 0.9661 - val_acc: 0.6527
Epoch 39/300
4/4 [==============================] - 0s 25ms/step - loss: 0.5440 - acc: 0.8130 - val_loss: 0.8942 - val_acc: 0.6798
Epoch 40/300
4/4 [==============================] - 0s 25ms/step - loss: 0.4912 - acc: 0.8382 - val_loss: 0.8475 - val_acc: 0.6970
Epoch 41/300
4/4 [==============================] - 0s 24ms/step - loss: 0.4693 - acc: 0.8471 - val_loss: 0.8311 - val_acc: 0.7044
Epoch 42/300
4/4 [==============================] - 0s 25ms/step - loss: 0.4881 - acc: 0.8229 - val_loss: 0.8260 - val_acc: 0.7143
Epoch 43/300
4/4 [==============================] - 0s 23ms/step - loss: 0.5774 - acc: 0.8057 - val_loss: 0.8223 - val_acc: 0.7167
Epoch 44/300
4/4 [==============================] - 0s 24ms/step - loss: 0.4929 - acc: 0.8413 - val_loss: 0.7844 - val_acc: 0.7291
Epoch 45/300
4/4 [==============================] - 0s 22ms/step - loss: 0.5215 - acc: 0.8166 - val_loss: 0.7863 - val_acc: 0.7512
Epoch 46/300
4/4 [==============================] - 0s 22ms/step - loss: 0.5190 - acc: 0.8082 - val_loss: 0.8038 - val_acc: 0.7537
Epoch 47/300
4/4 [==============================] - 0s 22ms/step - loss: 0.4560 - acc: 0.8441 - val_loss: 0.7901 - val_acc: 0.7463
Epoch 48/300
4/4 [==============================] - 0s 20ms/step - loss: 0.4957 - acc: 0.8206 - val_loss: 0.7734 - val_acc: 0.7315
Epoch 49/300
4/4 [==============================] - 0s 21ms/step - loss: 0.4361 - acc: 0.8612 - val_loss: 0.7983 - val_acc: 0.7167
Epoch 50/300
4/4 [==============================] - 0s 21ms/step - loss: 0.4455 - acc: 0.8445 - val_loss: 0.7975 - val_acc: 0.7094
Epoch 51/300
4/4 [==============================] - 0s 26ms/step - loss: 0.4158 - acc: 0.8616 - val_loss: 0.7616 - val_acc: 0.7438
Epoch 52/300
4/4 [==============================] - 0s 23ms/step - loss: 0.4022 - acc: 0.8525 - val_loss: 0.7440 - val_acc: 0.7512
Epoch 53/300
4/4 [==============================] - 0s 22ms/step - loss: 0.4550 - acc: 0.8498 - val_loss: 0.7500 - val_acc: 0.7488
Epoch 54/300
4/4 [==============================] - 0s 24ms/step - loss: 0.4162 - acc: 0.8621 - val_loss: 0.7827 - val_acc: 0.7266
Epoch 55/300
4/4 [==============================] - 0s 25ms/step - loss: 0.4257 - acc: 0.8446 - val_loss: 0.7798 - val_acc: 0.7389
Epoch 56/300
4/4 [==============================] - 0s 24ms/step - loss: 0.4507 - acc: 0.8400 - val_loss: 0.7519 - val_acc: 0.7438
Epoch 57/300
4/4 [==============================] - 0s 25ms/step - loss: 0.4080 - acc: 0.8621 - val_loss: 0.7299 - val_acc: 0.7463
Epoch 58/300
4/4 [==============================] - 0s 25ms/step - loss: 0.4382 - acc: 0.8482 - val_loss: 0.7199 - val_acc: 0.7463
Epoch 59/300
4/4 [==============================] - 0s 23ms/step - loss: 0.4226 - acc: 0.8536 - val_loss: 0.7423 - val_acc: 0.7340
Epoch 60/300
4/4 [==============================] - 0s 24ms/step - loss: 0.4122 - acc: 0.8641 - val_loss: 0.7533 - val_acc: 0.7192
Epoch 61/300
4/4 [==============================] - 0s 22ms/step - loss: 0.4062 - acc: 0.8583 - val_loss: 0.7262 - val_acc: 0.7340
Epoch 62/300
4/4 [==============================] - 0s 24ms/step - loss: 0.4168 - acc: 0.8524 - val_loss: 0.7119 - val_acc: 0.7488
Epoch 63/300
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4/4 [==============================] - 0s 21ms/step - loss: 0.3668 - acc: 0.8708 - val_loss: 0.7258 - val_acc: 0.7488
Epoch 64/300
4/4 [==============================] - 0s 20ms/step - loss: 0.3812 - acc: 0.8744 - val_loss: 0.7432 - val_acc: 0.7488
Epoch 65/300
4/4 [==============================] - 0s 19ms/step - loss: 0.3797 - acc: 0.8767 - val_loss: 0.7273 - val_acc: 0.7562
Epoch 66/300
4/4 [==============================] - 0s 20ms/step - loss: 0.3506 - acc: 0.8847 - val_loss: 0.7070 - val_acc: 0.7586
Epoch 67/300
4/4 [==============================] - 0s 19ms/step - loss: 0.3931 - acc: 0.8640 - val_loss: 0.6947 - val_acc: 0.7660
Epoch 68/300
4/4 [==============================] - 0s 20ms/step - loss: 0.3532 - acc: 0.8761 - val_loss: 0.6885 - val_acc: 0.7783
Epoch 69/300
4/4 [==============================] - 0s 19ms/step - loss: 0.3800 - acc: 0.8594 - val_loss: 0.6982 - val_acc: 0.7685
Epoch 70/300
4/4 [==============================] - 0s 19ms/step - loss: 0.3804 - acc: 0.8655 - val_loss: 0.7153 - val_acc: 0.7660
Epoch 71/300
4/4 [==============================] - 0s 19ms/step - loss: 0.4175 - acc: 0.8601 - val_loss: 0.7182 - val_acc: 0.7635
Epoch 72/300
4/4 [==============================] - 0s 19ms/step - loss: 0.4269 - acc: 0.8330 - val_loss: 0.7006 - val_acc: 0.7709
Epoch 73/300
4/4 [==============================] - 0s 20ms/step - loss: 0.3542 - acc: 0.8722 - val_loss: 0.6951 - val_acc: 0.7685
Epoch 74/300
4/4 [==============================] - 0s 21ms/step - loss: 0.3884 - acc: 0.8665 - val_loss: 0.6918 - val_acc: 0.7759
Epoch 75/300
4/4 [==============================] - 0s 20ms/step - loss: 0.3921 - acc: 0.8521 - val_loss: 0.7014 - val_acc: 0.7709
Epoch 76/300
4/4 [==============================] - 0s 20ms/step - loss: 0.3897 - acc: 0.8693 - val_loss: 0.7038 - val_acc: 0.7734
Epoch 77/300
4/4 [==============================] - 0s 20ms/step - loss: 0.2996 - acc: 0.9046 - val_loss: 0.7024 - val_acc: 0.7685
Epoch 78/300
4/4 [==============================] - 0s 21ms/step - loss: 0.3001 - acc: 0.8814 - val_loss: 0.7071 - val_acc: 0.7660
Epoch 79/300
4/4 [==============================] - 0s 20ms/step - loss: 0.3693 - acc: 0.8767 - val_loss: 0.7051 - val_acc: 0.7709
Epoch 80/300
4/4 [==============================] - 0s 22ms/step - loss: 0.3478 - acc: 0.8700 - val_loss: 0.7026 - val_acc: 0.7635
Epoch 81/300
4/4 [==============================] - 0s 21ms/step - loss: 0.4196 - acc: 0.8655 - val_loss: 0.6949 - val_acc: 0.7660
Epoch 82/300
4/4 [==============================] - 0s 20ms/step - loss: 0.3576 - acc: 0.8643 - val_loss: 0.6825 - val_acc: 0.7734
Epoch 83/300
4/4 [==============================] - 0s 22ms/step - loss: 0.3608 - acc: 0.8815 - val_loss: 0.6944 - val_acc: 0.7734
Epoch 84/300
4/4 [==============================] - 0s 20ms/step - loss: 0.4049 - acc: 0.8665 - val_loss: 0.7019 - val_acc: 0.7759
Epoch 85/300
4/4 [==============================] - 0s 20ms/step - loss: 0.3475 - acc: 0.8744 - val_loss: 0.7013 - val_acc: 0.7759
Epoch 86/300
4/4 [==============================] - 0s 20ms/step - loss: 0.3193 - acc: 0.8853 - val_loss: 0.7127 - val_acc: 0.7660
Epoch 87/300
4/4 [==============================] - 0s 20ms/step - loss: 0.4019 - acc: 0.8660 - val_loss: 0.7122 - val_acc: 0.7611
Epoch 88/300
4/4 [==============================] - 0s 20ms/step - loss: 0.3630 - acc: 0.8639 - val_loss: 0.6912 - val_acc: 0.7734
Epoch 89/300
4/4 [==============================] - 0s 20ms/step - loss: 0.3356 - acc: 0.8814 - val_loss: 0.6927 - val_acc: 0.7709
Epoch 90/300
4/4 [==============================] - 0s 20ms/step - loss: 0.3852 - acc: 0.8608 - val_loss: 0.6892 - val_acc: 0.7709
Epoch 91/300
4/4 [==============================] - 0s 20ms/step - loss: 0.3119 - acc: 0.9011 - val_loss: 0.6914 - val_acc: 0.7734
Epoch 92/300
4/4 [==============================] - 0s 20ms/step - loss: 0.3228 - acc: 0.8925 - val_loss: 0.7123 - val_acc: 0.7512
Epoch 93/300
4/4 [==============================] - 0s 21ms/step - loss: 0.3435 - acc: 0.8805 - val_loss: 0.7149 - val_acc: 0.7537
Epoch 94/300
4/4 [==============================] - 0s 20ms/step - loss: 0.3325 - acc: 0.8860 - val_loss: 0.7327 - val_acc: 0.7660
Epoch 95/300
4/4 [==============================] - 0s 20ms/step - loss: 0.3778 - acc: 0.8658 - val_loss: 0.7689 - val_acc: 0.7537
Epoch 96/300
4/4 [==============================] - 0s 19ms/step - loss: 0.3226 - acc: 0.8846 - val_loss: 0.7793 - val_acc: 0.7488
Epoch 97/300
4/4 [==============================] - 0s 20ms/step - loss: 0.3407 - acc: 0.8879 - val_loss: 0.7638 - val_acc: 0.7537
Epoch 98/300
4/4 [==============================] - 0s 21ms/step - loss: 0.3735 - acc: 0.8673 - val_loss: 0.7451 - val_acc: 0.7685
Epoch 99/300
4/4 [==============================] - 0s 22ms/step - loss: 0.3512 - acc: 0.8726 - val_loss: 0.7338 - val_acc: 0.7537
Epoch 100/300
4/4 [==============================] - 0s 20ms/step - loss: 0.3410 - acc: 0.8914 - val_loss: 0.7329 - val_acc: 0.7611
Epoch 101/300
4/4 [==============================] - 0s 21ms/step - loss: 0.3106 - acc: 0.9060 - val_loss: 0.7480 - val_acc: 0.7660
Epoch 102/300
4/4 [==============================] - 0s 21ms/step - loss: 0.3429 - acc: 0.8873 - val_loss: 0.7723 - val_acc: 0.7660
Epoch 103/300
4/4 [==============================] - 0s 21ms/step - loss: 0.3363 - acc: 0.8854 - val_loss: 0.7660 - val_acc: 0.7611
Epoch 104/300
4/4 [==============================] - 0s 21ms/step - loss: 0.3218 - acc: 0.8819 - val_loss: 0.7597 - val_acc: 0.7635
Epoch 105/300
4/4 [==============================] - 0s 21ms/step - loss: 0.3259 - acc: 0.8830 - val_loss: 0.7873 - val_acc: 0.7562
Epoch 106/300
4/4 [==============================] - 0s 20ms/step - loss: 0.3144 - acc: 0.8928 - val_loss: 0.7995 - val_acc: 0.7586
Epoch 107/300
4/4 [==============================] - 0s 23ms/step - loss: 0.2915 - acc: 0.8981 - val_loss: 0.7906 - val_acc: 0.7537
Epoch 108/300
4/4 [==============================] - 0s 22ms/step - loss: 0.3119 - acc: 0.8999 - val_loss: 0.8118 - val_acc: 0.7611
Epoch 109/300
4/4 [==============================] - 0s 21ms/step - loss: 0.3589 - acc: 0.8903 - val_loss: 0.8442 - val_acc: 0.7488
Epoch 110/300
4/4 [==============================] - 0s 21ms/step - loss: 0.2876 - acc: 0.9120 - val_loss: 0.8550 - val_acc: 0.7463
Epoch 111/300
4/4 [==============================] - 0s 22ms/step - loss: 0.2903 - acc: 0.8938 - val_loss: 0.8331 - val_acc: 0.7562
Epoch 112/300
4/4 [==============================] - 0s 22ms/step - loss: 0.3183 - acc: 0.8834 - val_loss: 0.8244 - val_acc: 0.7537
Epoch 113/300
4/4 [==============================] - 0s 20ms/step - loss: 0.2898 - acc: 0.8958 - val_loss: 0.8213 - val_acc: 0.7562
Epoch 114/300
4/4 [==============================] - 0s 20ms/step - loss: 0.3144 - acc: 0.8861 - val_loss: 0.8198 - val_acc: 0.7537
Epoch 115/300
4/4 [==============================] - 0s 19ms/step - loss: 0.3394 - acc: 0.8910 - val_loss: 0.8359 - val_acc: 0.7562
Epoch 116/300
4/4 [==============================] - 0s 20ms/step - loss: 0.3722 - acc: 0.8746 - val_loss: 0.8455 - val_acc: 0.7562
Epoch 117/300
4/4 [==============================] - 0s 22ms/step - loss: 0.3565 - acc: 0.8729 - val_loss: 0.8356 - val_acc: 0.7586
Epoch 118/300
4/4 [==============================] - 0s 21ms/step - loss: 0.2833 - acc: 0.8940 - val_loss: 0.8337 - val_acc: 0.7562
</pre></div>
</div>
</div>
</div>
<p>Let’s plot the learning curves.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">display_learning_curves</span><span class="p">(</span><span class="n">history</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/GraphNeuralNetworks_54_0.png" src="../_images/GraphNeuralNetworks_54_0.png" />
</div>
</div>
<p>Now we evaluate the baseline model on the test data split.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">_</span><span class="p">,</span> <span class="n">test_accuracy</span> <span class="o">=</span> <span class="n">baseline_model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Test accuracy: </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="n">test_accuracy</span> <span class="o">*</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Test accuracy: 72.84%
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="examine-the-baseline-model-predictions">
<h3>Examine the baseline model predictions<a class="headerlink" href="#examine-the-baseline-model-predictions" title="Permalink to this headline">¶</a></h3>
<p>Let’s create new data instances by randomly generating binary word vectors with respect to
the word presence probabilities.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">generate_random_instances</span><span class="p">(</span><span class="n">num_instances</span><span class="p">):</span>
    <span class="n">token_probability</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">instances</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_instances</span><span class="p">):</span>
        <span class="n">probabilities</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">token_probability</span><span class="p">))</span>
        <span class="n">instance</span> <span class="o">=</span> <span class="p">(</span><span class="n">probabilities</span> <span class="o">&lt;=</span> <span class="n">token_probability</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="n">instances</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">instance</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">instances</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">display_class_probabilities</span><span class="p">(</span><span class="n">probabilities</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">instance_idx</span><span class="p">,</span> <span class="n">probs</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">probabilities</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Instance </span><span class="si">{</span><span class="n">instance_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s2">:&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">class_idx</span><span class="p">,</span> <span class="n">prob</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">probs</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;- </span><span class="si">{</span><span class="n">class_values</span><span class="p">[</span><span class="n">class_idx</span><span class="p">]</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="n">prob</span> <span class="o">*</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now we show the baseline model predictions given these randomly generated instances.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">new_instances</span> <span class="o">=</span> <span class="n">generate_random_instances</span><span class="p">(</span><span class="n">num_classes</span><span class="p">)</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">baseline_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">new_instances</span><span class="p">)</span>
<span class="n">probabilities</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">activations</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">logits</span><span class="p">))</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">display_class_probabilities</span><span class="p">(</span><span class="n">probabilities</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Instance 1:
- 0: 4.34%
- 1: 31.56%
- 2: 28.02%
- 3: 13.85%
- 4: 14.08%
- 5: 2.46%
- 6: 5.7%
Instance 2:
- 0: 0.57%
- 1: 6.75%
- 2: 28.0%
- 3: 48.49%
- 4: 11.08%
- 5: 1.89%
- 6: 3.22%
Instance 3:
- 0: 0.12%
- 1: 1.92%
- 2: 54.04%
- 3: 28.46%
- 4: 5.09%
- 5: 0.73%
- 6: 9.65%
Instance 4:
- 0: 11.87%
- 1: 15.76%
- 2: 3.08%
- 3: 63.7%
- 4: 2.88%
- 5: 1.54%
- 6: 1.18%
Instance 5:
- 0: 2.28%
- 1: 42.63%
- 2: 6.96%
- 3: 12.47%
- 4: 17.69%
- 5: 5.1%
- 6: 12.86%
Instance 6:
- 0: 0.05%
- 1: 2.42%
- 2: 94.55%
- 3: 0.5%
- 4: 1.64%
- 5: 0.1%
- 6: 0.75%
Instance 7:
- 0: 5.22%
- 1: 5.25%
- 2: 8.5%
- 3: 4.47%
- 4: 4.61%
- 5: 44.63%
- 6: 27.32%
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="build-a-graph-neural-network-model">
<h2>Build a Graph Neural Network Model<a class="headerlink" href="#build-a-graph-neural-network-model" title="Permalink to this headline">¶</a></h2>
<p><a id='graphinfo'></a></p>
<div class="section" id="prepare-the-data-for-the-graph-model">
<h3>Prepare the data for the graph model<a class="headerlink" href="#prepare-the-data-for-the-graph-model" title="Permalink to this headline">¶</a></h3>
<p>Preparing and loading the graphs data into the model for training is the most challenging
part in GNN models, which is addressed in different ways by the specialised libraries.
In this example, we show a simple approach for preparing and using graph data that is suitable
if your dataset consists of a single graph that fits entirely in memory.</p>
<p>The graph data is represented by the <code class="docutils literal notranslate"><span class="pre">graph_info</span></code> tuple, which consists of the following
three elements:</p>
<ol class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">node_features</span></code>: This is a <code class="docutils literal notranslate"><span class="pre">[num_nodes,</span> <span class="pre">num_features]</span></code> NumPy array that includes the
node features. In this dataset, the nodes are the papers, and the <code class="docutils literal notranslate"><span class="pre">node_features</span></code> are the
word-presence binary vectors of each paper.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">edges</span></code>:  This is a <code class="docutils literal notranslate"><span class="pre">[2,</span> <span class="pre">num_edges]</span></code> NumPy array. The first column contains the id of the source-node and the second column contains the id of the target node of an edge. However, in this application we do not consider directed edges, i.e. it does matter which of the two nodes is the target and the source, respectively.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">edge_weights</span></code> (optional): This is a <code class="docutils literal notranslate"><span class="pre">[num_edges]</span></code> NumPy array that includes the edge weights, which <em>quantify</em>
the relationships between nodes in the graph. In this example, there are no weights for the paper citations.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create an edges array (sparse adjacency matrix) of shape [2, num_edges].</span>
<span class="n">edges</span> <span class="o">=</span> <span class="n">citations</span><span class="p">[[</span><span class="s2">&quot;source&quot;</span><span class="p">,</span> <span class="s2">&quot;target&quot;</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">T</span>
<span class="c1"># Create an edge weights array of ones.</span>
<span class="n">edge_weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">edges</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="c1"># Create a node features array of shape [num_nodes, num_features].</span>
<span class="n">node_features</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span>
    <span class="n">papers</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s2">&quot;paper_id&quot;</span><span class="p">)[</span><span class="n">feature_names</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">dtypes</span><span class="o">.</span><span class="n">float32</span>
<span class="p">)</span>
<span class="c1"># Create graph info tuple with node_features, edges, and edge_weights.</span>
<span class="n">graph_info</span> <span class="o">=</span> <span class="p">(</span><span class="n">node_features</span><span class="p">,</span> <span class="n">edges</span><span class="p">,</span> <span class="n">edge_weights</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Edges shape:&quot;</span><span class="p">,</span> <span class="n">edges</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Edge-Weight shape:&quot;</span><span class="p">,</span> <span class="n">edge_weights</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Nodes shape:&quot;</span><span class="p">,</span> <span class="n">node_features</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Edges shape: (2, 5429)
Edge-Weight shape: (5429,)
Nodes shape: (2708, 1433)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">edges</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[  21,  905,  906, ..., 2586, 1874, 2707],
       [   0,    0,    0, ..., 1874, 1876, 1897]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">edge_weights</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;tf.Tensor: shape=(5429,), dtype=float32, numpy=array([1., 1., 1., ..., 1., 1., 1.], dtype=float32)&gt;
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="implement-a-graph-convolution-layer">
<h3>Implement a graph convolution layer<a class="headerlink" href="#implement-a-graph-convolution-layer" title="Permalink to this headline">¶</a></h3>
<p>We implement a graph convolution module as a <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer?version=nightly">Keras Layer</a>.
Our <code class="docutils literal notranslate"><span class="pre">GraphConvLayer</span></code> performs the following steps:</p>
<ol class="simple">
<li><p><strong>Prepare</strong>: The input node representations are processed using a FFN to produce a <em>message</em>. You can simplify
the processing by only applying linear transformation to the representations.</p></li>
<li><p><strong>Aggregate</strong>: The messages of the neighbours of each node are aggregated with
respect to the <code class="docutils literal notranslate"><span class="pre">edge_weights</span></code> using a <em>permutation invariant</em> pooling operation, such as <em>sum</em>, <em>mean</em>, and <em>max</em>,
to prepare a single aggregated message for each node. See, for example, <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/math/unsorted_segment_sum">tf.math.unsorted_segment_sum</a>
APIs used to aggregate neighbour messages.</p></li>
<li><p><strong>Update</strong>: The <code class="docutils literal notranslate"><span class="pre">node_repesentations</span></code> and <code class="docutils literal notranslate"><span class="pre">aggregated_messages</span></code>—both of shape <code class="docutils literal notranslate"><span class="pre">[num_nodes,</span> <span class="pre">representation_dim]</span></code>—
are combined and processed to produce the new state of the node representations (node embeddings).
If <code class="docutils literal notranslate"><span class="pre">combination_type</span></code> is <code class="docutils literal notranslate"><span class="pre">gru</span></code>, the <code class="docutils literal notranslate"><span class="pre">node_repesentations</span></code> and <code class="docutils literal notranslate"><span class="pre">aggregated_messages</span></code> are stacked to create a sequence,
then processed by a GRU layer. Otherwise, the <code class="docutils literal notranslate"><span class="pre">node_repesentations</span></code> and <code class="docutils literal notranslate"><span class="pre">aggregated_messages</span></code> are added
or concatenated, then processed using a FFN.</p></li>
</ol>
<p>The technique implemented use ideas from <a class="reference external" href="https://arxiv.org/abs/1609.02907">Graph Convolutional Networks</a>,
<a class="reference external" href="https://arxiv.org/abs/1706.02216">GraphSage</a>, <a class="reference external" href="https://arxiv.org/abs/1810.00826">Graph Isomorphism Network</a>,
<a class="reference external" href="https://arxiv.org/abs/1902.07153">Simple Graph Networks</a>, and
<a class="reference external" href="https://arxiv.org/abs/1511.05493">Gated Graph Sequence Neural Networks</a>.
Two other key techniques that are not covered are <a class="reference external" href="https://arxiv.org/abs/1710.10903">Graph Attention Networks</a>
and <a class="reference external" href="https://arxiv.org/abs/1704.01212">Message Passing Neural Networks</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">GraphConvLayer</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">hidden_units</span><span class="p">,</span>
        <span class="n">dropout_rate</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
        <span class="n">aggregation_type</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span>
        <span class="n">combination_type</span><span class="o">=</span><span class="s2">&quot;concat&quot;</span><span class="p">,</span>
        <span class="n">normalize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GraphConvLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">aggregation_type</span> <span class="o">=</span> <span class="n">aggregation_type</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">combination_type</span> <span class="o">=</span> <span class="n">combination_type</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">normalize</span> <span class="o">=</span> <span class="n">normalize</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">ffn_prepare</span> <span class="o">=</span> <span class="n">create_ffn</span><span class="p">(</span><span class="n">hidden_units</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">combination_type</span> <span class="o">==</span> <span class="s2">&quot;gated&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">update_fn</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">GRU</span><span class="p">(</span>
                <span class="n">units</span><span class="o">=</span><span class="n">hidden_units</span><span class="p">,</span>
                <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;tanh&quot;</span><span class="p">,</span>
                <span class="n">recurrent_activation</span><span class="o">=</span><span class="s2">&quot;sigmoid&quot;</span><span class="p">,</span>
                <span class="n">dropout</span><span class="o">=</span><span class="n">dropout_rate</span><span class="p">,</span>
                <span class="n">return_state</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">recurrent_dropout</span><span class="o">=</span><span class="n">dropout_rate</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">update_fn</span> <span class="o">=</span> <span class="n">create_ffn</span><span class="p">(</span><span class="n">hidden_units</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">prepare</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node_repesentations</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="c1"># node_repesentations shape is [num_edges, embedding_dim].</span>
        <span class="n">messages</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn_prepare</span><span class="p">(</span><span class="n">node_repesentations</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">messages</span> <span class="o">=</span> <span class="n">messages</span> <span class="o">*</span> <span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">messages</span>

    <span class="k">def</span> <span class="nf">aggregate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node_indices</span><span class="p">,</span> <span class="n">neighbour_messages</span><span class="p">):</span>
        <span class="c1"># node_indices shape is [num_edges].</span>
        <span class="c1"># neighbour_messages shape: [num_edges, representation_dim].</span>
        <span class="n">num_nodes</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">reduce_max</span><span class="p">(</span><span class="n">node_indices</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">aggregation_type</span> <span class="o">==</span> <span class="s2">&quot;sum&quot;</span><span class="p">:</span>
            <span class="n">aggregated_message</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">unsorted_segment_sum</span><span class="p">(</span>
                <span class="n">neighbour_messages</span><span class="p">,</span> <span class="n">node_indices</span><span class="p">,</span> <span class="n">num_segments</span><span class="o">=</span><span class="n">num_nodes</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">aggregation_type</span> <span class="o">==</span> <span class="s2">&quot;mean&quot;</span><span class="p">:</span>
            <span class="n">aggregated_message</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">unsorted_segment_mean</span><span class="p">(</span>
                <span class="n">neighbour_messages</span><span class="p">,</span> <span class="n">node_indices</span><span class="p">,</span> <span class="n">num_segments</span><span class="o">=</span><span class="n">num_nodes</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">aggregation_type</span> <span class="o">==</span> <span class="s2">&quot;max&quot;</span><span class="p">:</span>
            <span class="n">aggregated_message</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">unsorted_segment_max</span><span class="p">(</span>
                <span class="n">neighbour_messages</span><span class="p">,</span> <span class="n">node_indices</span><span class="p">,</span> <span class="n">num_segments</span><span class="o">=</span><span class="n">num_nodes</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Invalid aggregation type: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">aggregation_type</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">aggregated_message</span>

    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">node_repesentations</span><span class="p">,</span> <span class="n">aggregated_messages</span><span class="p">):</span>
        <span class="c1"># node_repesentations shape is [num_nodes, representation_dim].</span>
        <span class="c1"># aggregated_messages shape is [num_nodes, representation_dim].</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">combination_type</span> <span class="o">==</span> <span class="s2">&quot;gru&quot;</span><span class="p">:</span>
            <span class="c1"># Create a sequence of two elements for the GRU layer.</span>
            <span class="n">h</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">node_repesentations</span><span class="p">,</span> <span class="n">aggregated_messages</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">combination_type</span> <span class="o">==</span> <span class="s2">&quot;concat&quot;</span><span class="p">:</span>
            <span class="c1"># Concatenate the node_repesentations and aggregated_messages.</span>
            <span class="n">h</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">node_repesentations</span><span class="p">,</span> <span class="n">aggregated_messages</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">combination_type</span> <span class="o">==</span> <span class="s2">&quot;add&quot;</span><span class="p">:</span>
            <span class="c1"># Add node_repesentations and aggregated_messages.</span>
            <span class="n">h</span> <span class="o">=</span> <span class="n">node_repesentations</span> <span class="o">+</span> <span class="n">aggregated_messages</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Invalid combination type: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">combination_type</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

        <span class="c1"># Apply the processing function.</span>
        <span class="n">node_embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">update_fn</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">combination_type</span> <span class="o">==</span> <span class="s2">&quot;gru&quot;</span><span class="p">:</span>
            <span class="n">node_embeddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">unstack</span><span class="p">(</span><span class="n">node_embeddings</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalize</span><span class="p">:</span>
            <span class="n">node_embeddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">l2_normalize</span><span class="p">(</span><span class="n">node_embeddings</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">node_embeddings</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Process the inputs to produce the node_embeddings.</span>

<span class="sd">        inputs: a tuple of three elements: node_repesentations, edges, edge_weights.</span>
<span class="sd">        Returns: node_embeddings of shape [num_nodes, representation_dim].</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">node_repesentations</span><span class="p">,</span> <span class="n">edges</span><span class="p">,</span> <span class="n">edge_weights</span> <span class="o">=</span> <span class="n">inputs</span>
        <span class="c1"># Get node_indices (source) and neighbour_indices (target) from edges.</span>
        <span class="n">node_indices</span><span class="p">,</span> <span class="n">neighbour_indices</span> <span class="o">=</span> <span class="n">edges</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">edges</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="c1"># neighbour_repesentations shape is [num_edges, representation_dim].</span>
        <span class="n">neighbour_repesentations</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">node_repesentations</span><span class="p">,</span> <span class="n">neighbour_indices</span><span class="p">)</span>

        <span class="c1"># Prepare the messages of the neighbours.</span>
        <span class="n">neighbour_messages</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prepare</span><span class="p">(</span><span class="n">neighbour_repesentations</span><span class="p">,</span> <span class="n">edge_weights</span><span class="p">)</span>
        <span class="c1"># Aggregate the neighbour messages.</span>
        <span class="n">aggregated_messages</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">aggregate</span><span class="p">(</span><span class="n">node_indices</span><span class="p">,</span> <span class="n">neighbour_messages</span><span class="p">)</span>
        <span class="c1"># Update the node embedding with the neighbour messages.</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">node_repesentations</span><span class="p">,</span> <span class="n">aggregated_messages</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="implement-a-graph-neural-network-node-classifier">
<h3>Implement a graph neural network node classifier<a class="headerlink" href="#implement-a-graph-neural-network-node-classifier" title="Permalink to this headline">¶</a></h3>
<p>The GNN classification model follows the <a class="reference external" href="https://arxiv.org/abs/2011.08843">Design Space for Graph Neural Networks</a> approach,
as follows:</p>
<ol class="simple">
<li><p>Apply preprocessing using FFN to the node features to generate initial node representations.</p></li>
<li><p>Apply one or more graph convolutional layer, with skip connections,  to the node representation
to produce node embeddings.</p></li>
<li><p>Apply post-processing using FFN to the node embeddings to generate the final node embeddings.</p></li>
<li><p>Feed the node embeddings in a Softmax layer to predict the node class.</p></li>
</ol>
<figure align="center">
<img src="https://maucher.home.hdm-stuttgart.de/Pics/GNNDesign.png" width="600px" align="center">
<figcaption>
Image Source: <a href="https://arxiv.org/abs/2011.08843">Design Space for Graph Neural Networks</a> 
</figcaption>
</figure>
<p>Each graph convolutional layer added captures information from a further level of neighbours.
However, adding many graph convolutional layer can cause oversmoothing, where the model
produces similar embeddings for all the nodes.</p>
<p>Note that the <code class="docutils literal notranslate"><span class="pre">graph_info</span></code> passed to the constructor of the Keras model, and used as a <em>property</em>
of the Keras model object, rather than input data for training or prediction.
The model will accept a <strong>batch</strong> of <code class="docutils literal notranslate"><span class="pre">node_indices</span></code>, which are used to lookup the
node features and neighbours from the <code class="docutils literal notranslate"><span class="pre">graph_info</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">GNNNodeClassifier</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">graph_info</span><span class="p">,</span>
        <span class="n">num_classes</span><span class="p">,</span>
        <span class="n">hidden_units</span><span class="p">,</span>
        <span class="n">aggregation_type</span><span class="o">=</span><span class="s2">&quot;sum&quot;</span><span class="p">,</span>
        <span class="n">combination_type</span><span class="o">=</span><span class="s2">&quot;concat&quot;</span><span class="p">,</span>
        <span class="n">dropout_rate</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
        <span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GNNNodeClassifier</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># Unpack graph_info to three elements: node_features, edges, and edge_weight.</span>
        <span class="n">node_features</span><span class="p">,</span> <span class="n">edges</span><span class="p">,</span> <span class="n">edge_weights</span> <span class="o">=</span> <span class="n">graph_info</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">node_features</span> <span class="o">=</span> <span class="n">node_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">edges</span> <span class="o">=</span> <span class="n">edges</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">edge_weights</span> <span class="o">=</span> <span class="n">edge_weights</span>
        <span class="c1"># Set edge_weights to ones if not provided.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">edge_weights</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">edge_weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">edges</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="c1"># Scale edge_weights to sum to 1.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">edge_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">edge_weights</span> <span class="o">/</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">edge_weights</span><span class="p">)</span>

        <span class="c1"># Create a process layer.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">preprocess</span> <span class="o">=</span> <span class="n">create_ffn</span><span class="p">(</span><span class="n">hidden_units</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;preprocess&quot;</span><span class="p">)</span>
        <span class="c1"># Create the first GraphConv layer.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">GraphConvLayer</span><span class="p">(</span>
            <span class="n">hidden_units</span><span class="p">,</span>
            <span class="n">dropout_rate</span><span class="p">,</span>
            <span class="n">aggregation_type</span><span class="p">,</span>
            <span class="n">combination_type</span><span class="p">,</span>
            <span class="n">normalize</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">&quot;graph_conv1&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># Create the second GraphConv layer.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">GraphConvLayer</span><span class="p">(</span>
            <span class="n">hidden_units</span><span class="p">,</span>
            <span class="n">dropout_rate</span><span class="p">,</span>
            <span class="n">aggregation_type</span><span class="p">,</span>
            <span class="n">combination_type</span><span class="p">,</span>
            <span class="n">normalize</span><span class="p">,</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">&quot;graph_conv2&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># Create a postprocess layer.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">postprocess</span> <span class="o">=</span> <span class="n">create_ffn</span><span class="p">(</span><span class="n">hidden_units</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;postprocess&quot;</span><span class="p">)</span>
        <span class="c1"># Create a compute logits layer.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">compute_logits</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;logits&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_node_indices</span><span class="p">):</span>
        <span class="c1"># Preprocess the node_features to produce node representations.</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">preprocess</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">node_features</span><span class="p">)</span>
        <span class="c1"># Apply the first graph conv layer.</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">edges</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">edge_weights</span><span class="p">))</span>
        <span class="c1"># Skip connection.</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x1</span> <span class="o">+</span> <span class="n">x</span>
        <span class="c1"># Apply the second graph conv layer.</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">edges</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">edge_weights</span><span class="p">))</span>
        <span class="c1"># Skip connection.</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x2</span> <span class="o">+</span> <span class="n">x</span>
        <span class="c1"># Postprocess node embedding.</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">postprocess</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># Fetch node embeddings for the input node_indices.</span>
        <span class="n">node_embeddings</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">input_node_indices</span><span class="p">))</span>
        <span class="c1"># Compute logits</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_logits</span><span class="p">(</span><span class="n">node_embeddings</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s test instantiating and calling the GNN model.
Notice that if you provide <code class="docutils literal notranslate"><span class="pre">N</span></code> node indices, the output will be a tensor of shape <code class="docutils literal notranslate"><span class="pre">[N,</span> <span class="pre">num_classes]</span></code>,
regardless of the size of the graph.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">create_ffn</span><span class="p">(</span><span class="n">hidden_units</span><span class="p">,</span> <span class="n">dropout_rate</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">fnn_layers</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">units</span> <span class="ow">in</span> <span class="n">hidden_units</span><span class="p">:</span>
        <span class="n">fnn_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">BatchNormalization</span><span class="p">())</span>
        <span class="n">fnn_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">))</span>
        <span class="n">fnn_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">gelu</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">fnn_layers</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gnn_model</span> <span class="o">=</span> <span class="n">GNNNodeClassifier</span><span class="p">(</span>
    <span class="n">graph_info</span><span class="o">=</span><span class="n">graph_info</span><span class="p">,</span>
    <span class="n">num_classes</span><span class="o">=</span><span class="n">num_classes</span><span class="p">,</span>
    <span class="n">hidden_units</span><span class="o">=</span><span class="n">hidden_units</span><span class="p">,</span>
    <span class="n">dropout_rate</span><span class="o">=</span><span class="n">dropout_rate</span><span class="p">,</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;gnn_model&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;GNN output shape:&quot;</span><span class="p">,</span> <span class="n">gnn_model</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">]))</span>

<span class="n">gnn_model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>GNN output shape: tf.Tensor(
[[-0.01866406 -0.01698142  0.06285025  0.01854473 -0.04537843  0.09679846
   0.12072642]
 [ 0.09394638  0.14712316  0.07002003  0.00241234 -0.03190225 -0.05208772
   0.05198889]
 [ 0.05843008 -0.05706381  0.00147219  0.00412669 -0.02097375  0.01396486
  -0.1652648 ]], shape=(3, 7), dtype=float32)
Model: &quot;gnn_model&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
preprocess (Sequential)      (2708, 32)                52804     
_________________________________________________________________
graph_conv1 (GraphConvLayer) multiple                  5888      
_________________________________________________________________
graph_conv2 (GraphConvLayer) multiple                  5888      
_________________________________________________________________
postprocess (Sequential)     (2708, 32)                2368      
_________________________________________________________________
logits (Dense)               multiple                  231       
=================================================================
Total params: 67,179
Trainable params: 63,481
Non-trainable params: 3,698
_________________________________________________________________
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="train-the-gnn-model">
<h3>Train the GNN model<a class="headerlink" href="#train-the-gnn-model" title="Permalink to this headline">¶</a></h3>
<p>Note that we use the standard <em>supervised</em> cross-entropy loss to train the model.
However, we can add another <em>self-supervised</em> loss term for the generated node embeddings
that makes sure that neighbouring nodes in graph have similar representations, while faraway
nodes have dissimilar representations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_train</span> <span class="o">=</span> <span class="n">train_data</span><span class="o">.</span><span class="n">paper_id</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="c1">#print(x_train.shape)</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">run_experiment</span><span class="p">(</span><span class="n">gnn_model</span><span class="p">,</span> <span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/300
4/4 [==============================] - 4s 196ms/step - loss: 2.2721 - acc: 0.2064 - val_loss: 1.9129 - val_acc: 0.2882
Epoch 2/300
4/4 [==============================] - 0s 107ms/step - loss: 2.0104 - acc: 0.2653 - val_loss: 1.9038 - val_acc: 0.2882
Epoch 3/300
4/4 [==============================] - 0s 112ms/step - loss: 1.9227 - acc: 0.2857 - val_loss: 1.8985 - val_acc: 0.2882
Epoch 4/300
4/4 [==============================] - 0s 100ms/step - loss: 1.8775 - acc: 0.2990 - val_loss: 1.8973 - val_acc: 0.2882
Epoch 5/300
4/4 [==============================] - 0s 102ms/step - loss: 1.8715 - acc: 0.2831 - val_loss: 1.8910 - val_acc: 0.2882
Epoch 6/300
4/4 [==============================] - 0s 101ms/step - loss: 1.8291 - acc: 0.2977 - val_loss: 1.8818 - val_acc: 0.2882
Epoch 7/300
4/4 [==============================] - 0s 107ms/step - loss: 1.8419 - acc: 0.2994 - val_loss: 1.8761 - val_acc: 0.2882
Epoch 8/300
4/4 [==============================] - 0s 106ms/step - loss: 1.8176 - acc: 0.3090 - val_loss: 1.8662 - val_acc: 0.2882
Epoch 9/300
4/4 [==============================] - 0s 106ms/step - loss: 1.8287 - acc: 0.3296 - val_loss: 1.8560 - val_acc: 0.2882
Epoch 10/300
4/4 [==============================] - 0s 113ms/step - loss: 1.8362 - acc: 0.2862 - val_loss: 1.8482 - val_acc: 0.2882
Epoch 11/300
4/4 [==============================] - 0s 112ms/step - loss: 1.7853 - acc: 0.3191 - val_loss: 1.8396 - val_acc: 0.2882
Epoch 12/300
4/4 [==============================] - 0s 108ms/step - loss: 1.7651 - acc: 0.3173 - val_loss: 1.8271 - val_acc: 0.2882
Epoch 13/300
4/4 [==============================] - 0s 112ms/step - loss: 1.7336 - acc: 0.3354 - val_loss: 1.8128 - val_acc: 0.2882
Epoch 14/300
4/4 [==============================] - 0s 108ms/step - loss: 1.7692 - acc: 0.3302 - val_loss: 1.7974 - val_acc: 0.2882
Epoch 15/300
4/4 [==============================] - 0s 99ms/step - loss: 1.7097 - acc: 0.3435 - val_loss: 1.7723 - val_acc: 0.2882
Epoch 16/300
4/4 [==============================] - 0s 114ms/step - loss: 1.6852 - acc: 0.3588 - val_loss: 1.7404 - val_acc: 0.2882
Epoch 17/300
4/4 [==============================] - 0s 106ms/step - loss: 1.6563 - acc: 0.3746 - val_loss: 1.7016 - val_acc: 0.2980
Epoch 18/300
4/4 [==============================] - 0s 92ms/step - loss: 1.6296 - acc: 0.3788 - val_loss: 1.6793 - val_acc: 0.3103
Epoch 19/300
4/4 [==============================] - 0s 91ms/step - loss: 1.6060 - acc: 0.4033 - val_loss: 1.6552 - val_acc: 0.3276
Epoch 20/300
4/4 [==============================] - 0s 98ms/step - loss: 1.5499 - acc: 0.4250 - val_loss: 1.6531 - val_acc: 0.3473
Epoch 21/300
4/4 [==============================] - 0s 93ms/step - loss: 1.5395 - acc: 0.4067 - val_loss: 1.5974 - val_acc: 0.3966
Epoch 22/300
4/4 [==============================] - 0s 91ms/step - loss: 1.4930 - acc: 0.4267 - val_loss: 1.5369 - val_acc: 0.4360
Epoch 23/300
4/4 [==============================] - 0s 93ms/step - loss: 1.4175 - acc: 0.4903 - val_loss: 1.4887 - val_acc: 0.4729
Epoch 24/300
4/4 [==============================] - 0s 95ms/step - loss: 1.4167 - acc: 0.4922 - val_loss: 1.4481 - val_acc: 0.4877
Epoch 25/300
4/4 [==============================] - 0s 112ms/step - loss: 1.4479 - acc: 0.4694 - val_loss: 1.4068 - val_acc: 0.4852
Epoch 26/300
4/4 [==============================] - 0s 112ms/step - loss: 1.3632 - acc: 0.5053 - val_loss: 1.3653 - val_acc: 0.4951
Epoch 27/300
4/4 [==============================] - 0s 125ms/step - loss: 1.2411 - acc: 0.5587 - val_loss: 1.3387 - val_acc: 0.5148
Epoch 28/300
4/4 [==============================] - 0s 122ms/step - loss: 1.3133 - acc: 0.5359 - val_loss: 1.3762 - val_acc: 0.5197
Epoch 29/300
4/4 [==============================] - 1s 135ms/step - loss: 1.2565 - acc: 0.5621 - val_loss: 1.4072 - val_acc: 0.5271
Epoch 30/300
4/4 [==============================] - 0s 113ms/step - loss: 1.1699 - acc: 0.5990 - val_loss: 1.3965 - val_acc: 0.5246
Epoch 31/300
4/4 [==============================] - 0s 98ms/step - loss: 1.1942 - acc: 0.5936 - val_loss: 1.3294 - val_acc: 0.5369
Epoch 32/300
4/4 [==============================] - 0s 98ms/step - loss: 1.1386 - acc: 0.5973 - val_loss: 1.2993 - val_acc: 0.5517
Epoch 33/300
4/4 [==============================] - 0s 118ms/step - loss: 1.0117 - acc: 0.6584 - val_loss: 1.2568 - val_acc: 0.5714
Epoch 34/300
4/4 [==============================] - 0s 124ms/step - loss: 1.0402 - acc: 0.6334 - val_loss: 1.2774 - val_acc: 0.5591
Epoch 35/300
4/4 [==============================] - 0s 124ms/step - loss: 1.0473 - acc: 0.6379 - val_loss: 1.3202 - val_acc: 0.5197
Epoch 36/300
4/4 [==============================] - 0s 103ms/step - loss: 1.0109 - acc: 0.6490 - val_loss: 1.2755 - val_acc: 0.5887
Epoch 37/300
4/4 [==============================] - 0s 102ms/step - loss: 1.0404 - acc: 0.6481 - val_loss: 1.3698 - val_acc: 0.5665
Epoch 38/300
4/4 [==============================] - 0s 100ms/step - loss: 0.9442 - acc: 0.6587 - val_loss: 1.3547 - val_acc: 0.5714
Epoch 39/300
4/4 [==============================] - 0s 97ms/step - loss: 0.9683 - acc: 0.6721 - val_loss: 1.2881 - val_acc: 0.5739
Epoch 40/300
4/4 [==============================] - 0s 99ms/step - loss: 0.8973 - acc: 0.6886 - val_loss: 1.2801 - val_acc: 0.5862
Epoch 41/300
4/4 [==============================] - 0s 99ms/step - loss: 0.8215 - acc: 0.7155 - val_loss: 1.3215 - val_acc: 0.5813
Epoch 42/300
4/4 [==============================] - 0s 113ms/step - loss: 0.8651 - acc: 0.6838 - val_loss: 1.3776 - val_acc: 0.5714
Epoch 43/300
4/4 [==============================] - 0s 112ms/step - loss: 0.8721 - acc: 0.6939 - val_loss: 1.4371 - val_acc: 0.5567
Epoch 44/300
4/4 [==============================] - 0s 122ms/step - loss: 0.8562 - acc: 0.6999 - val_loss: 1.4131 - val_acc: 0.5517
Epoch 45/300
4/4 [==============================] - 0s 126ms/step - loss: 0.8894 - acc: 0.6941 - val_loss: 1.3953 - val_acc: 0.5567
Epoch 46/300
4/4 [==============================] - 0s 103ms/step - loss: 0.8415 - acc: 0.7041 - val_loss: 1.3057 - val_acc: 0.5714
Epoch 47/300
4/4 [==============================] - 0s 132ms/step - loss: 0.7938 - acc: 0.7228 - val_loss: 1.2698 - val_acc: 0.5764
Epoch 48/300
4/4 [==============================] - 0s 108ms/step - loss: 0.7735 - acc: 0.7336 - val_loss: 1.2341 - val_acc: 0.5837
Epoch 49/300
4/4 [==============================] - 0s 123ms/step - loss: 0.6959 - acc: 0.7504 - val_loss: 1.1602 - val_acc: 0.6084
Epoch 50/300
4/4 [==============================] - 0s 105ms/step - loss: 0.7648 - acc: 0.7204 - val_loss: 1.1096 - val_acc: 0.6379
Epoch 51/300
4/4 [==============================] - 0s 108ms/step - loss: 0.7456 - acc: 0.7400 - val_loss: 1.2204 - val_acc: 0.6305
Epoch 52/300
4/4 [==============================] - 0s 105ms/step - loss: 0.7037 - acc: 0.7647 - val_loss: 1.3266 - val_acc: 0.6010
Epoch 53/300
4/4 [==============================] - 0s 112ms/step - loss: 0.7502 - acc: 0.7391 - val_loss: 1.3158 - val_acc: 0.6133
Epoch 54/300
4/4 [==============================] - 0s 99ms/step - loss: 0.7393 - acc: 0.7235 - val_loss: 1.2730 - val_acc: 0.6182
Epoch 55/300
4/4 [==============================] - 0s 111ms/step - loss: 0.6958 - acc: 0.7448 - val_loss: 1.2529 - val_acc: 0.6207
Epoch 56/300
4/4 [==============================] - 0s 100ms/step - loss: 0.7171 - acc: 0.7487 - val_loss: 1.1758 - val_acc: 0.6256
Epoch 57/300
4/4 [==============================] - 0s 106ms/step - loss: 0.6142 - acc: 0.7898 - val_loss: 1.1651 - val_acc: 0.6404
Epoch 58/300
4/4 [==============================] - 0s 91ms/step - loss: 0.6834 - acc: 0.7572 - val_loss: 1.1464 - val_acc: 0.6453
Epoch 59/300
4/4 [==============================] - 0s 104ms/step - loss: 0.6917 - acc: 0.7648 - val_loss: 1.0697 - val_acc: 0.6576
Epoch 60/300
4/4 [==============================] - 0s 101ms/step - loss: 0.6850 - acc: 0.7650 - val_loss: 0.9847 - val_acc: 0.6847
Epoch 61/300
4/4 [==============================] - 0s 85ms/step - loss: 0.6822 - acc: 0.7819 - val_loss: 0.9573 - val_acc: 0.6921
Epoch 62/300
4/4 [==============================] - 0s 100ms/step - loss: 0.6769 - acc: 0.7618 - val_loss: 0.9612 - val_acc: 0.6946
Epoch 63/300
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4/4 [==============================] - 0s 100ms/step - loss: 0.6885 - acc: 0.7712 - val_loss: 0.9542 - val_acc: 0.6921
Epoch 64/300
4/4 [==============================] - 0s 96ms/step - loss: 0.6070 - acc: 0.7767 - val_loss: 0.9474 - val_acc: 0.6872
Epoch 65/300
4/4 [==============================] - 0s 103ms/step - loss: 0.6633 - acc: 0.7832 - val_loss: 0.9726 - val_acc: 0.6675
Epoch 66/300
4/4 [==============================] - 0s 99ms/step - loss: 0.6252 - acc: 0.7711 - val_loss: 1.0091 - val_acc: 0.6576
Epoch 67/300
4/4 [==============================] - 0s 88ms/step - loss: 0.6230 - acc: 0.7765 - val_loss: 0.9711 - val_acc: 0.6650
Epoch 68/300
4/4 [==============================] - 0s 106ms/step - loss: 0.6515 - acc: 0.7908 - val_loss: 0.9950 - val_acc: 0.6626
Epoch 69/300
4/4 [==============================] - 0s 111ms/step - loss: 0.6439 - acc: 0.7858 - val_loss: 1.0026 - val_acc: 0.6700
Epoch 70/300
4/4 [==============================] - 0s 106ms/step - loss: 0.5518 - acc: 0.8169 - val_loss: 0.9906 - val_acc: 0.6847
Epoch 71/300
4/4 [==============================] - 0s 112ms/step - loss: 0.6312 - acc: 0.7790 - val_loss: 0.8951 - val_acc: 0.7118
Epoch 72/300
4/4 [==============================] - 0s 111ms/step - loss: 0.5608 - acc: 0.7946 - val_loss: 0.8396 - val_acc: 0.7266
Epoch 73/300
4/4 [==============================] - 0s 107ms/step - loss: 0.5471 - acc: 0.8074 - val_loss: 0.7969 - val_acc: 0.7167
Epoch 74/300
4/4 [==============================] - 0s 102ms/step - loss: 0.6039 - acc: 0.7821 - val_loss: 0.7561 - val_acc: 0.7340
Epoch 75/300
4/4 [==============================] - 0s 108ms/step - loss: 0.5597 - acc: 0.8091 - val_loss: 0.7097 - val_acc: 0.7660
Epoch 76/300
4/4 [==============================] - 0s 106ms/step - loss: 0.5049 - acc: 0.8203 - val_loss: 0.6959 - val_acc: 0.7685
Epoch 77/300
4/4 [==============================] - 0s 108ms/step - loss: 0.5436 - acc: 0.8158 - val_loss: 0.7152 - val_acc: 0.7562
Epoch 78/300
4/4 [==============================] - 0s 103ms/step - loss: 0.5609 - acc: 0.8131 - val_loss: 0.7232 - val_acc: 0.7611
Epoch 79/300
4/4 [==============================] - 0s 106ms/step - loss: 0.5746 - acc: 0.7988 - val_loss: 0.7163 - val_acc: 0.7611
Epoch 80/300
4/4 [==============================] - 0s 105ms/step - loss: 0.5840 - acc: 0.8014 - val_loss: 0.7520 - val_acc: 0.7389
Epoch 81/300
4/4 [==============================] - 0s 101ms/step - loss: 0.5509 - acc: 0.8276 - val_loss: 0.7568 - val_acc: 0.7315
Epoch 82/300
4/4 [==============================] - 0s 100ms/step - loss: 0.4974 - acc: 0.8230 - val_loss: 0.7520 - val_acc: 0.7488
Epoch 83/300
4/4 [==============================] - 0s 107ms/step - loss: 0.5522 - acc: 0.8280 - val_loss: 0.7377 - val_acc: 0.7537
Epoch 84/300
4/4 [==============================] - 0s 89ms/step - loss: 0.4459 - acc: 0.8599 - val_loss: 0.7032 - val_acc: 0.7685
Epoch 85/300
4/4 [==============================] - 0s 106ms/step - loss: 0.4743 - acc: 0.8458 - val_loss: 0.7030 - val_acc: 0.7734
Epoch 86/300
4/4 [==============================] - 0s 121ms/step - loss: 0.4646 - acc: 0.8571 - val_loss: 0.7262 - val_acc: 0.7685
Epoch 87/300
4/4 [==============================] - 0s 119ms/step - loss: 0.5163 - acc: 0.8389 - val_loss: 0.7692 - val_acc: 0.7660
Epoch 88/300
4/4 [==============================] - 1s 131ms/step - loss: 0.4445 - acc: 0.8516 - val_loss: 0.7475 - val_acc: 0.7759
Epoch 89/300
4/4 [==============================] - 1s 140ms/step - loss: 0.4383 - acc: 0.8623 - val_loss: 0.7659 - val_acc: 0.7857
Epoch 90/300
4/4 [==============================] - 0s 129ms/step - loss: 0.4954 - acc: 0.8420 - val_loss: 0.7750 - val_acc: 0.7660
Epoch 91/300
4/4 [==============================] - 1s 140ms/step - loss: 0.4984 - acc: 0.8380 - val_loss: 0.7423 - val_acc: 0.7709
Epoch 92/300
4/4 [==============================] - 0s 124ms/step - loss: 0.5017 - acc: 0.8262 - val_loss: 0.7130 - val_acc: 0.7783
Epoch 93/300
4/4 [==============================] - 1s 145ms/step - loss: 0.4737 - acc: 0.8353 - val_loss: 0.6305 - val_acc: 0.8079
Epoch 94/300
4/4 [==============================] - 0s 129ms/step - loss: 0.4240 - acc: 0.8648 - val_loss: 0.6341 - val_acc: 0.8054
Epoch 95/300
4/4 [==============================] - 1s 139ms/step - loss: 0.4865 - acc: 0.8305 - val_loss: 0.6538 - val_acc: 0.8005
Epoch 96/300
4/4 [==============================] - 1s 131ms/step - loss: 0.4956 - acc: 0.8399 - val_loss: 0.6802 - val_acc: 0.7833
Epoch 97/300
4/4 [==============================] - 1s 147ms/step - loss: 0.4664 - acc: 0.8472 - val_loss: 0.6969 - val_acc: 0.7734
Epoch 98/300
4/4 [==============================] - 0s 127ms/step - loss: 0.4493 - acc: 0.8635 - val_loss: 0.7043 - val_acc: 0.7759
Epoch 99/300
4/4 [==============================] - 1s 130ms/step - loss: 0.4125 - acc: 0.8663 - val_loss: 0.6582 - val_acc: 0.8030
Epoch 100/300
4/4 [==============================] - 1s 137ms/step - loss: 0.4522 - acc: 0.8488 - val_loss: 0.6460 - val_acc: 0.8030
Epoch 101/300
4/4 [==============================] - 1s 152ms/step - loss: 0.4313 - acc: 0.8569 - val_loss: 0.6737 - val_acc: 0.7980
Epoch 102/300
4/4 [==============================] - 1s 135ms/step - loss: 0.4512 - acc: 0.8486 - val_loss: 0.6880 - val_acc: 0.7956
Epoch 103/300
4/4 [==============================] - 1s 161ms/step - loss: 0.4443 - acc: 0.8609 - val_loss: 0.6936 - val_acc: 0.8054
Epoch 104/300
4/4 [==============================] - 0s 126ms/step - loss: 0.4066 - acc: 0.8675 - val_loss: 0.6854 - val_acc: 0.8128
Epoch 105/300
4/4 [==============================] - 0s 125ms/step - loss: 0.4330 - acc: 0.8629 - val_loss: 0.6619 - val_acc: 0.8079
Epoch 106/300
4/4 [==============================] - 1s 142ms/step - loss: 0.4210 - acc: 0.8640 - val_loss: 0.6647 - val_acc: 0.8103
Epoch 107/300
4/4 [==============================] - 1s 136ms/step - loss: 0.4252 - acc: 0.8621 - val_loss: 0.6766 - val_acc: 0.8079
Epoch 108/300
4/4 [==============================] - 0s 122ms/step - loss: 0.4312 - acc: 0.8619 - val_loss: 0.7146 - val_acc: 0.7833
Epoch 109/300
4/4 [==============================] - 0s 120ms/step - loss: 0.3991 - acc: 0.8711 - val_loss: 0.7487 - val_acc: 0.7685
Epoch 110/300
4/4 [==============================] - 0s 119ms/step - loss: 0.4375 - acc: 0.8701 - val_loss: 0.7652 - val_acc: 0.7685
Epoch 111/300
4/4 [==============================] - 1s 135ms/step - loss: 0.4142 - acc: 0.8689 - val_loss: 0.6677 - val_acc: 0.7980
Epoch 112/300
4/4 [==============================] - 0s 130ms/step - loss: 0.3849 - acc: 0.8772 - val_loss: 0.6393 - val_acc: 0.8128
Epoch 113/300
4/4 [==============================] - 1s 147ms/step - loss: 0.4098 - acc: 0.8728 - val_loss: 0.6441 - val_acc: 0.8128
Epoch 114/300
4/4 [==============================] - 1s 134ms/step - loss: 0.4132 - acc: 0.8697 - val_loss: 0.6494 - val_acc: 0.8054
Epoch 115/300
4/4 [==============================] - 1s 130ms/step - loss: 0.3855 - acc: 0.8758 - val_loss: 0.6485 - val_acc: 0.8079
Epoch 116/300
4/4 [==============================] - 1s 160ms/step - loss: 0.4175 - acc: 0.8808 - val_loss: 0.6715 - val_acc: 0.8079
Epoch 117/300
4/4 [==============================] - 0s 120ms/step - loss: 0.4066 - acc: 0.8618 - val_loss: 0.6754 - val_acc: 0.8202
Epoch 118/300
4/4 [==============================] - 1s 138ms/step - loss: 0.3963 - acc: 0.8618 - val_loss: 0.6693 - val_acc: 0.8202
Epoch 119/300
4/4 [==============================] - 1s 160ms/step - loss: 0.3666 - acc: 0.8950 - val_loss: 0.6577 - val_acc: 0.8153
Epoch 120/300
4/4 [==============================] - 1s 144ms/step - loss: 0.4589 - acc: 0.8665 - val_loss: 0.6689 - val_acc: 0.8079
Epoch 121/300
4/4 [==============================] - 1s 137ms/step - loss: 0.3983 - acc: 0.8639 - val_loss: 0.6968 - val_acc: 0.8005
Epoch 122/300
4/4 [==============================] - 0s 129ms/step - loss: 0.3998 - acc: 0.8809 - val_loss: 0.6911 - val_acc: 0.7980
Epoch 123/300
4/4 [==============================] - 1s 140ms/step - loss: 0.3780 - acc: 0.8786 - val_loss: 0.6808 - val_acc: 0.8202
Epoch 124/300
4/4 [==============================] - 1s 175ms/step - loss: 0.3706 - acc: 0.8863 - val_loss: 0.6545 - val_acc: 0.8227
Epoch 125/300
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4/4 [==============================] - 1s 150ms/step - loss: 0.3557 - acc: 0.8885 - val_loss: 0.6439 - val_acc: 0.8103
Epoch 126/300
4/4 [==============================] - 1s 163ms/step - loss: 0.3409 - acc: 0.8881 - val_loss: 0.6406 - val_acc: 0.8177
Epoch 127/300
4/4 [==============================] - 1s 181ms/step - loss: 0.3653 - acc: 0.8820 - val_loss: 0.6640 - val_acc: 0.8202
Epoch 128/300
4/4 [==============================] - 1s 178ms/step - loss: 0.4244 - acc: 0.8757 - val_loss: 0.6929 - val_acc: 0.8177
Epoch 129/300
4/4 [==============================] - 1s 148ms/step - loss: 0.3896 - acc: 0.8865 - val_loss: 0.6984 - val_acc: 0.8054
Epoch 130/300
4/4 [==============================] - 1s 132ms/step - loss: 0.3487 - acc: 0.8821 - val_loss: 0.6906 - val_acc: 0.8005
Epoch 131/300
4/4 [==============================] - 0s 90ms/step - loss: 0.3322 - acc: 0.9004 - val_loss: 0.6542 - val_acc: 0.8227
Epoch 132/300
4/4 [==============================] - 0s 93ms/step - loss: 0.3434 - acc: 0.8903 - val_loss: 0.6651 - val_acc: 0.8153
Epoch 133/300
4/4 [==============================] - 0s 116ms/step - loss: 0.3862 - acc: 0.8851 - val_loss: 0.6996 - val_acc: 0.8079
Epoch 134/300
4/4 [==============================] - 0s 131ms/step - loss: 0.3885 - acc: 0.8754 - val_loss: 0.7101 - val_acc: 0.7980
Epoch 135/300
4/4 [==============================] - 0s 123ms/step - loss: 0.3689 - acc: 0.8873 - val_loss: 0.6984 - val_acc: 0.8030
Epoch 136/300
4/4 [==============================] - 1s 148ms/step - loss: 0.3388 - acc: 0.8901 - val_loss: 0.6831 - val_acc: 0.8128
Epoch 137/300
4/4 [==============================] - 0s 131ms/step - loss: 0.3948 - acc: 0.8872 - val_loss: 0.6637 - val_acc: 0.8251
Epoch 138/300
4/4 [==============================] - 0s 130ms/step - loss: 0.4007 - acc: 0.8755 - val_loss: 0.6599 - val_acc: 0.8202
Epoch 139/300
4/4 [==============================] - 1s 140ms/step - loss: 0.4070 - acc: 0.8818 - val_loss: 0.6643 - val_acc: 0.8251
Epoch 140/300
4/4 [==============================] - 1s 139ms/step - loss: 0.3358 - acc: 0.8944 - val_loss: 0.6637 - val_acc: 0.8202
Epoch 141/300
4/4 [==============================] - 1s 169ms/step - loss: 0.3578 - acc: 0.9002 - val_loss: 0.6835 - val_acc: 0.8153
Epoch 142/300
4/4 [==============================] - 1s 143ms/step - loss: 0.3785 - acc: 0.8819 - val_loss: 0.7209 - val_acc: 0.8103
Epoch 143/300
4/4 [==============================] - 1s 141ms/step - loss: 0.3000 - acc: 0.9123 - val_loss: 0.7257 - val_acc: 0.8202
Epoch 144/300
4/4 [==============================] - 1s 141ms/step - loss: 0.3607 - acc: 0.8861 - val_loss: 0.7416 - val_acc: 0.8153
Epoch 145/300
4/4 [==============================] - 0s 134ms/step - loss: 0.3690 - acc: 0.8935 - val_loss: 0.7413 - val_acc: 0.8153
Epoch 146/300
4/4 [==============================] - 0s 122ms/step - loss: 0.3231 - acc: 0.8879 - val_loss: 0.7432 - val_acc: 0.8177
Epoch 147/300
4/4 [==============================] - 1s 144ms/step - loss: 0.3087 - acc: 0.9087 - val_loss: 0.7442 - val_acc: 0.8128
Epoch 148/300
4/4 [==============================] - 1s 144ms/step - loss: 0.3026 - acc: 0.8909 - val_loss: 0.7358 - val_acc: 0.8202
Epoch 149/300
4/4 [==============================] - 1s 146ms/step - loss: 0.2983 - acc: 0.8930 - val_loss: 0.7420 - val_acc: 0.8177
Epoch 150/300
4/4 [==============================] - 0s 130ms/step - loss: 0.3435 - acc: 0.8932 - val_loss: 0.7422 - val_acc: 0.8177
Epoch 151/300
4/4 [==============================] - 0s 121ms/step - loss: 0.3548 - acc: 0.8867 - val_loss: 0.7505 - val_acc: 0.8153
Epoch 152/300
4/4 [==============================] - 1s 165ms/step - loss: 0.3690 - acc: 0.8926 - val_loss: 0.7601 - val_acc: 0.8103
Epoch 153/300
4/4 [==============================] - 0s 128ms/step - loss: 0.3693 - acc: 0.8907 - val_loss: 0.7478 - val_acc: 0.8153
Epoch 154/300
4/4 [==============================] - 0s 135ms/step - loss: 0.3806 - acc: 0.8783 - val_loss: 0.7432 - val_acc: 0.8079
Epoch 155/300
4/4 [==============================] - 0s 98ms/step - loss: 0.3080 - acc: 0.9077 - val_loss: 0.7592 - val_acc: 0.7980
Epoch 156/300
4/4 [==============================] - 0s 87ms/step - loss: 0.3271 - acc: 0.8909 - val_loss: 0.7446 - val_acc: 0.8128
Epoch 157/300
4/4 [==============================] - 0s 119ms/step - loss: 0.3364 - acc: 0.8992 - val_loss: 0.7469 - val_acc: 0.8054
Epoch 158/300
4/4 [==============================] - 0s 105ms/step - loss: 0.3305 - acc: 0.8853 - val_loss: 0.7576 - val_acc: 0.8079
Epoch 159/300
4/4 [==============================] - 0s 94ms/step - loss: 0.3128 - acc: 0.9000 - val_loss: 0.7680 - val_acc: 0.8103
Epoch 160/300
4/4 [==============================] - 0s 123ms/step - loss: 0.4017 - acc: 0.8814 - val_loss: 0.7607 - val_acc: 0.8103
Epoch 161/300
4/4 [==============================] - 0s 129ms/step - loss: 0.3289 - acc: 0.9009 - val_loss: 0.7563 - val_acc: 0.8079
Epoch 162/300
4/4 [==============================] - 0s 115ms/step - loss: 0.3377 - acc: 0.8942 - val_loss: 0.7894 - val_acc: 0.8054
Epoch 163/300
4/4 [==============================] - 0s 106ms/step - loss: 0.3552 - acc: 0.8837 - val_loss: 0.7775 - val_acc: 0.8030
Epoch 164/300
4/4 [==============================] - 0s 113ms/step - loss: 0.3628 - acc: 0.8765 - val_loss: 0.7534 - val_acc: 0.8054
Epoch 165/300
4/4 [==============================] - 0s 101ms/step - loss: 0.3381 - acc: 0.8971 - val_loss: 0.7309 - val_acc: 0.7980
Epoch 166/300
4/4 [==============================] - 0s 110ms/step - loss: 0.3105 - acc: 0.9126 - val_loss: 0.7353 - val_acc: 0.8103
Epoch 167/300
4/4 [==============================] - 0s 112ms/step - loss: 0.3643 - acc: 0.8818 - val_loss: 0.7740 - val_acc: 0.8005
Epoch 168/300
4/4 [==============================] - 0s 101ms/step - loss: 0.3523 - acc: 0.8811 - val_loss: 0.7693 - val_acc: 0.8005
Epoch 169/300
4/4 [==============================] - 0s 123ms/step - loss: 0.3482 - acc: 0.8896 - val_loss: 0.7517 - val_acc: 0.8079
Epoch 170/300
4/4 [==============================] - 1s 161ms/step - loss: 0.3616 - acc: 0.8831 - val_loss: 0.7627 - val_acc: 0.8103
Epoch 171/300
4/4 [==============================] - 1s 141ms/step - loss: 0.3162 - acc: 0.9063 - val_loss: 0.7710 - val_acc: 0.8030
Epoch 172/300
4/4 [==============================] - 0s 131ms/step - loss: 0.3411 - acc: 0.8917 - val_loss: 0.7914 - val_acc: 0.8005
Epoch 173/300
4/4 [==============================] - 0s 127ms/step - loss: 0.3381 - acc: 0.8985 - val_loss: 0.7978 - val_acc: 0.8030
Epoch 174/300
4/4 [==============================] - 0s 103ms/step - loss: 0.3311 - acc: 0.9122 - val_loss: 0.8184 - val_acc: 0.8005
Epoch 175/300
4/4 [==============================] - 0s 113ms/step - loss: 0.3108 - acc: 0.8886 - val_loss: 0.8308 - val_acc: 0.7980
Epoch 176/300
4/4 [==============================] - 0s 120ms/step - loss: 0.3181 - acc: 0.8941 - val_loss: 0.8196 - val_acc: 0.8005
Epoch 177/300
4/4 [==============================] - 1s 137ms/step - loss: 0.2624 - acc: 0.9131 - val_loss: 0.7880 - val_acc: 0.7906
Epoch 178/300
4/4 [==============================] - 0s 121ms/step - loss: 0.3221 - acc: 0.9057 - val_loss: 0.7745 - val_acc: 0.7980
Epoch 179/300
4/4 [==============================] - 1s 141ms/step - loss: 0.3334 - acc: 0.8930 - val_loss: 0.8004 - val_acc: 0.8030
Epoch 180/300
4/4 [==============================] - 0s 128ms/step - loss: 0.3100 - acc: 0.8925 - val_loss: 0.8310 - val_acc: 0.7980
Epoch 181/300
4/4 [==============================] - 0s 123ms/step - loss: 0.3432 - acc: 0.9036 - val_loss: 0.8333 - val_acc: 0.7906
Epoch 182/300
4/4 [==============================] - 0s 110ms/step - loss: 0.2948 - acc: 0.9093 - val_loss: 0.8082 - val_acc: 0.8030
Epoch 183/300
4/4 [==============================] - 0s 124ms/step - loss: 0.3648 - acc: 0.8885 - val_loss: 0.7874 - val_acc: 0.8005
Epoch 184/300
4/4 [==============================] - 0s 121ms/step - loss: 0.2932 - acc: 0.9188 - val_loss: 0.7964 - val_acc: 0.8079
Epoch 185/300
4/4 [==============================] - 0s 115ms/step - loss: 0.3257 - acc: 0.8979 - val_loss: 0.8066 - val_acc: 0.8030
Epoch 186/300
4/4 [==============================] - 0s 125ms/step - loss: 0.2868 - acc: 0.9182 - val_loss: 0.8193 - val_acc: 0.7980
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 187/300
4/4 [==============================] - 0s 117ms/step - loss: 0.3059 - acc: 0.8979 - val_loss: 0.8079 - val_acc: 0.8079
</pre></div>
</div>
</div>
</div>
<p>Let’s plot the learning curves</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">display_learning_curves</span><span class="p">(</span><span class="n">history</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/GraphNeuralNetworks_75_0.png" src="../_images/GraphNeuralNetworks_75_0.png" />
</div>
</div>
<p>Now we evaluate the GNN model on the test data split.
The results may vary depending on the training sample, however the GNN model always outperforms
the baseline model in terms of the test accuracy.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_test</span> <span class="o">=</span> <span class="n">test_data</span><span class="o">.</span><span class="n">paper_id</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">_</span><span class="p">,</span> <span class="n">test_accuracy</span> <span class="o">=</span> <span class="n">gnn_model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Test accuracy: </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="n">test_accuracy</span> <span class="o">*</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Test accuracy: 79.41%
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="examine-the-gnn-model-predictions">
<h3>Examine the GNN model predictions<a class="headerlink" href="#examine-the-gnn-model-predictions" title="Permalink to this headline">¶</a></h3>
<p>Let’s add the new instances as nodes to the <code class="docutils literal notranslate"><span class="pre">node_features</span></code>, and generate links
(citations) to existing nodes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># First we add the N new_instances as nodes to the graph</span>
<span class="c1"># by appending the new_instance to node_features.</span>
<span class="n">num_nodes</span> <span class="o">=</span> <span class="n">node_features</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">new_node_features</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">node_features</span><span class="p">,</span> <span class="n">new_instances</span><span class="p">])</span>
<span class="c1"># Second we add the M edges (citations) from each new node to a set</span>
<span class="c1"># of existing nodes in a particular subject</span>
<span class="n">new_node_indices</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="n">num_nodes</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_classes</span><span class="p">)]</span>
<span class="n">new_citations</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">subject_idx</span><span class="p">,</span> <span class="n">group</span> <span class="ow">in</span> <span class="n">papers</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;subject&quot;</span><span class="p">):</span>
    <span class="n">subject_papers</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">group</span><span class="o">.</span><span class="n">paper_id</span><span class="p">)</span>
    <span class="c1"># Select random x papers specific subject.</span>
    <span class="n">selected_paper_indices1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">subject_papers</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
    <span class="c1"># Select random y papers from any subject (where y &lt; x).</span>
    <span class="n">selected_paper_indices2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">papers</span><span class="o">.</span><span class="n">paper_id</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span>
    <span class="c1"># Merge the selected paper indices.</span>
    <span class="n">selected_paper_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
        <span class="p">[</span><span class="n">selected_paper_indices1</span><span class="p">,</span> <span class="n">selected_paper_indices2</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span>
    <span class="p">)</span>
    <span class="c1"># Create edges between a citing paper idx and the selected cited papers.</span>
    <span class="n">citing_paper_indx</span> <span class="o">=</span> <span class="n">new_node_indices</span><span class="p">[</span><span class="n">subject_idx</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">cited_paper_idx</span> <span class="ow">in</span> <span class="n">selected_paper_indices</span><span class="p">:</span>
        <span class="n">new_citations</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">citing_paper_indx</span><span class="p">,</span> <span class="n">cited_paper_idx</span><span class="p">])</span>

<span class="n">new_citations</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">new_citations</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
<span class="n">new_edges</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">edges</span><span class="p">,</span> <span class="n">new_citations</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now let’s update the <code class="docutils literal notranslate"><span class="pre">node_features</span></code> and the <code class="docutils literal notranslate"><span class="pre">edges</span></code> in the GNN model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Original node_features shape:&quot;</span><span class="p">,</span> <span class="n">gnn_model</span><span class="o">.</span><span class="n">node_features</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Original edges shape:&quot;</span><span class="p">,</span> <span class="n">gnn_model</span><span class="o">.</span><span class="n">edges</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">gnn_model</span><span class="o">.</span><span class="n">node_features</span> <span class="o">=</span> <span class="n">new_node_features</span>
<span class="n">gnn_model</span><span class="o">.</span><span class="n">edges</span> <span class="o">=</span> <span class="n">new_edges</span>
<span class="n">gnn_model</span><span class="o">.</span><span class="n">edge_weights</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">new_edges</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;New node_features shape:&quot;</span><span class="p">,</span> <span class="n">gnn_model</span><span class="o">.</span><span class="n">node_features</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;New edges shape:&quot;</span><span class="p">,</span> <span class="n">gnn_model</span><span class="o">.</span><span class="n">edges</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">logits</span> <span class="o">=</span> <span class="n">gnn_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">new_node_indices</span><span class="p">))</span>
<span class="n">probabilities</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">activations</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">logits</span><span class="p">))</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">display_class_probabilities</span><span class="p">(</span><span class="n">probabilities</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Original node_features shape: (2708, 1433)
Original edges shape: (2, 5429)
New node_features shape: (2715, 1433)
New edges shape: (2, 5478)
Instance 1:
- 0: 79.83%
- 1: 11.26%
- 2: 1.79%
- 3: 1.63%
- 4: 0.44%
- 5: 3.27%
- 6: 1.78%
Instance 2:
- 0: 0.21%
- 1: 91.51%
- 2: 0.32%
- 3: 0.6%
- 4: 7.27%
- 5: 0.05%
- 6: 0.04%
Instance 3:
- 0: 0.47%
- 1: 1.5%
- 2: 57.76%
- 3: 5.35%
- 4: 28.49%
- 5: 0.32%
- 6: 6.11%
Instance 4:
- 0: 0.53%
- 1: 0.1%
- 2: 0.55%
- 3: 98.61%
- 4: 0.01%
- 5: 0.03%
- 6: 0.16%
Instance 5:
- 0: 0.9%
- 1: 7.48%
- 2: 11.73%
- 3: 1.42%
- 4: 72.63%
- 5: 0.53%
- 6: 5.32%
Instance 6:
- 0: 0.4%
- 1: 0.23%
- 2: 94.24%
- 3: 1.21%
- 4: 0.38%
- 5: 0.72%
- 6: 2.81%
Instance 7:
- 0: 2.98%
- 1: 0.36%
- 2: 0.67%
- 3: 0.25%
- 4: 0.13%
- 5: 43.06%
- 6: 52.55%
</pre></div>
</div>
</div>
</div>
<p>Notice that the probabilities of the expected subjects
(to which several citations are added) are higher compared to the baseline model.</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./neuralnetworks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="../text/02TextClassification.html" title="previous page">Text classification with CNNs and LSTMs</a>
    <a class='right-next' id="next-link" href="../transformer/attention.html" title="next page">Sequence-To-Sequence, Attention, Transformer</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Prof. Dr. Johannes Maucher<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>