
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Neural Networks Introduction &#8212; Machine Learning Lecture</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Recurrent Neural Networks" href="02RecurrentNeuralNetworks.html" />
    <link rel="prev" title="Gaussian Process: Implementation in Python" href="../machinelearning/GaussianProcessRegression.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/hdmlogomed.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Machine Learning Lecture</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Intro and Overview Machine Learning Lecture
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../00BasicConcepts.html">
   Basic Concepts of Data Mining and Machine Learning
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Conventional ML
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../machinelearning/knn.html">
   K - Nearest Neighbour Classification / Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machinelearning/parametricClassification1D.html">
   Bayes- and Naive Bayes Classifier
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machinelearning/LinReg.html">
   Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machinelearning/heartRateRegression.html">
   Example Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machinelearning/LinearClassification.html">
   Linear Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machinelearning/svm.html">
   Support Vector Machines (SVM)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machinelearning/gp.html">
   Gaussian Process
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machinelearning/GaussianProcessRegression.html">
   Gaussian Process: Implementation in Python
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Neural Networks
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Neural Networks Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02RecurrentNeuralNetworks.html">
   Recurrent Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03ConvolutionNeuralNetworks.html">
   Convolutional Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="convolutionDemos.html">
   Animations of Convolution and Deconvolution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03KerasMLPandCNNcifar.html">
   Implementing Neural Networks with Keras
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="04KerasPretrainedClassifiers.html">
   Applying Pretrained Deep Neural Networks for Image Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="05KerasPretrainedCovid.html">
   Apply Pretrained Neural Networks on new Task
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Autoencoder
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="04VariationalAutoencoder.html">
   Variational Autoencoder (VAE) to generate handwritten digits
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  GAN
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../gan/GAN.html">
   Generative Adversarial Nets (GAN)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../gan/DCGAN.html">
   DCGAN Keras Implementation
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Diffusion Models
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../diffusion/denoisingDiffusion.html">
   Difussion Models
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Reinforcement Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../rl/reinforcement.html">
   Reinforcement Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../rl/DQN.html">
   Deep Reinforcement Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../rl/QLearnFrozenLake.html">
   Example Q-Learning
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Text
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../text/01ModellingWordsAndTexts.html">
   Representations for Words and Texts
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../text/02TextClassification.html">
   Text classification with CNNs and LSTMs
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Graph Neural Networks
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="GraphNeuralNetworks.html">
   Graph Neural Networks (GNN)
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Transformer
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../transformer/attention.html">
   Sequence-To-Sequence, Attention, Transformer
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../transformer/intent_classification_with_bert.html">
   Intent Classification with BERT
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../referenceSection.html">
   References
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/neuralnetworks/01NeuralNets.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/neuralnetworks/01NeuralNets.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#natural-neuron">
   Natural Neuron
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#artificial-neuron">
   Artificial Neuron
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#activation-function">
     Activation Function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bias">
     Bias
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#artificial-neural-networks-general-notions">
   Artificial Neural Networks: General Notions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#layers">
     Layers
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#feedforward-and-recurrent-neural-networks">
     Feedforward- and Recurrent Neural Networks
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#general-concept-of-supervised-learning">
     General Concept of Supervised Learning
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradient-descent-learning">
     Gradient Descent Learning
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     Gradient Descent Learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#single-layer-perceptron">
   Single Layer Perceptron
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#slp-for-regression">
     SLP for Regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#slp-for-binary-classification">
     SLP for binary classification
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#slp-for-classification-in-k-2-classes">
     SLP for classification in
     <span class="math notranslate nohighlight">
      \(K&gt;2\)
     </span>
     classes
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gradient-of-error-function">
   Gradient of Error Function
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#stochastic-gradient-descent-learning">
     Stochastic Gradient Descent Learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary-single-layer-perceptron">
   Summary Single Layer Perceptron
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multi-layer-perceptron">
   Multi Layer Perceptron
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#notations-and-basic-characteristics">
     Notations and Basic Characteristics
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#architecture">
   Architecture
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#number-of-hidden-layers">
     Number of hidden layers
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#activation-and-loss-functions">
     Activation- and Loss-functions
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#activation-functions-in-hidden-layers">
       Activation functions in hidden layers
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#activation-functions-in-the-output-layer-and-loss-functions">
       Activation functions in the output layer and loss functions
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   Gradient Descent Learning
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#hyperparameters">
   Hyperparameters
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#early-mlp-example-autonomos-driving">
   Early MLP Example: Autonomos Driving
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Neural Networks Introduction</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#natural-neuron">
   Natural Neuron
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#artificial-neuron">
   Artificial Neuron
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#activation-function">
     Activation Function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bias">
     Bias
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#artificial-neural-networks-general-notions">
   Artificial Neural Networks: General Notions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#layers">
     Layers
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#feedforward-and-recurrent-neural-networks">
     Feedforward- and Recurrent Neural Networks
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#general-concept-of-supervised-learning">
     General Concept of Supervised Learning
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradient-descent-learning">
     Gradient Descent Learning
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     Gradient Descent Learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#single-layer-perceptron">
   Single Layer Perceptron
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#slp-for-regression">
     SLP for Regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#slp-for-binary-classification">
     SLP for binary classification
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#slp-for-classification-in-k-2-classes">
     SLP for classification in
     <span class="math notranslate nohighlight">
      \(K&gt;2\)
     </span>
     classes
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gradient-of-error-function">
   Gradient of Error Function
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#stochastic-gradient-descent-learning">
     Stochastic Gradient Descent Learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary-single-layer-perceptron">
   Summary Single Layer Perceptron
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multi-layer-perceptron">
   Multi Layer Perceptron
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#notations-and-basic-characteristics">
     Notations and Basic Characteristics
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#architecture">
   Architecture
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#number-of-hidden-layers">
     Number of hidden layers
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#activation-and-loss-functions">
     Activation- and Loss-functions
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#activation-functions-in-hidden-layers">
       Activation functions in hidden layers
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#activation-functions-in-the-output-layer-and-loss-functions">
       Activation functions in the output layer and loss functions
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   Gradient Descent Learning
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#hyperparameters">
   Hyperparameters
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#early-mlp-example-autonomos-driving">
   Early MLP Example: Autonomos Driving
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="neural-networks-introduction">
<h1>Neural Networks Introduction<a class="headerlink" href="#neural-networks-introduction" title="Permalink to this headline">#</a></h1>
<ul class="simple">
<li><p>Author: Johannes Maucher</p></li>
<li><p>Last Update: 10.05.2022</p></li>
</ul>
<section id="natural-neuron">
<h2>Natural Neuron<a class="headerlink" href="#natural-neuron" title="Permalink to this headline">#</a></h2>
<p><img alt="Natural Neuron" src="https://maucher.home.hdm-stuttgart.de/Pics/neuron.png" /></p>
<p>Neurons are the basic elements for information processing. A neuron consists of a cell-body, many dendrites and an axon. The neuron receives electrical signals from other neurons via the dendrites. In the cell-body all input-signals received via the dendrites are accumulated. If the accumulated electrical signal exceeds a certain threshold, the cell-body outputs an electrical signal via it’s axon. In this case the neuron is said to be activated. Otherwise, if the accumulated input at the cell-body is below the threshold, the neuron is not active, i.e. it does not send a signal to connected neurons. The point, where dendrites of neurons are connected to axons of other neurons is called synapse. The synapse consists of an electrochemical substance. The conductivity of this substance depends on it’s concentration of neurotransmitters. The process of learning adapts the conductivity of synapses and, i.e. the degree of connection between neurons. A single neuron can receive inputs from 10-100000 other neurons. However, there is only one axon, but multiple dendrites of other cell can be connected to this axon.</p>
</section>
<section id="artificial-neuron">
<h2>Artificial Neuron<a class="headerlink" href="#artificial-neuron" title="Permalink to this headline">#</a></h2>
<img src="http://maucher.home.hdm-stuttgart.de/Pics/einneuron.jpg" width="400">
<p>The artificial model of a neuron is shown in the picture below. At the input of each neuron the weighted sum</p>
<div class="math notranslate nohighlight">
\[in=\sum\limits_{j=0}^d w_jx_j = \mathbf{w}\cdot \mathbf{x^T}=(w_0, w_1, \ldots, w_d) \cdot (x_0, x_1, \ldots, x_d)^T \]</div>
<p>is calculated. The values <span class="math notranslate nohighlight">\(x_j\)</span> are the outputs of other neurons. Each <span class="math notranslate nohighlight">\(x_j\)</span> is weighted by a scalar <span class="math notranslate nohighlight">\(w_j\)</span>, similar as in the natural model the signal-strength from a connected neuron is damped by the conductivity of the synapse. As in the natural model, learning of an artificial network means adaptation of the weights between neurons. Also, as in the natural model, the weighted sum at the input of the neuron is fed to an <strong>activation function g()</strong>, which can be a simple threshold-function that outputs a <code class="docutils literal notranslate"><span class="pre">1</span></code> if the weighted sum <span class="math notranslate nohighlight">\(in=\sum\limits_{j=0}^d w_jx_j\)</span>  exceeds a certain threshold and a <code class="docutils literal notranslate"><span class="pre">0</span></code> otherwise.</p>
<section id="activation-function">
<h3>Activation Function<a class="headerlink" href="#activation-function" title="Permalink to this headline">#</a></h3>
<p>The most common activation functions are:</p>
<ul>
<li><p><strong>Threshold:</strong></p>
<div class="math notranslate nohighlight">
\[\begin{split}g(in)= \left\lbrace \begin{array}{ll} 1, &amp; in \geq 0 \\ 0, &amp; else \\ \end{array} \right.\end{split}\]</div>
</li>
<li><p><strong>Sigmoid:</strong></p>
<div class="math notranslate nohighlight">
\[g(in)=\frac{1}{1+exp(-in)}\]</div>
</li>
<li><p><strong>Tanh:</strong></p>
<div class="math notranslate nohighlight">
\[g(in)=\tanh(in)\]</div>
</li>
<li><p><strong>Identity:</strong></p>
<div class="math notranslate nohighlight">
\[
	g(in)=in
	\]</div>
</li>
<li><p><strong>ReLu:</strong></p>
<div class="math notranslate nohighlight">
\[g(in)=max\left( 0 , in \right)\]</div>
</li>
<li><p><strong>Softmax:</strong></p>
<div class="math notranslate nohighlight">
\[g(in_i,in_j)=\frac{\exp(in_i)}{\sum\limits_{j=1}^{K} \exp(in_j)}\]</div>
</li>
</ul>
<p><img alt="Activationfunctions" src="https://maucher.home.hdm-stuttgart.de/Pics/activationsViz.png" /></p>
<p>All artificial neurons calculate the sum of weighted inputs <span class="math notranslate nohighlight">\(in\)</span>. Neurons differ in the activation function, which is applied on <span class="math notranslate nohighlight">\(in\)</span>. In the sections below it will be described how to choose an appropriate activation function.</p>
</section>
<section id="bias">
<h3>Bias<a class="headerlink" href="#bias" title="Permalink to this headline">#</a></h3>
<p>Among the input-signals, <span class="math notranslate nohighlight">\(x_0\)</span> has a special meaning. In contrast to all other <span class="math notranslate nohighlight">\(x_j\)</span> the value of this so called <strong>bias</strong> is constant <span class="math notranslate nohighlight">\(x_0=1\)</span>. Instead of denoting the bias input to a neuron by <span class="math notranslate nohighlight">\(w_0 \cdot x_0 = w_0\)</span> it can also be written as <span class="math notranslate nohighlight">\(b\)</span>. I.e.</p>
<div class="math notranslate nohighlight">
\[in=\sum\limits_{j=0}^d w_jx_j  \quad \mbox{  is equivalent to  } \quad in=\sum\limits_{j=1}^d w_jx_j+b\]</div>
<p>Hence the following two graphical representations are equivalent:</p>
<img src="https://maucher.home.hdm-stuttgart.de/Pics/slpPresentations.png" style="width: 700px;"/></section>
</section>
<section id="artificial-neural-networks-general-notions">
<h2>Artificial Neural Networks: General Notions<a class="headerlink" href="#artificial-neural-networks-general-notions" title="Permalink to this headline">#</a></h2>
<section id="layers">
<h3>Layers<a class="headerlink" href="#layers" title="Permalink to this headline">#</a></h3>
<img src="https://maucher.home.hdm-stuttgart.de/Pics/mlpL3.png" style="width: 600px;"/><p>In Neural Networks neurons are arranged in layers. All Neurons of a single layer are of the same type, i.e. they apply the same activation function on the weighted sum at their input (see previous section). Each Neural Network has at least one input-layer and one output-layer. The number of neurons in the input-layer is determined by the number of features (attributes) in the given Machine-Learning problem. The number of neurons in the output-layer depends on the task. E.g. for <strong>binary-classification</strong> and <strong>regression</strong> only one neuron in the output-layer is requried, for classification into <span class="math notranslate nohighlight">\(K&gt;2\)</span> classes the output-layer consists of <span class="math notranslate nohighlight">\(K\)</span> neurons.</p>
<p>Actually, the <strong>input-layer</strong> is not considered as a <em>real</em> layer, since it only takes in the values of the current feature-vector, but does not perform any processing, such as calculating an activation function of a weighted sum. The input layer is ignored when determining the number of layers in a neural-network.</p>
<p>For <strong>example</strong> for a binary credit-worthiness classification of customers, which are modelled by the numeric features <em>age, annual income, equity</em>, <span class="math notranslate nohighlight">\(3+1=4\)</span> neurons are required at the input (3 neurons <span class="math notranslate nohighlight">\(x_1,x_2,x_3\)</span> for the 3 features plus the constant bias <span class="math notranslate nohighlight">\(x_0=1\)</span>) and one neuron is required at the output. For non-numeric features at the input, the number of neurons in the inut-layer is not directly given by the number of features, since each non-numeric feature must be <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html">One-Hot encoded</a> before passing it to the Neural Network.</p>
<p>Inbetween the input- and the output-layer there may be zero, one or more other layers. The number of layers in a Neural Network is an essential architectural hyperparameter. <strong>Hyperparameters</strong> in Neural Networks, as well as in all other Machine Learning algorithms, are parameters, which are not learned automatically in the training phase, but must be configured from outside. Finding appropriate hyperparameters for the given task and the given data is possibly the most challenging task in machine-learning.</p>
</section>
<section id="feedforward-and-recurrent-neural-networks">
<h3>Feedforward- and Recurrent Neural Networks<a class="headerlink" href="#feedforward-and-recurrent-neural-networks" title="Permalink to this headline">#</a></h3>
<p>In <strong>Feedforward Neural Networks (FNN)</strong> signals are propagated only in one direction - from the input- towards the output layer. In a network with <span class="math notranslate nohighlight">\(L\)</span> layers, the input-layer is typically indexed by 0 and the output-layer’s index is <span class="math notranslate nohighlight">\(L\)</span> (as mentioned above the input-layer is ignored in the layer-count). Then in a FNN the output of layer <span class="math notranslate nohighlight">\(j\)</span> can be passed to the input of neurons in layer <span class="math notranslate nohighlight">\(i\)</span>, if and only if <span class="math notranslate nohighlight">\(i&gt;j\)</span>.</p>
<p><strong>Recurrent Neural Networks (RNN)</strong>, in contrast to FNNs, not only have forward connections, but also backward-connections. I.e the output of neurons in layer <span class="math notranslate nohighlight">\(j\)</span> can be passed to the input of neurons in the same layer or to neurons in layers of index <span class="math notranslate nohighlight">\(k&lt;j\)</span>.</p>
</section>
<section id="general-concept-of-supervised-learning">
<h3>General Concept of Supervised Learning<a class="headerlink" href="#general-concept-of-supervised-learning" title="Permalink to this headline">#</a></h3>
<p>Neural Networks can be applied for supervised and unsupervised learning. By far the most applications apply Neural Networks for <strong>supervised learning</strong> for classification or regression. This notebook only considers this case. Neural Networks for unsupervised learning would be for example <a class="reference external" href="https://en.wikipedia.org/wiki/Self-organizing_map">Self Organizing Maps</a>, <a class="reference external" href="https://en.wikipedia.org/wiki/Autoencoder">Auto Encoders</a> or <a class="reference external" href="https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine">Restricted Boltzmann Machines</a>.</p>
<p>The general concept of supervised learning of a neural network is sketched in the picture below.</p>
<p><img alt="Principle of Learning" src="https://maucher.home.hdm-stuttgart.de/Pics/learnGradientDescent.png" /></p>
<p>In supervised learning each training element is a pair of input/target. The input contains the observable features, and the target is either the true class-label in the case of classification or the true numeric output value in the case of regression. A Neural Network is trained by passing a single training-element to the network. For the given input the output of the network is calculated, based on the current weight values. This output of the network is compared with the target. As long as there is a significant difference between the output and the target, the weights of the networks are adapted.
In a well trained network, the deviation between output and target is as small as possible for all training-elements.</p>
</section>
<section id="gradient-descent-learning">
<h3>Gradient Descent Learning<a class="headerlink" href="#gradient-descent-learning" title="Permalink to this headline">#</a></h3>
<p>In the previous section the general idea of training a Neural Network has been presented. Now, a concrete realization of this concept is described - <strong>Gradient Descent -</strong> and <strong>Stochastic Gradient Descent Learning</strong>. This approach is not only applied for all types of Neural Networks, but for many other supervised Machine Learning algorithms.</p>
</section>
<section id="id1">
<h3>Gradient Descent Learning<a class="headerlink" href="#id1" title="Permalink to this headline">#</a></h3>
<p>The concept of Gradient Descent Learning is as follows:</p>
<ol class="simple">
<li><p>Define a <strong>Loss Function</strong> <span class="math notranslate nohighlight">\(E(T,\Theta)\)</span>, which somehow measures the deviation between the current network <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> output and the target output <span class="math notranslate nohighlight">\(\mathbf{r}\)</span>. As above,</p></li>
</ol>
<div class="math notranslate nohighlight">
\[T=\lbrace(x_{1,t},x_{2,t},\ldots,x_{d,t}),r_t \rbrace_{t=1}^N,\]</div>
<p>is the set of labeled training data and</p>
<div class="math notranslate nohighlight">
\[\Theta=\lbrace W_{1,0},W_{1,1},\ldots, W_{K,d+1} \rbrace\]</div>
<p>is the set of parameters (weights), which are adapted during training.
2. Calculate the gradient of the Loss Function:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\nabla E(T,\Theta) = \left( \begin{array}{c}  \frac{\partial E}{\partial W_{1,0}} \\ \frac{\partial E}{\partial W_{1,1}} \\ \vdots \\  \frac{\partial E}{\partial W_{K,d+1}} \end{array} \right). \end{split}\]</div>
<p>The gradient of a function points towards the steepest ascent of the function at the point, where it is calculated. The negative gradient <span class="math notranslate nohighlight">\(-\nabla E(T,\Theta)\)</span> points towards the steepest descent of the function.
3. Adapt all parameters into the direction of the negative gradient. This weight adaptation guarantees that the Loss Function is iteratively minimized.:</p>
<div class="math notranslate nohighlight">
\[W_{i,j}=W_{i,j}+\Delta W_{i,j} = W_{i,j}+\eta \cdot -\frac{\partial E}{\partial W_{i,j}},\]</div>
<p>where <span class="math notranslate nohighlight">\(\eta\)</span> is the important hyperparameter <strong>learning rate</strong>. The learning rate controls the step-size of weight adaptations. A small <span class="math notranslate nohighlight">\(\eta\)</span> implies that weights are adapted only slightly per iteration and the learning algorithm converges slowly. A large learning-rate implies strong adaptations per iteration. However, in this case the risk of <em>jumping over the minimum</em> is increased. Typical values for <span class="math notranslate nohighlight">\(\eta\)</span> are in the range of <span class="math notranslate nohighlight">\([0.0001,0.1]\)</span>.</p>
<p>Below, the calculation of</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial E}{\partial W_{i,j}},
\]</div>
<p>for regression, binary and non-binary classification is presented.-</p>
<p><img alt="Gradient Descent Flowchart" src="https://maucher.home.hdm-stuttgart.de/Pics/peaksexampleboth.jpg" /></p>
</section>
</section>
<section id="single-layer-perceptron">
<h2>Single Layer Perceptron<a class="headerlink" href="#single-layer-perceptron" title="Permalink to this headline">#</a></h2>
<p>A Single Layer Perceptron (SLP) is a Feedforward Neural Network (FNN), which consists only of an input- and an output layer (the output-layer is the <em>single</em> layer). All neurons of the input layer are connected to all neurons of the output layer. A layer with this property is also called a <strong>fully-connected layer</strong> or a <strong>dense layer</strong>. SLPs can be applied to learn</p>
<ul class="simple">
<li><p>a linear binary classifier</p></li>
<li><p>a linear classifier for more than 2 classes</p></li>
<li><p>a linear regression model</p></li>
</ul>
<section id="slp-for-regression">
<h3>SLP for Regression<a class="headerlink" href="#slp-for-regression" title="Permalink to this headline">#</a></h3>
<p>A SLP can be applied to learn a linear function</p>
<div class="math notranslate nohighlight">
\[y=f(x_1,x_2,\ldots,x_d)\]</div>
<p>from a set of N supervised observations</p>
<div class="math notranslate nohighlight">
\[T=\lbrace(x_{1,t},x_{2,t}, ,x_{d,t}),r_t \rbrace_{t=1}^N,\]</div>
<p>where <span class="math notranslate nohighlight">\(x_{j,t}\)</span> is the <a class="reference external" href="http://j.th">j.th</a> feature of the <a class="reference external" href="http://t.th">t.th</a> training-element and <span class="math notranslate nohighlight">\(r_t\)</span> is the numeric target value of the <a class="reference external" href="http://t.th">t.th</a> training-element.</p>
<img src="https://maucher.home.hdm-stuttgart.de/Pics/slpW0.png" width="350" class="center"><p>As depicted above, for linear regression only a <strong>single neuron in the output-layer</strong> is required. The activation function <span class="math notranslate nohighlight">\(g()\)</span> applied for regression is the <strong>identity function</strong>. The loss-function, which is minimized in the training procedure is the <strong>sum of squared error</strong>:</p>
<div class="math notranslate nohighlight">
\[
E(T,\Theta)=SSE(T,\Theta)= \frac{1}{2} \sum\limits_{t=1}^N (r_t-y_t)^2 = \frac{1}{2} \sum\limits_{t=1}^N \left( r_t-\sum\limits_{j=0}^d w_j x_{j,t}\right)^2,
\]</div>
<p>where <span class="math notranslate nohighlight">\(\Theta=\lbrace w_0,w_1,\ldots, w_d \rbrace\)</span> is the set of weights, which are adapted in the training process.</p>
<p><strong>Example:</strong> The <span class="math notranslate nohighlight">\(N=5\)</span> training-elements given in the table of the picture below contain only a single input feature <span class="math notranslate nohighlight">\(x_1\)</span> and the corresponding target-value <span class="math notranslate nohighlight">\(r\)</span>. From these training-elements a SLP can learn a linear function <span class="math notranslate nohighlight">\(y=w_0+w_1 x_1\)</span>, which minimizes the loss-function SSE.</p>
<p><img alt="Linear Regression" src="https://maucher.home.hdm-stuttgart.de/Pics/slp1dimlinearregression.png" /></p>
</section>
<section id="slp-for-binary-classification">
<h3>SLP for binary classification<a class="headerlink" href="#slp-for-binary-classification" title="Permalink to this headline">#</a></h3>
<p>A SLP can be applied to learn a binary classifier from a set of N labeled observations</p>
<div class="math notranslate nohighlight">
\[T=\lbrace(x_{1,t},x_{2,t},\ldots,x_{d,t}),r_t \rbrace_{t=1}^N,\]</div>
<p>where <span class="math notranslate nohighlight">\(x_{j,t}\)</span> is the <a class="reference external" href="http://j.th">j.th</a> feature of the <a class="reference external" href="http://t.th">t.th</a> training-element and <span class="math notranslate nohighlight">\(r_t \in \lbrace 0,1 \rbrace\)</span> is the class-index of the <a class="reference external" href="http://t.th">t.th</a> training-element.</p>
<img src="https://maucher.home.hdm-stuttgart.de/Pics/slpW0.png" width="350" class="center"><p>As depicted above, for binary classification only a <strong>single neuron in the output-layer</strong> is required. The activation function <span class="math notranslate nohighlight">\(g()\)</span> applied for binary classification is either the <strong>threshold-</strong> or the <strong>sigmoid-function</strong>. The threshold-function output values are either 0 or 1, i.e. this function can provide only a <em>hard</em> classifikcation-decision, with no further information on the certainty of this decision. In contrast the value range of the sigmoid-function covers all floats between 0 and 1. It can be shown that if the weighted-sum is processed by the sigmoid-function the output is an indicator for the a-posteriori propability that the given observation belongs to class <span class="math notranslate nohighlight">\(C_1\)</span>:</p>
<div class="math notranslate nohighlight">
\[P(C_1|(x_{1},x_{2}, ,x_{d}))=1-P(C_0|(x_{1},x_{2},\ldots,x_{d})).\]</div>
<p>If the output value</p>
<div class="math notranslate nohighlight">
\[y=sigmoid(\sum\limits_{j=0}^d w_j x_{j,t})\]</div>
<p>is larger than 0.5 the observation <span class="math notranslate nohighlight">\((x_{1},x_{2}, \ldots,x_{d})\)</span> is assigned to class <span class="math notranslate nohighlight">\(C_1\)</span>, otherwise it is assigned to class <span class="math notranslate nohighlight">\(C_0\)</span>. A value close to 0.5 indicates an uncertaion decision, whereas a value close to 0 or 1 indicates a certain decision.</p>
<p>In the case that the sigmoid-activation function is applied, the loss-function, which is minimized in the training procedure is the <strong>binary cross-entropy function</strong>:</p>
<div class="math notranslate nohighlight">
\[
E(T,\Theta)=  - \sum\limits_{t=1}^N r_{t} \log y_{t}+(1-r_{t}) \log(1-y_{t}),
\]</div>
<p>where <span class="math notranslate nohighlight">\(r_t\)</span> is the target class-index and <span class="math notranslate nohighlight">\(y_t\)</span> is the output of the sigmoid-function, for the <a class="reference external" href="http://t.th">t.th</a> training-element. Again, <span class="math notranslate nohighlight">\(\Theta=\lbrace w_0,w_1,\ldots, w_d \rbrace\)</span> is the set of weights, which are adapted in the training process.</p>
<p><strong>Example:</strong> The <span class="math notranslate nohighlight">\(N=9\)</span> 2-dimensional labeled training-elements given in the table of the picture below are applied to learn a SLP for binary classification. The learned model can be specified by the parameters (weights)</p>
<div class="math notranslate nohighlight">
\[w_0=-3, w_1=0.6, w_2=1.\]</div>
<p>These weights define a line</p>
<div class="math notranslate nohighlight">
\[w_0+w_1x_1+w_2x_2=0 \Longrightarrow x_2 = -\frac{w_1}{w_2}x_1 -\frac{w_0}{w_2} ,\]</div>
<p>whose slope is</p>
<div class="math notranslate nohighlight">
\[m=-\frac{w_1}{w_2}=-0.6\]</div>
<p>and whose intersection with the <span class="math notranslate nohighlight">\(x_2\)</span>-axis is</p>
<div class="math notranslate nohighlight">
\[
b=-\frac{w_0}{w_2}=3. 
\]</div>
<img src="https://maucher.home.hdm-stuttgart.de/Pics/slpBinaryClassification.png" style="width: 600px;">
<p>Once this model, i.e. the set of weights, is learned it can be applied for classification as follows: A new observation <span class="math notranslate nohighlight">\(\mathbf{x'}=(x'_1,x'_2)\)</span> is inserted into the learned equation <span class="math notranslate nohighlight">\(w_0 \cdot 1 + w_1 \cdot x'_1 + w_2 \cdot x'_2\)</span>. The result of this linear equation is passed to the sigmoid-function. If sigmoid-function’s output is <span class="math notranslate nohighlight">\(&gt;0.5\)</span> the most probable class is <span class="math notranslate nohighlight">\(C_1\)</span>, otherwise it is <span class="math notranslate nohighlight">\(C_0\)</span>.</p>
</section>
<section id="slp-for-classification-in-k-2-classes">
<h3>SLP for classification in <span class="math notranslate nohighlight">\(K&gt;2\)</span> classes<a class="headerlink" href="#slp-for-classification-in-k-2-classes" title="Permalink to this headline">#</a></h3>
<p>A SLP can be applied to learn a non-binary classifier from a set of N labeled observations</p>
<div class="math notranslate nohighlight">
\[T=\lbrace(x_{1,t},x_{2,t}, \ldots, x_{d,t}),r_t \rbrace_{t=1}^N,\]</div>
<p>where <span class="math notranslate nohighlight">\(x_{j,t}\)</span> is the <a class="reference external" href="http://j.th">j.th</a> feature of the <a class="reference external" href="http://t.th">t.th</a> training-element and <span class="math notranslate nohighlight">\(r_t \in \lbrace 0,1 \rbrace\)</span> is the class-index of the <a class="reference external" href="http://t.th">t.th</a> training-element.
<a id="slpmulitclass"></a>
<img src="https://maucher.home.hdm-stuttgart.de/Pics/slpK3B.png" width="450" class="center"></p>
<p>As depicted above, for classification into <span class="math notranslate nohighlight">\(K&gt;2\)</span> classes <strong>K neurons are required in the output-layer</strong>. The activation function <span class="math notranslate nohighlight">\(g()\)</span> applied for non-binary classification is usually the <strong>softmax-function</strong>:</p>
<div class="math notranslate nohighlight">
\[g(in_i,in_j)=\frac{\exp(in_i)}{\sum\limits_{j=1}^{K} \exp(in_j)} \quad with \quad in_j=\sum\limits_{j=0}^d w_j x_{j,t}\]</div>
<p>The softmax-function outputs for for each neuron in the output-layer a value <span class="math notranslate nohighlight">\(y_k\)</span>, with the property, that</p>
<div class="math notranslate nohighlight">
\[\sum\limits_{k=1}^K y_k = 1.\]</div>
<p>Each of these outputs is an indicator for the
a-posteriori propability that the given observation belongs to class <span class="math notranslate nohighlight">\(C_i\)</span>:</p>
<div class="math notranslate nohighlight">
\[P(C_i|(x_{1},x_{2}, \ldots,x_{d})).\]</div>
<p>The class, whose neuron outputs the maximum value is the most likely class for the current observation at the input of the SLP.</p>
<p>In the case that the softmax-activation function is applied, the loss-function, which is minimized in the training procedure is the <strong>cross-entropy function</strong>:</p>
<div class="math notranslate nohighlight">
\[
E(T,\Theta)= - \sum\limits_{t=1}^N \sum\limits_{k=1}^K r_{t,k} \log(y_{t,k}),
\]</div>
<p>where <span class="math notranslate nohighlight">\(\Theta=\lbrace w_0,w_1,\ldots, w_d \rbrace\)</span> is the set of weights, which are adapted in the training process. <span class="math notranslate nohighlight">\(r_{t,k}=1\)</span>, if the <a class="reference external" href="http://t.th">t.th</a> training-element belongs to class <span class="math notranslate nohighlight">\(k\)</span>, otherwise it is 0. <span class="math notranslate nohighlight">\(y_{t,k}\)</span> is the output of the <a class="reference external" href="http://k.th">k.th</a> neuron for the <a class="reference external" href="http://t.th">t.th</a> training-element.</p>
<p>Each output neuron has its own set of weights, and each weight-set defines a (d-1)-dimensional hyperplane in the d-dimensional space. However, now these hyperplanes are not the class boundary itself, but they determine the class boundaries, which are actually of convex shape as depicted below. In the picture below, the red area indicates the inputs, who yield a maximum output at the neuron, whose weights belong to the red line, the blue area is the of inputs, whose maximum value is at the neuron, which belongs to the blue line and the green area comprises the inputs, whose maximum value is at the neuron, which belongs to the green line.</p>
<img src="https://maucher.home.hdm-stuttgart.de/Pics/slpMultiClassRegions.png" width="400" class="center"></section>
</section>
<section id="gradient-of-error-function">
<h2>Gradient of Error Function<a class="headerlink" href="#gradient-of-error-function" title="Permalink to this headline">#</a></h2>
<p>Above, the general idea of Gradient Descent Learning has been described. The corresponding weight-adaptation rule is:</p>
<div class="math notranslate nohighlight">
\[
W_{i,j}=W_{i,j}+\Delta W_{i,j} = W_{i,j}+\eta \cdot -\frac{\partial E}{\partial W_{i,j}},
\]</div>
<p>Now, we know which loss-function <span class="math notranslate nohighlight">\(E(T,\Theta)\)</span> is applied for the 3 different cases. Hence, we can calculate for each of these loss-functions the value of</p>
<div class="math notranslate nohighlight">
\[
\Delta W_{i,j} = \eta \cdot -\frac{\partial E}{\partial W_{i,j}},
\]</div>
<p>Even though different loss functions are applied, the gradient calculation of the loss-function yields the same weight-adaptation for all of the 3 mentioned cases:</p>
<div class="math notranslate nohighlight">
\[
\Delta W_{i,j}=\eta \sum\limits_{t=1}^N (r_{t,i}-y_{t,i}) \cdot x_{t,j},
\]</div>
<p>for all <span class="math notranslate nohighlight">\(i \in [1,K]\)</span> and <span class="math notranslate nohighlight">\(j \in [0,d]\)</span> (<span class="math notranslate nohighlight">\(K\)</span> is the number of neurons in the output-layer). The difference between target- and network output at the <a class="reference external" href="http://i.th">i.th</a> neuron of the <a class="reference external" href="http://t.th">t.th</a> training-element is also called the error-signal</p>
<div class="math notranslate nohighlight">
\[
\Delta_t= (r_{t,i}-y_{t,i}),
\]</div>
<p>and the error-vector comprises the error signals at all <span class="math notranslate nohighlight">\(K\)</span> output-neurons for the <a class="reference external" href="http://t.th">t.th</a> training-element:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{\Delta}_t = \left( \begin{array}{c}  r_{t,1}-y_{t,1} \\ r_{t,2}-y_{t,2} \\ \vdots \\  r_{t,K}-y_{t,K} \end{array} \right).
\end{split}\]</div>
<p>With this notation the simultaneous adaptation of all weights in <span class="math notranslate nohighlight">\(W\)</span> can be calculated as follows:</p>
<div class="math notranslate nohighlight">
\[
W=W+\Delta W,
\]</div>
<p>with</p>
<div class="math notranslate nohighlight">
\[
\Delta W= \eta \sum\limits_{t=1}^N \boldsymbol{\Delta}_t  \boldsymbol{x}_{t}^T.
\]</div>
<p>Note that <span class="math notranslate nohighlight">\(\boldsymbol{\Delta}_t\)</span> is a column vector and <span class="math notranslate nohighlight">\(\boldsymbol{x}_{t}^T\)</span> is a row-vector. Hence, the resulting matrix-product <span class="math notranslate nohighlight">\(\boldsymbol{\Delta}_t  \boldsymbol{x}_{t}^T\)</span> has as much rows as there are rows in <span class="math notranslate nohighlight">\(\boldsymbol{\Delta}_t\)</span> and as much columns as there are columns in <span class="math notranslate nohighlight">\(\boldsymbol{x}_{t}^T\)</span>.</p>
<p>This calculation of the weight-adaption from the error-vector at the output of the neural network is also called the <strong>Backward-Pass</strong>.</p>
<p>As can be seen in this weight-adaptation formula, for each single adaptation the entire batch of <span class="math notranslate nohighlight">\(N\)</span> training-elements is regarded (since the sum goes over all training-elements). This is called <strong>Batch-Learning</strong>. The flowchart of the described Gradient Descent Learning is depicted below. In the context of Neural Network training an <em>epoch</em> means that each element of the entire training-dataset has been shown used once for weight-adaptation. Usually, the entire training-process comprises many epochs. The termination of the training-process is either defined by a fixed number of epochs, or by a threshold on the decrease of the Loss Function. I.e. if the loss does not decrease significantly over several iterations, training terminates.</p>
<p><img alt="Gradient Descent Flowchart" src="https://maucher.home.hdm-stuttgart.de/Pics/gradientDescent.png" /></p>
<section id="stochastic-gradient-descent-learning">
<h3>Stochastic Gradient Descent Learning<a class="headerlink" href="#stochastic-gradient-descent-learning" title="Permalink to this headline">#</a></h3>
<p>A quite popular variant of the Gradient Descent Batch-Learning is to regard for a single weight-adaptation only a single training element. In this variant the weight adaptation with respect to the <a class="reference external" href="http://t.th">t.th</a> training-element is:</p>
<div class="math notranslate nohighlight">
\[
\Delta W= \eta  \boldsymbol{\Delta}_t  \boldsymbol{x}_{t}^T
\]</div>
<p>In contrast to Batch-Learning this variant is called <strong>Online-Learning</strong>. The true Gradient Descent approach would require Batch-Learning. Since the Online-Learning variant is not fully conform to the Gradient Descent approach it is called <strong>Stochastic Gradient Descent (SGD)</strong>.</p>
</section>
</section>
<section id="summary-single-layer-perceptron">
<h2>Summary Single Layer Perceptron<a class="headerlink" href="#summary-single-layer-perceptron" title="Permalink to this headline">#</a></h2>
<p><img alt="SLP summary" src="https://maucher.home.hdm-stuttgart.de/Pics/slpSummary.png" /></p>
</section>
<section id="multi-layer-perceptron">
<h2>Multi Layer Perceptron<a class="headerlink" href="#multi-layer-perceptron" title="Permalink to this headline">#</a></h2>
<section id="notations-and-basic-characteristics">
<h3>Notations and Basic Characteristics<a class="headerlink" href="#notations-and-basic-characteristics" title="Permalink to this headline">#</a></h3>
<p>A Multi Layer Perceptron (MLP) with <span class="math notranslate nohighlight">\(L\geq 2\)</span> layers is a Feedforward Neural Network (FNN), which consists of</p>
<ul class="simple">
<li><p>an input-layer (which is actually not counted as <em>layer</em>)</p></li>
<li><p>an output layer</p></li>
<li><p>a sequence of <span class="math notranslate nohighlight">\(L-1\)</span> hidden layers inbetween the input- and output-layer</p></li>
</ul>
<p>Usually the number of hidden layers is 1,2 or 3. All neurons of a layer are connected to all neurons of the successive layer. A layer with this property is also called a <strong>fully-connected layer</strong> or a <strong>dense layer</strong>.</p>
<p>An example of a <span class="math notranslate nohighlight">\(L=3\)</span> layer MLP is shown in the following picture.</p>
<img src="https://maucher.home.hdm-stuttgart.de/Pics/mlpL3.png" alt="Drawing" style="width: 600px;">
<p>As in the case of SLPs, the biases in MLP can be modelled implicitily by including to all non-output-layers a constant neuron <span class="math notranslate nohighlight">\(x_0=1\)</span>, or by the explicit bias-vector <span class="math notranslate nohighlight">\(\mathbf{b^l}\)</span> in layer <span class="math notranslate nohighlight">\(l\)</span>. In the picture above, the latter option is applied.</p>
<p>In order to provide a unified description the following notation is used:</p>
<ul class="simple">
<li><p>the number of neurons in layer <span class="math notranslate nohighlight">\(l\)</span> is denoted by <span class="math notranslate nohighlight">\(z_l\)</span>.</p></li>
<li><p>the output of the layer in depth <span class="math notranslate nohighlight">\(l\)</span> is denoted by the vector <span class="math notranslate nohighlight">\(\mathbf{h^l}=(h_1^l,h_2^l,\ldots,h_{z_l}^l)\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{x}=\mathbf{h^0}\)</span> is the input to the network,</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{y}=\mathbf{h^L}\)</span> is the network’s output,</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{b^l}\)</span> is the bias-vector of layer <span class="math notranslate nohighlight">\(l\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(W^l\)</span> is the weight-matrix of layer <span class="math notranslate nohighlight">\(l\)</span>. It’s entry <span class="math notranslate nohighlight">\(W_{ij}^l\)</span> is the weight from the <a class="reference external" href="http://j.th">j.th</a> neuron in layer <span class="math notranslate nohighlight">\(l-1\)</span> to the <a class="reference external" href="http://i.th">i.th</a> neuron in layer <span class="math notranslate nohighlight">\(l\)</span>. Hence, the weight-matrix <span class="math notranslate nohighlight">\(W^l\)</span> has <span class="math notranslate nohighlight">\(z_l\)</span> rows and <span class="math notranslate nohighlight">\(z_{l-1}\)</span> columns.</p></li>
</ul>
<p>With this notation the <strong>Forward-Pass</strong> of the MLP in the picture above can be calculated as follows:</p>
<p><strong>Output of first hidden-layer:</strong></p>
<div class="math notranslate nohighlight">
\[\begin{split}\left( \begin{array}{c} h_1^1 \\ h_2^1 \\ h_3^1 \\ h_4^1 \end{array} \right) = g\left( \left( \begin{array}{ccc} W_{11}^1 &amp; W_{12}^1 &amp; W_{13}^1 \\ W_{21}^1 &amp; W_{22}^1 &amp; W_{23}^1 \\ W_{31}^1 &amp; W_{32}^1 &amp; W_{33}^1 \\ W_{41}^1 &amp; W_{42}^1 &amp; W_{43}^1 \end{array} \right) \left( \begin{array}{c} x_1 \\ x_2 \\ x_3 \end{array} \right) + \left( \begin{array}{c} b_1^1 \\ b_2^1 \\ b_3^1 \\ b_4^1 \end{array} \right) \right)\end{split}\]</div>
<p><strong>Output of second hidden-layer:</strong></p>
<div class="math notranslate nohighlight">
\[\begin{split}\left( \begin{array}{c} h_1^2 \\ h_2^2 \\ h_3^2 \end{array} \right) = g\left( \left( \begin{array}{cccc} W_{11}^2 &amp; W_{12}^2 &amp; W_{13}^2 &amp; W_{14}^2\\ W_{21}^2 &amp; W_{22}^2 &amp; W_{23}^2 &amp; W_{24}^2\\ W_{31}^2 &amp; W_{32}^2 &amp; W_{33}^2 &amp; W_{34}^2 \end{array} \right) \left( \begin{array}{c} h^1_1 \\ h^1_2 \\ h^1_3 \\ h^1_4 \end{array} \right) + \left( \begin{array}{c} b_1^2 \\ b_2^2 \\ b_3^2 \end{array} \right) \right)\end{split}\]</div>
<p><strong>Output of the network:</strong></p>
<div class="math notranslate nohighlight">
\[\begin{split}y = \left( \begin{array}{c} h_1^3 \\ \end{array} \right) = g\left( \left( \begin{array}{ccc} W_{11}^3 &amp; W_{12}^3 &amp; W_{13}^3 \end{array} \right) \left( \begin{array}{c} h^2_1 \\ h^2_2 \\ h^2_3 \end{array} \right) + \left( \begin{array}{c} b_1^3 \end{array} \right) \right)\end{split}\]</div>
<p>As in the case of Single Layer Perceptrons the three categories</p>
<ul class="simple">
<li><p>regression,</p></li>
<li><p>binary classification</p></li>
<li><p><span class="math notranslate nohighlight">\(K\)</span>-ary classification</p></li>
</ul>
<p>are distinguished. The corresponding MLP output-layer is the same as in the case of a SLP.</p>
<p>In contrast to SLPs, MLPs are able to <strong>learn non-linear</strong> models. This difference is depicted below: The left hand side shows the linear classification-boundary, as learned by a SLP, whereas on the right-hand side the non-linear boundary, as learned by a MLP from the same training data, is plotted.</p>
<img src="https://maucher.home.hdm-stuttgart.de/Pics/nonlinearClassification.png" alt="Drawing" style="width: 800px;"/></section>
</section>
<section id="architecture">
<h2>Architecture<a class="headerlink" href="#architecture" title="Permalink to this headline">#</a></h2>
<section id="number-of-hidden-layers">
<h3>Number of hidden layers<a class="headerlink" href="#number-of-hidden-layers" title="Permalink to this headline">#</a></h3>
<p>In the design of MLPs, the number of required hidden layers and the number of neurons per hidden layer are crucial hyperparameters, which strongly influence the network’s performance. Appropriate values for these parameters strongly depend on the application and data at hand. They can not be calculated analytically, but have to be determined in corresponding evaluation- and optimization experiments.</p>
<p>In order to roughly determine ranges for a suitable number of hidden neurons, one should consider, that an increasing number of hidden neurons</p>
<ul class="simple">
<li><p>requires more training data to learn a robust model, since more parameters must be learned,</p></li>
<li><p>allows to learn more complex models,</p></li>
<li><p>increases the risk of overfitting.</p></li>
</ul>
</section>
<section id="activation-and-loss-functions">
<h3>Activation- and Loss-functions<a class="headerlink" href="#activation-and-loss-functions" title="Permalink to this headline">#</a></h3>
<section id="activation-functions-in-hidden-layers">
<h4>Activation functions in hidden layers<a class="headerlink" href="#activation-functions-in-hidden-layers" title="Permalink to this headline">#</a></h4>
<p>The type of activation function to be used in the hidden-layers of a MLP is an hyperparameter, which must be configured by the user, i.e. it is not determined by e.g. the application category. Typical activations for the hidden-layers are:</p>
<ul class="simple">
<li><p>sigmoid</p></li>
<li><p>tanh</p></li>
<li><p>relu</p></li>
<li><p>leaky relu</p></li>
</ul>
<p>Finding the best, or at least an appropriate, activation function for the application and data at hand requires empirical analysis.</p>
</section>
<section id="activation-functions-in-the-output-layer-and-loss-functions">
<h4>Activation functions in the output layer and loss functions<a class="headerlink" href="#activation-functions-in-the-output-layer-and-loss-functions" title="Permalink to this headline">#</a></h4>
<p>The configuration of the activation function in the output-layer and the loss function, which is minimized in the training-stage, depend on the application-category in the same way as in the <span class="xref myst">SLP</span>:</p>
<p><strong>Regression:</strong></p>
<ul class="simple">
<li><p>Number of neurons in the output-layer: 1</p></li>
<li><p>Activation function in the output-layer: identity</p></li>
<li><p>Loss Function: Sum of Squared Errors (SSE)</p></li>
</ul>
<p><strong>Binary Classification:</strong></p>
<ul class="simple">
<li><p>Number of neurons in the output-layer: 1</p></li>
<li><p>Activation function in the output-layer: sigmoid</p></li>
<li><p>Loss Function: binary Cross-Entropy</p></li>
</ul>
<p><strong><span class="math notranslate nohighlight">\(K\)</span>-ary Classification:</strong></p>
<ul class="simple">
<li><p>Number of neurons in the output-layer: K</p></li>
<li><p>Activation function in the output-layer: softmax</p></li>
<li><p>Loss Function: Cross-Entropy</p></li>
</ul>
</section>
</section>
</section>
<section id="id2">
<h2>Gradient Descent Learning<a class="headerlink" href="#id2" title="Permalink to this headline">#</a></h2>
<p>For training, MLPs apply the same approach as SLPs: Gradient Descent. The general consept of Gradient Descent learning is:</p>
<ol class="simple">
<li><p>Define a <strong>Loss Function</strong> <span class="math notranslate nohighlight">\(E(T,\Theta)\)</span>, which somehow measures the deviation between the current network <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> output and the target output <span class="math notranslate nohighlight">\(\mathbf{r}\)</span>.</p></li>
<li><p>Calculate the gradient of the Loss Function:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\begin{split}\nabla E(T,\Theta) = \left( \begin{array}{c}  \frac{\partial E}{\partial W^l_{1,0}} \\ \frac{\partial E}{\partial W_{1,1}^l} \\ \vdots \\  \frac{\partial E}{\partial W^l_{K,d+1}} \end{array} \right). 
\end{split}\]</div>
<ol class="simple">
<li><p>Adapt all parameters into the direction of the negative gradient. This weight adaptation guarantees that the Loss Function is iteratively minimized.:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
W^l_{i,j}=W^l_{i,j}+\Delta W^l_{i,j} = W^l_{i,j}+\eta \cdot -\frac{\partial E}{\partial W^l_{i,j}},
\]</div>
<p>where <span class="math notranslate nohighlight">\(\eta\)</span> is the important hyperparameter <strong>learning rate</strong>.</p>
<p>For the MLP, here we just present the <em>Backward Pass</em> weight adaptation-rule, resulting from the aforementioned Gradient Descent approach. The algorithm is denoted <strong>Backpropagation Algorithm</strong>.</p>
<p>The weight-matrix <span class="math notranslate nohighlight">\(W^l\)</span> in layer <span class="math notranslate nohighlight">\(l\)</span>, with <span class="math notranslate nohighlight">\(l \in \lbrace 1,\ldots,L\rbrace\)</span>, is adapted in each iteration by</p>
<div class="math notranslate nohighlight">
\[
W^l=W^l+ \Delta W^l,
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
\Delta W^l = \eta \sum\limits_{t=1}^N \boldsymbol{D}_t^l * (\mathbf{h}_t^{l-1})^T
\]</div>
<p>for Gradient Descent Batch-Learning, and</p>
<div class="math notranslate nohighlight">
\[
\Delta W^l = \eta \boldsymbol{D}_t^l * (\mathbf{h}_t^{l-1})^T
\]</div>
<p>for Stochastic Gradient Descent (SGD) Online-Learning. In this adaptation formulas</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{h}_t^{l-1}\)</span> is the output-vector at layer <span class="math notranslate nohighlight">\(l-1\)</span>, if the <a class="reference external" href="http://t.th">t.th</a> training-element <span class="math notranslate nohighlight">\(\mathbf{x}_t\)</span> is at the input of the MLP,</p></li>
<li><p>the matrix <span class="math notranslate nohighlight">\(\boldsymbol{D}_t^l\)</span> is calculated recursively as follows:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{array}{|c|c|}
		\hline
		layer \; l &amp; \boldsymbol{D}_t^l \\
		\hline
		L &amp; \boldsymbol{\Delta}_t \\
		L-1 &amp; \left( (W^{L})^T * \boldsymbol{\Delta}_t \right) \cdot g'(W^{L-1}\mathbf{h}_t^{L-2}) \\
		L-2 &amp; \left((W^{L-1})^T * \boldsymbol{D}_t^{L-1} \right) \cdot g'(W^{L-2}\mathbf{h}_t^{L-3}) \\
		\vdots &amp; \vdots \\
		l &amp; \left((W^{l+1})^T * \boldsymbol{D}_t^{l+1} \right) \cdot g'(W^{l}\mathbf{h}_t^{l-1}) \\
        \hline
\end{array}
\end{split}\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(*\)</span> denotes matrix-multiplication</p></li>
<li><p><span class="math notranslate nohighlight">\(\cdot\)</span> denotes elementwise-multiplication</p></li>
<li><p><span class="math notranslate nohighlight">\(g'()\)</span> is the first derivation of the activation function applied in layer <span class="math notranslate nohighlight">\(l\)</span>.</p></li>
<li><p>the error-vector <span class="math notranslate nohighlight">\(\Delta_t\)</span> is as defined in <span class="xref myst">notebook SLP</span>:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{\Delta}_t=\left( \begin{array}{c} \Delta_{t,1} \\ \Delta_{t,2} \\ \vdots \\ \Delta_{t,z_L} \end{array} \right) = \left( \begin{array}{c} r_{t,1} - h^L_{t,1} \\ r_{t,2} - h^L_{t,2} \\ \vdots \\ r_{t,z_L} - h^L_{t,z_L} \end{array} \right)
\end{split}\]</div>
<p>Note that in the calculation of <span class="math notranslate nohighlight">\(\boldsymbol{D}_t^l\)</span> depends on the weight-matrices <span class="math notranslate nohighlight">\(W^{l+1},\ldots W^{L}\)</span>. It is important that for this the old weight-matrices (before the update in the current iteration) are used.</p>
</section>
<section id="hyperparameters">
<h2>Hyperparameters<a class="headerlink" href="#hyperparameters" title="Permalink to this headline">#</a></h2>
<p>In the previous sections the following Neural Network hyperparameters, i.e. parameters, which must be configured and optimized by the user, have been mentioned:</p>
<ul class="simple">
<li><p><strong>Number of layers L</strong> in the network: For SLP this is fixed to <span class="math notranslate nohighlight">\(L=1\)</span>.</p></li>
<li><p><strong>Number of neurons per layer</strong>: For the input layer this number is given by the number of features and for the output-layer this number is given by the task (regression, binary classification or <span class="math notranslate nohighlight">\(K\)</span>-ary classification). Since in SLPs there are no other layers, the number of neurons per layer is actually fixed.</p></li>
<li><p><strong>Activation function g():</strong> For the 3 different tasks (regression, binary classification or <span class="math notranslate nohighlight">\(K\)</span>-ary classification), the most convenient activation functions in the neurons of the output-layer have been described above. However, others are possible.</p></li>
<li><p><strong>Learning Parameters:</strong></p>
<ul>
<li><p>Batch- or Online-Learning</p></li>
<li><p>The learning rate <span class="math notranslate nohighlight">\(\eta\)</span> is a very sensitive parameter, which must be selected carefully.</p></li>
<li><p>Learning termination criteria and maximum number of epochs</p></li>
<li><p>Actually, there are many variants of Gradient Descent and Stochastic Gradient Descent, e.g. Backpropagation, RMSprop, Adam, …</p></li>
</ul>
</li>
</ul>
<p>There are more crucial hyperparameters, which are shortly described here:</p>
<ul class="simple">
<li><p><strong>Learnrate Decay <span class="math notranslate nohighlight">\(\gamma\)</span>:</strong> If a learnrate decay of <span class="math notranslate nohighlight">\(\gamma \lt 1\)</span> is selected the learnrate <span class="math notranslate nohighlight">\(\eta\)</span> is decreased after each epoch as follows:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\eta^l = \gamma  \cdot  \eta^{l-1},
\]</div>
<p>where <span class="math notranslate nohighlight">\(l-1\)</span> and <span class="math notranslate nohighlight">\(l\)</span> indicate the previous and current epoch, respectively.</p>
<ul class="simple">
<li><p><strong>Momentum <span class="math notranslate nohighlight">\(\alpha\)</span>:</strong> In order to avoid a ping-ponging of weight adaptations the previous weight adaptation is integrated into the current weight-adaptations as follows:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ 
\Delta W_{i,j}^l=- \eta \cdot \frac{\partial E}{\partial W_{i,j}} + \alpha \Delta W_{i,j}^{l-1}.
\]</div>
<p>Here, the superscripts <span class="math notranslate nohighlight">\(l-1\)</span> and <span class="math notranslate nohighlight">\(l\)</span> indicate the previous and current weight adaptations, respectively.</p>
<ul class="simple">
<li><p><strong>Weight Decay <span class="math notranslate nohighlight">\(\beta\)</span>:</strong> Different loss functions <span class="math notranslate nohighlight">\(E(T,\Theta)\)</span> have been presented above. In the case of weight-decay, i.e. <span class="math notranslate nohighlight">\(\beta \gt 0\)</span>
an additional term</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\beta \cdot \frac{1}{2} \sum\limits_{W_{i,j} \in W} W_{i,j}^2
\]</div>
<p>is added to the loss function. Then for this extended loss function the gradient is calculated and weights are adapted into the direction of the negative gradient of the extended loss-function. The effect is, that then the error-signals and the weights are minimized simultaneously. Small weight values reduce the risk of overfitting. However, too small weights may yield too simple models (underfitting). The extend of weight-minimization can be controlled by the value of <span class="math notranslate nohighlight">\(\beta\)</span>. This process is also called <strong>Regularization</strong>. It is applied not only for Neural Networks, but in many other Machine Learning algorithms.</p>
<ul class="simple">
<li><p><strong>Dropout:</strong> Dropout is a technique to prevent Neural Networks, such as MLPs, CNNs, LSTMs from overfitting. The key idea is to randomly drop units along with their connections from the neural network during training. This prevents units from co-adapting too much. The drop of a defined ratio (0.1-0.5) of random neurons is valid only for one iteration. During this iteration the weights of the droped units are not adapted. In the next iteration another set of units, which are dropped temporarily is randomly selected. Dropout is only applied in the training-phase.<br />
<img alt="Dropout" src="https://maucher.home.hdm-stuttgart.de/Pics/dropout.PNG" /></p></li>
</ul>
</section>
<section id="early-mlp-example-autonomos-driving">
<h2>Early MLP Example: Autonomos Driving<a class="headerlink" href="#early-mlp-example-autonomos-driving" title="Permalink to this headline">#</a></h2>
<p>The ALVINN net is a MLP with one hidden layer. It has been designed and trained for <em>road following</em> in autonomous driving. The input has been provided by a simple <span class="math notranslate nohighlight">\(30 \times 32\)</span> greyscale camera. As shown in the picture below, the hidden layer contains only 4 neurons. In the output-layer each of the 30 neurons belongs to one “steering-wheel-direction”. The training data has been collected by recording videos while an expert driver steers the car. For each frame (input) the steering-wheel-direction (label) has been tracked.</p>
<img src="https://maucher.home.hdm-stuttgart.de/Pics/alvinnNN.jpg" width=450 class="center">
<p>After training the vehicle cruised autonomously for 90 miles on a highway at a speed of up to 70mph. The test-highway has not been included in the training cruises.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./neuralnetworks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="../machinelearning/GaussianProcessRegression.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Gaussian Process: Implementation in Python</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="02RecurrentNeuralNetworks.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Recurrent Neural Networks</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Prof. Dr. Johannes Maucher<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>