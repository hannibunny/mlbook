
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Representations for Words and Texts &#8212; Machine Learning Lecture</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.5f77b4aec8189eecf79907ce328c390d.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Text classification with CNNs and LSTMs" href="02TextClassification.html" />
    <link rel="prev" title="Example Q-Learning" href="../rl/QLearnFrozenLake.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      <img src="../_static/hdmlogomed.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Machine Learning Lecture</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Intro and Overview Machine Learning Lecture
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../00BasicConcepts.html">
   Basic Concepts of Data Mining and Machine Learning
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Conventional ML
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../machinelearning/knn.html">
   K - Nearest Neighbour Classification / Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machinelearning/parametricClassification1D.html">
   Bayes- and Naive Bayes Classifier
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machinelearning/LinReg.html">
   Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machinelearning/heartRateRegression.html">
   Example Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machinelearning/LinearClassification.html">
   Linear Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machinelearning/svm.html">
   Support Vector Machines (SVM)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machinelearning/gp.html">
   Gaussian Process
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../machinelearning/GaussianProcessRegression.html">
   Gaussian Process: Implementation in Python
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Neural Networks
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../neuralnetworks/01NeuralNets.html">
   Neural Networks Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../neuralnetworks/02RecurrentNeuralNetworks.html">
   Recurrent Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../neuralnetworks/03ConvolutionNeuralNetworks.html">
   Convolutional Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../neuralnetworks/convolutionDemos.html">
   Animations of Convolution and Deconvolution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../neuralnetworks/03KerasMLPandCNNcifar.html">
   Implementing Neural Networks with Keras
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../neuralnetworks/04KerasPretrainedClassifiers.html">
   Applying Pretrained Deep Neural Networks for Image Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../neuralnetworks/05KerasPretrainedCovid.html">
   Apply Pretrained Neural Networks on new Task
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Autoencoder
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../neuralnetworks/04VariationalAutoencoder.html">
   Variational Autoencoder (VAE) to generate handwritten digits
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  GAN
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../gan/GAN.html">
   Generative Adversarial Nets (GAN)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../gan/DCGAN.html">
   DCGAN Keras Implementation
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Reinforcement Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../rl/reinforcement.html">
   Reinforcement Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../rl/DQN.html">
   Deep Reinforcement Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../rl/QLearnFrozenLake.html">
   Example Q-Learning
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Text
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Representations for Words and Texts
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02TextClassification.html">
   Text classification with CNNs and LSTMs
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Transformer
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../transformer/attention.html">
   Sequence-To-Sequence, Attention, Transformer
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../transformer/intent_classification_with_bert.html">
   Intent Classification with BERT
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../referenceSection.html">
   References
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/text/01ModellingWordsAndTexts.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/text/01ModellingWordsAndTexts.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#one-hot-encoding-of-single-words">
   One-Hot-Encoding of Single Words
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#word-embeddings">
   Word Embeddings
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bag-of-word-modell-of-documents">
   Bag of Word Modell of documents
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#term-frequencies">
     Term Frequencies
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#term-frequency-inverse-document-frequency">
     Term Frequency Inverse Document Frequency
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#how-to-generate-wordembeddings-cbow-and-skipgram">
   How to generate Wordembeddings? CBOW and Skipgram
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#continous-bag-of-words-cbow">
     Continous Bag-Of-Words (CBOW)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#skip-gram">
     Skip-Gram
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#other-word-embeddings">
     Other Word-Embeddings
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#how-to-access-pretrained-word-embeddings">
   How to Access Pretrained Word-Embeddings?
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#fasttext-word-embeddings">
     Fasttext Word-Embeddings
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#glove-word-embeddings">
     Glove Word-Embeddings
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#comparision-bow-vs-sequence-of-word-embeddings">
   Comparision: BoW vs. Sequence of Word-Embeddings
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="representations-for-words-and-texts">
<h1>Representations for Words and Texts<a class="headerlink" href="#representations-for-words-and-texts" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><p>Author: Johannes Maucher</p></li>
<li><p>Last Update: 16.12.2020</p></li>
</ul>
<p>In previous sections different types of data, numeric and categorial, have been applied. It has been shown how categorical data is mapped to numeric values or numeric vectors, such that it can be applied as input of a Machine Learning algorithm.</p>
<p>Another type of data is text, either single words, sentences, sections or entire documents. How to map these types to numeric representations?</p>
<div class="section" id="one-hot-encoding-of-single-words">
<h2>One-Hot-Encoding of Single Words<a class="headerlink" href="#one-hot-encoding-of-single-words" title="Permalink to this headline">¶</a></h2>
<p>A very simple option for representing single words as numeric vectors is One-Hot-Encoding. This type of encoding has already been introduced above for modelling non-binary categorial features. Each possible value (word) is uniquely mapped to an index, and the associated vector contains only zeros, except at the position of the value’s (word’s) index.</p>
<p>For example, assume that the entire set of possible words is</p>
<div class="math notranslate nohighlight">
\[
V=(\mbox{all, and, at, boys, girls, home, kids, not, stay}).
\]</div>
<p>Then a possible One-Hot-Encoding of these words is then</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p></p></th>
<th class="head"><p></p></th>
<th class="head"><p></p></th>
<th class="head"><p></p></th>
<th class="head"><p></p></th>
<th class="head"><p></p></th>
<th class="head"><p></p></th>
<th class="head"><p></p></th>
<th class="head"><p></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>all</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>and</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>at</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>boys</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>girls</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>home</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>kids</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-odd"><td><p>not</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>stay</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
</tr>
</tbody>
</table>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</pre></div>
</div>
</div>
</div>
<p>A <strong>word-index</strong> is just a one-to-one mapping of words to integers. Usually the word-index defines the One-Hot-Encoding of words: If <span class="math notranslate nohighlight">\(i(w)\)</span> is the index of word <span class="math notranslate nohighlight">\(w\)</span>, then the One-Hot-Encoding of <span class="math notranslate nohighlight">\(v(w)\)</span> is a vector, which consists of only zeros, except at the element at position <span class="math notranslate nohighlight">\(i(w)\)</span>. The value at this position is 1.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">simpleWordDF</span><span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;all&quot;</span><span class="p">,</span> <span class="s2">&quot;and&quot;</span><span class="p">,</span> <span class="s2">&quot;at&quot;</span><span class="p">,</span> <span class="s2">&quot;boys&quot;</span><span class="p">,</span> <span class="s2">&quot;girls&quot;</span><span class="p">,</span> <span class="s2">&quot;home&quot;</span><span class="p">,</span> <span class="s2">&quot;kids&quot;</span><span class="p">,</span> <span class="s2">&quot;not&quot;</span><span class="p">,</span> <span class="s2">&quot;stay&quot;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Word Index:&quot;</span><span class="p">)</span>
<span class="n">simpleWordDF</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Word Index:
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>all</td>
    </tr>
    <tr>
      <th>1</th>
      <td>and</td>
    </tr>
    <tr>
      <th>2</th>
      <td>at</td>
    </tr>
    <tr>
      <th>3</th>
      <td>boys</td>
    </tr>
    <tr>
      <th>4</th>
      <td>girls</td>
    </tr>
    <tr>
      <th>5</th>
      <td>home</td>
    </tr>
    <tr>
      <th>6</th>
      <td>kids</td>
    </tr>
    <tr>
      <th>7</th>
      <td>not</td>
    </tr>
    <tr>
      <th>8</th>
      <td>stay</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Corresponding One-Hot-Encoding&quot;</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">simpleWordDF</span><span class="p">,</span><span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Corresponding One-Hot-Encoding
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>_all</th>
      <th>_and</th>
      <th>_at</th>
      <th>_boys</th>
      <th>_girls</th>
      <th>_home</th>
      <th>_kids</th>
      <th>_not</th>
      <th>_stay</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</div>
<div class="section" id="word-embeddings">
<h2>Word Embeddings<a class="headerlink" href="#word-embeddings" title="Permalink to this headline">¶</a></h2>
<p>One-Hot-Encoding of words suffer from the following drawbacks:</p>
<ol class="simple">
<li><p>The vectors are usually very long - there length is given by the number of words in the vocabulary.</p></li>
<li><p>The vectors are extremely sparse: only one non-zero entry.</p></li>
<li><p>Semantic relations between words are not modelled. This means that in this model there is no information about the fact that word <em>car</em> is more related to word <em>vehicle</em> than to word <em>lake</em>.</p></li>
</ol>
<p>All of these drawbacks can be solved by applying <em>Word Embeddings</em> and by the way the resulting <em>Word Embeddings</em> are passed to neural networks.</p>
<p>Word embeddings have revolutionalized many fields of Natural Language Processing since their efficient neural-network-based generation has been published in <a class="reference external" href="https://arxiv.org/pdf/1301.3781.pdf">Efficient Estimation of Word Representations in Vector Space</a> by Mikolov et al (2013). Word embeddings map words into vector-spaces such that semantically or syntactically related words are close together, whereas unrelated words are far from each other. Moreover, it has been shown that the word-embeddings, generated by <em>word2vec</em>-techniques <em>CBOW</em> or <em>Skipgram</em>, are well-structured in the sense that also relations such as <em>is-capital-of</em>, <em>is-female-of</em>, <em>is-plural-of</em> are encoded in the vector space. In this way questions like <em>woman is to queen, as man is to ?</em> can be answered by simple operations of linear algebra in the word-vector-space. Compared to the length of one-hot encoded word-vectors, word-embedding-vectors are short (typical lengths in the range from 100-300) and dense (float-values).</p>
<img src="https://maucher.home.hdm-stuttgart.de/Pics/dsm.png" alt="Drawing" style="width: 600px;"/>
<p><em>CBOW</em> and <em>Skipgram</em>, are techniques to learn word-embeddings, i.e. a mapping of words to vectors by relatively simple neural networks. Usually large corpora are applied for learning, e.g. the entire Wikipedia corpus in a given language. Today, pretrained models for the most common languages are available, for example from <a class="reference external" href="https://fasttext.cc/">FastText project</a>.</p>
</div>
<div class="section" id="bag-of-word-modell-of-documents">
<h2>Bag of Word Modell of documents<a class="headerlink" href="#bag-of-word-modell-of-documents" title="Permalink to this headline">¶</a></h2>
<div class="section" id="term-frequencies">
<h3>Term Frequencies<a class="headerlink" href="#term-frequencies" title="Permalink to this headline">¶</a></h3>
<p>The conventional model for representing texts of arbitrary length as numeric vectors, is the <strong>Bag-of-Words</strong> model.
In this model each word of the underlying vocabulary corresponds to one column and each document (text) corresponds to a single row of a matrix. The entry in row <span class="math notranslate nohighlight">\(i\)</span>, column <span class="math notranslate nohighlight">\(j\)</span> is just the term-frequency <span class="math notranslate nohighlight">\(tf_{i,j}\)</span> of word <span class="math notranslate nohighlight">\(j\)</span> in document <span class="math notranslate nohighlight">\(i\)</span>.</p>
<p>For example, assume, that we have only two documents</p>
<ul class="simple">
<li><p>Document 1: <em>not all kids stay at home</em></p></li>
<li><p>Document 2: <em>all boys and girls stay not at home</em></p></li>
</ul>
<p>The BoW model of these documents is then</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>all</p></th>
<th class="head"><p>and</p></th>
<th class="head"><p>at</p></th>
<th class="head"><p>boys</p></th>
<th class="head"><p>girls</p></th>
<th class="head"><p>home</p></th>
<th class="head"><p>kids</p></th>
<th class="head"><p>not</p></th>
<th class="head"><p>stay</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Document 1</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-odd"><td><p>Document 2</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
</tr>
</tbody>
</table>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>
<span class="n">vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;not all kids stay at home.&#39;</span><span class="p">,</span>
          <span class="s1">&#39;all boys and girls stay not at home.&#39;</span><span class="p">,</span>
         <span class="p">]</span>
<span class="n">BoW</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
<span class="n">vectorizer</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;all&#39;, &#39;and&#39;, &#39;at&#39;, &#39;boys&#39;, &#39;girls&#39;, &#39;home&#39;, &#39;kids&#39;, &#39;not&#39;, &#39;stay&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">BoW</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[1, 0, 1, 0, 0, 1, 1, 1, 1],
       [1, 1, 1, 1, 1, 1, 0, 1, 1]])
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="term-frequency-inverse-document-frequency">
<h3>Term Frequency Inverse Document Frequency<a class="headerlink" href="#term-frequency-inverse-document-frequency" title="Permalink to this headline">¶</a></h3>
<p>Instead of the term-frequency <span class="math notranslate nohighlight">\(tf_{i,j}\)</span> it is also possible to fill the BoW-vector with</p>
<ul class="simple">
<li><p>a binary indicator which indicates if the term <span class="math notranslate nohighlight">\(j\)</span> appears in document <span class="math notranslate nohighlight">\(i\)</span></p></li>
<li><p>the tf-idf-values</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
tfidf_{i,j}=tf_{i,j} \cdot log \frac{N}{df_j},
\]</div>
<p>where <span class="math notranslate nohighlight">\(df_j\)</span> is the frequency of documents, in which term <span class="math notranslate nohighlight">\(j\)</span> appears, and <span class="math notranslate nohighlight">\(N\)</span> is the total number of documents. The advantage of tf-idf, compared to just tf-entries, is that in <em>tf-idf</em> the term-frequency <em>tf</em> is multiplied by a value <em>idf</em>,  which is small for less informative words, i.e. words which appear in many documents, and high for words, which appear in only few documents. It is assumed, that words, which appear only in a few documents have a stronger <em>semantic focus</em> and are therefore more important.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>
<span class="n">vectorizer</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tfidf_BoW</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tfidf_BoW</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[0.37863221, 0.        , 0.37863221, 0.        , 0.        ,
        0.37863221, 0.53215436, 0.37863221, 0.37863221],
       [0.30253071, 0.42519636, 0.30253071, 0.42519636, 0.42519636,
        0.30253071, 0.        , 0.30253071, 0.30253071]])
</pre></div>
</div>
</div>
</div>
<p>As can be seen in the example above, words which appear in all documents are weighted by 0, i.e. they are considered to be not relevant.</p>
</div>
</div>
<div class="section" id="how-to-generate-wordembeddings-cbow-and-skipgram">
<h2>How to generate Wordembeddings? CBOW and Skipgram<a class="headerlink" href="#how-to-generate-wordembeddings-cbow-and-skipgram" title="Permalink to this headline">¶</a></h2>
<p>In 2013 Mikolov et al. published <a class="reference external" href="https://arxiv.org/pdf/1301.3781.pdf">Efficient Estimation of Word Representations in Vector Space</a>. They proposed quite simple neural network architectures to efficiently create word-embeddings: CBOW and Skipgram. These architectures are better known as <strong>Word2Vec</strong>. In both techniques neural networks are trained for a pseudo-task. After training, the network itself is usually not of interest. However, the learned weights in the input-layer constitute the word-embeddings, which can then be applied for a large field of NLP-tasks, e.g. document classification.</p>
<div class="section" id="continous-bag-of-words-cbow">
<h3>Continous Bag-Of-Words (CBOW)<a class="headerlink" href="#continous-bag-of-words-cbow" title="Permalink to this headline">¶</a></h3>
<p>The idea of CBOW is to predict the target word <span class="math notranslate nohighlight">\(w_i\)</span>, given the <span class="math notranslate nohighlight">\(N\)</span> context-words <span class="math notranslate nohighlight">\(w_{i-N/2},\ldots, w_{i-1}, \quad w_{i+1}, w_{i+N/2}\)</span>.
In order to learn such a predictor a large but unlabeled corpus is required. The extraction of training-samples from a corpus is sketched in the picture below:</p>
<img src="https://maucher.home.hdm-stuttgart.de/Pics/CBowTrainSamples.png" alt="Drawing" style="width: 600px;"/>
<p>In this example a context length of <span class="math notranslate nohighlight">\(N=4\)</span> has been applied. The first training-element consists of</p>
<ul class="simple">
<li><p>the <span class="math notranslate nohighlight">\(N=4\)</span> input-words <em>(happy,families,all,alike)</em></p></li>
<li><p>the target word <em>are</em>.</p></li>
</ul>
<p>In order to obtain the second training-sample the window of length <span class="math notranslate nohighlight">\(N+1\)</span> is just shifted by one to the right. The concrete architecture for CBOW is shown in the picture below. At the input the <span class="math notranslate nohighlight">\(N\)</span> context words are one-hot-encoded. The fully-connected <em>Projection-layer</em> maps the context words to a vector representation of the context. This vector representation is the input of a softmax-output-layer. The output-layer has as much neurons as there are words in the vocabulary <span class="math notranslate nohighlight">\(V\)</span>. Each neurons uniquely corresponds to a word of the vocabulary and outputs an estimation of the probaility, that the word appears as target for the current context-words at the input.</p>
<img src="https://maucher.home.hdm-stuttgart.de/Pics/cbowGramArchitecture.png" alt="Drawing" style="width: 600px;"/>
<p>After training the CBOW-network the vector representation of word <span class="math notranslate nohighlight">\(w\)</span> are the weights from the one-hot encoded word <span class="math notranslate nohighlight">\(w\)</span> at the input of the network to the neurons in the projection-layer. I.e. the number of neurons in the projection layer define the length of the word-embedding.</p>
</div>
<div class="section" id="skip-gram">
<h3>Skip-Gram<a class="headerlink" href="#skip-gram" title="Permalink to this headline">¶</a></h3>
<p>Skip-Gram is similar to CBOW, but has a reversed prediction process: For a given target word at the input, the Skip-Gram model predicts words, which are likely in the context of this target word. Again, the context is defined by the <span class="math notranslate nohighlight">\(N\)</span> neighbouring words. The extraction of training-samples from a corpus is sketched in the picture below:</p>
<img src="https://maucher.home.hdm-stuttgart.de/Pics/skipGramTrainSamples.png" alt="Drawing" style="width: 600px;"/>
<p>Again a context length of <span class="math notranslate nohighlight">\(N=4\)</span> has been applied. The first training-element consists of</p>
<ul class="simple">
<li><p>the first target word <em>(happy)</em> as input to the network</p></li>
<li><p>the first context word <em>(families)</em> as network-output.</p></li>
</ul>
<p>The concrete architecture for Skip-gram is shown in the picture below. At the input the target-word is one-hot-encoded. The fully-connected <em>Projection-layer</em> outputs the current vector representation of the target-word. This vector representation is the input of a softmax-output-layer. The output-layer has as much neurons as there are words in the vocabulary <span class="math notranslate nohighlight">\(V\)</span>. Each neurons uniquely corresponds to a word of the vocabulary and outputs an estimation of the probaility, that the word appears in the context of the current target-word at the input.</p>
<img src="https://maucher.home.hdm-stuttgart.de/Pics/skipGramArchitecture.png" alt="Drawing" style="width: 600px;"/></div>
<div class="section" id="other-word-embeddings">
<h3>Other Word-Embeddings<a class="headerlink" href="#other-word-embeddings" title="Permalink to this headline">¶</a></h3>
<p>CBOW- and Skip-Gram are possibly the most popular word-embeddings. However, there are more count-based and prediction-based methods to generate them, e.g. Random-Indexing, <a class="reference external" href="https://nlp.stanford.edu/projects/glove/">Glove</a>, <a class="reference external" href="https://fasttext.cc/">FastText</a>.</p>
</div>
</div>
<div class="section" id="how-to-access-pretrained-word-embeddings">
<h2>How to Access Pretrained Word-Embeddings?<a class="headerlink" href="#how-to-access-pretrained-word-embeddings" title="Permalink to this headline">¶</a></h2>
<div class="section" id="fasttext-word-embeddings">
<h3>Fasttext Word-Embeddings<a class="headerlink" href="#fasttext-word-embeddings" title="Permalink to this headline">¶</a></h3>
<p>After downloading word embeddings from <a class="reference external" href="https://fasttext.cc/">FastText</a> they can be imported as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="c1"># Creating the model</span>
<span class="c1">#en_model = KeyedVectors.load_word2vec_format(&#39;/Users/maucher/DataSets/Gensim/FastText/Gensim/FastText/wiki-news-300d-1M.vec&#39;)</span>
<span class="c1">#en_model = KeyedVectors.load_word2vec_format(r&#39;C:\Users\maucher\DataSets\Gensim\Data\Fasttext\wiki-news-300d-1M.vec\wiki-news-300d-1M.vec&#39;) #path on surface</span>
<span class="n">en_model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="s1">&#39;/Users/johannes/DataSets/Gensim/FastText/fasttextEnglish300.vec&#39;</span><span class="p">)</span>
<span class="c1"># Getting the tokens </span>
<span class="n">words</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">en_model</span><span class="o">.</span><span class="n">key_to_index</span><span class="p">:</span>
    <span class="n">words</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>

<span class="c1"># Printing out number of tokens available</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of Tokens: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)))</span>

<span class="c1"># Printing out the dimension of a word vector </span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Dimension of a word vector: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
    <span class="nb">len</span><span class="p">(</span><span class="n">en_model</span><span class="p">[</span><span class="n">words</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
<span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">words</span><span class="p">[</span><span class="mi">100</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;First 10 components of word-vector: </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span><span class="n">en_model</span><span class="p">[</span><span class="n">words</span><span class="p">[</span><span class="mi">100</span><span class="p">]][:</span><span class="mi">10</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>than
First 10 components of word-vector: 
 [ 0.1016 -0.1216 -0.0356  0.0096 -0.1015  0.1766 -0.0593  0.032   0.0892
 -0.0727]
</pre></div>
</div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">KeyedVectors</span></code>-class provides many interesting methods on word-embeddings. For example the <code class="docutils literal notranslate"><span class="pre">most_similar(w)</span></code>-methode returns the words, whose word-vectors match best with the word-vector of <code class="docutils literal notranslate"><span class="pre">w</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">en_model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s2">&quot;car&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(&#39;cars&#39;, 0.8045914769172668),
 (&#39;automobile&#39;, 0.7667388916015625),
 (&#39;vehicle&#39;, 0.7534858584403992),
 (&#39;Car&#39;, 0.7177953124046326),
 (&#39;truck&#39;, 0.6989946961402893),
 (&#39;SUV&#39;, 0.6896128058433533),
 (&#39;automobiles&#39;, 0.6783526539802551),
 (&#39;dealership&#39;, 0.6682883501052856),
 (&#39;garage&#39;, 0.6681075096130371),
 (&#39;driver&#39;, 0.6541329026222229)]
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="glove-word-embeddings">
<h3>Glove Word-Embeddings<a class="headerlink" href="#glove-word-embeddings" title="Permalink to this headline">¶</a></h3>
<p>After downloading word-embeddings from <a class="reference external" href="https://nlp.stanford.edu/projects/glove/">Glove</a>, they can be imported as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="c1">#GLOVE_DIR = &quot;./Data/glove.6B&quot;</span>
<span class="c1">#GLOVE_DIR =&quot;/Users/maucher/DataSets/glove.6B&quot;</span>
<span class="n">GLOVE_DIR</span> <span class="o">=</span> <span class="s1">&#39;/Users/johannes/DataSets/Gensim/glove/&#39;</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">gensim.test.utils</span> <span class="kn">import</span> <span class="n">datapath</span><span class="p">,</span> <span class="n">get_tmpfile</span>
<span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>
<span class="kn">from</span> <span class="nn">gensim.scripts.glove2word2vec</span> <span class="kn">import</span> <span class="n">glove2word2vec</span>

<span class="n">glove_file</span> <span class="o">=</span> <span class="n">datapath</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">GLOVE_DIR</span><span class="p">,</span> <span class="s1">&#39;glove.6B.100d.txt&#39;</span><span class="p">))</span>
<span class="n">tmp_file</span> <span class="o">=</span> <span class="n">get_tmpfile</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">GLOVE_DIR</span><span class="p">,</span> <span class="s1">&#39;test_word2vec.txt&#39;</span><span class="p">))</span>

<span class="n">_</span> <span class="o">=</span> <span class="n">glove2word2vec</span><span class="p">(</span><span class="n">glove_file</span><span class="p">,</span> <span class="n">tmp_file</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="n">tmp_file</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;ipython-input-32-b7985f974e25&gt;:8: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).
  _ = glove2word2vec(glove_file, tmp_file)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s2">&quot;car&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(&#39;vehicle&#39;, 0.8630837798118591),
 (&#39;truck&#39;, 0.8597878813743591),
 (&#39;cars&#39;, 0.837166965007782),
 (&#39;driver&#39;, 0.8185911178588867),
 (&#39;driving&#39;, 0.781263530254364),
 (&#39;motorcycle&#39;, 0.7553156614303589),
 (&#39;vehicles&#39;, 0.7462257146835327),
 (&#39;parked&#39;, 0.74594646692276),
 (&#39;bus&#39;, 0.737270712852478),
 (&#39;taxi&#39;, 0.7155269384384155)]
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="comparision-bow-vs-sequence-of-word-embeddings">
<h2>Comparision: BoW vs. Sequence of Word-Embeddings<a class="headerlink" href="#comparision-bow-vs-sequence-of-word-embeddings" title="Permalink to this headline">¶</a></h2>
<img src="https://maucher.home.hdm-stuttgart.de/Pics/bowVsEmbedding.png" alt="Drawing" style="width: 600px;"/>
<p>BoW representations of text suffer from crucial drawbacks:</p>
<ol class="simple">
<li><p>The vectors are usually very long - there length is given by the number of words in the vocabulary. Moreover, the vectors are quite sparse, since the set of words appearing in one document is usually only a very small part of the set of all words in the vocabulary.</p></li>
<li><p>Semantic relations between words are not modelled. This means that in this model there is no information about the fact that word <em>car</em> is more related to word <em>vehicle</em> than to word <em>lake</em>.</p></li>
<li><p>In the BoW-model of documents word order is totally ignored. E.g. the model can not distinguish if word <em>not</em> appeared immediately before word <em>good</em> or before word <em>bad</em>.</p></li>
</ol>
<p>All of these drawbacks can be solved by</p>
<ul class="simple">
<li><p>applying <em>Word Embeddings</em></p></li>
<li><p>passing sequences of Word-Embedding-Vectors to the input of</p>
<ul>
<li><p>Recurrent Neural Networks,</p></li>
<li><p>Convolutional Neural Networks</p></li>
<li><p>Transformers</p></li>
</ul>
</li>
</ul>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./text"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="../rl/QLearnFrozenLake.html" title="previous page">Example Q-Learning</a>
    <a class='right-next' id="next-link" href="02TextClassification.html" title="next page">Text classification with CNNs and LSTMs</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Prof. Dr. Johannes Maucher<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>