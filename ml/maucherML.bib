Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@techreport{Choi,
abstract = {Figure 1. Multi-domain image-to-image translation results on the CelebA dataset via transferring knowledge learned from the RaFD dataset. The first and sixth columns show input images while the remaining columns are images generated by StarGAN. Note that the images are generated by a single generator network, and facial expression labels such as angry, happy, and fearful are from RaFD, not CelebA. Abstract Recent studies have shown remarkable success in image-to-image translation for two domains. However, existing approaches have limited scalability and robustness in handling more than two domains, since different models should be built independently for every pair of image domains. To address this limitation, we propose StarGAN, a novel and scalable approach that can perform image-to-image translations for multiple domains using only a single model. Such a unified model architecture of StarGAN allows simultaneous training of multiple datasets with different domains within a single network. This leads to StarGAN's superior quality of translated images compared to existing models as well as the novel capability of flexibly translating an input image to any desired target domain. We empirically demonstrate the effectiveness of our approach on a facial attribute transfer and a facial expression synthesis tasks.},
archivePrefix = {arXiv},
arxivId = {1711.09020v3},
author = {Choi, Yunjey and Choi, Minje and Kim, Munyoung and Ha, Jung-Woo and Kim, Sunghun and Choo, Jaegul},
eprint = {1711.09020v3},
file = {:Users/maucher/Library/Application Support/Mendeley Desktop/Downloaded/Choi et al. - Unknown - StarGAN Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation.pdf:pdf},
title = {{StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation}}
}
@article{LSTM1,
address = {Cambridge, MA, USA},
author = {Hochreiter, Sepp and Schmidhuber, J{\"{u}}rgen},
doi = {10.1162/neco.1997.9.8.1735},
issn = {0899-7667},
journal = {Neural Comput.},
month = {nov},
number = {8},
pages = {1735--1780},
publisher = {MIT Press},
title = {{Long Short-Term Memory}},
url = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
volume = {9},
year = {1997}
}
@article{Serre07,
author = {Serre, T and Kreiman, G and Kouh, M and Cadieu, C and Knoblich, U and Poggio, T},
doi = {10.1016/S0079-6123(06)65004-8},
issn = {0079-6123},
journal = {Progress in brain research},
keywords = {model,vision},
pages = {33--56},
pmid = {17925239},
title = {{A quantitative theory of immediate visual recognition.}},
url = {http://dx.doi.org/10.1016/S0079-6123(06)65004-8},
volume = {165},
year = {2007}
}
@techreport{Yang,
abstract = {With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking. 1 .},
archivePrefix = {arXiv},
arxivId = {1906.08237v2},
author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V},
eprint = {1906.08237v2},
file = {:Users/maucher/Library/Application Support/Mendeley Desktop/Downloaded/Yang et al. - Unknown - XLNet Generalized Autoregressive Pretraining for Language Understanding.pdf:pdf},
title = {{XLNet: Generalized Autoregressive Pretraining for Language Understanding}},
url = {https://github.com/zihangdai/xlnet}
}

@article{cortes95,
added-at = {2015-09-17T16:32:09.000+0200},
author = {Cortes, C. and Vapnik, V.},
biburl = {https://www.bibsonomy.org/bibtex/22b1eb8bea07ae0156a53a4e9c6eac1df/nosebrain},
interhash = {c223c465141618ad63aac5a6132280f7},
intrahash = {2b1eb8bea07ae0156a53a4e9c6eac1df},
journal = {Machine Learning},
keywords = {classification margin soft support svm vector},
pages = {273-297},
timestamp = {2015-09-17T17:15:55.000+0200},
title = {Support Vector Networks},
volume = 20,
year = 1995
}


@inproceedings{Bahdanau2015a,
abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder–decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder–decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
archivePrefix = {arXiv},
arxivId = {1409.0473},
author = {Bahdanau, Dzmitry and Cho, Kyung Hyun and Bengio, Yoshua},
booktitle = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
eprint = {1409.0473},
publisher = {International Conference on Learning Representations, ICLR},
title = {{Neural machine translation by jointly learning to align and translate}},
year = {2015}
}
@inproceedings{Rai07,
author = {Raina, Rajat and Battle, Alexis and Lee, Honglak and Packer, Benjamin and Ng, Andrew Y},
booktitle = {ICML '07: Proceedings of the 24th international conference on Machine learning},
title = {{Self-taught Learning: Transfer Learning from Unlabeled Data}},
year = {2007}
}
@article{Bengio10,
abstract = {In this paper, we propose a novel neural network model called RNN Encoder- Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
annote = {From Duplicate 4 (Auto-association by multilayer perceptrons and singular value decomposition - Bourlard, H; Kamp, Y)

10.1007/BF00332918},
archivePrefix = {arXiv},
arxivId = {1406.1078},
author = {Roux, Nicolas Le and Bengio, Yoshua and Hinton, Geoffrey E and Osindero, Simon and Bengio, Yoshua and Delalleau, Olivier and {Le Roux}, Nicolas and Lee, Honglak and Hinton, Geoffrey E and Bishop, Christopher M and Lee, Honglak and Ekanadham, Chaitanya and Ng, Andrew{\~{}}Y. and Hinton, Geoffrey E and Salakhutdinov, R R and Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine and Roux, Nicolas Le and Bengio, Yoshua and LeCun, Yann and Chopra, Sumit and Hadsell, Raia and Ranzato, Marc'Aurelio and Huang, Fu-Jie and Cortes, Corinna and Bourlard, H and Kamp, Y and Andrieu, Christophe and Hinton, Geoffrey E and Dayan, Peter and Frey, Brendan J and Neal, Radford M and Cho, Kyunghyun and {Van Merri{\"{e}}nboer}, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
doi = {10.1126/science.1127647},
edition = {1st ed. 20},
editor = {Bakir, G and Hofman, T and Sch{\"{o}}lkopf, B and Smola, A and Taskar, B},
eprint = {1406.1078},
howpublished = {$\backslash$url{\{}http://yann.lecun.com/exdb/mnist/{\}}},
isbn = {0387310738},
issn = {0340-1200},
journal = {Neural Computation},
keywords = {book,dimensionalityreduction neuralnetworks parameteres,imported,machine{\_}learning,pattern{\_}classification},
month = {oct},
number = {8},
pages = {2192--2207},
pmid = {16873662},
publisher = {Springer},
title = {{Pattern Recognition and Machine Learning (Information Science and Statistics)}},
url = {http://dx.doi.org/10.1007/BF00332918 http://www.cs.toronto.edu/{~}hinton/nipstutorial/nipstut3.pdf http://www.ncbi.nlm.nih.gov/sites/entrez?db=pubmed{\&}uid=16873662{\&}cmd=showdetailview{\&}indexed=google http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-2},
volume = {11},
year = {2006}
}
@inproceedings{LiuTrans,
author = {Liu, Shujie and Yang, Nan and Li, Mu and Zhou, Ming},
booktitle = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, {\{}ACL{\}} 2014, June 22-27, 2014, Baltimore, MD, USA, Volume 1: Long Papers},
pages = {1491--1500},
title = {{A Recursive Recurrent Neural Network for Statistical Machine Translation}},
url = {http://aclweb.org/anthology/P/P14/P14-1140.pdf},
year = {2014}
}
@techreport{Dumoulin2018,
archivePrefix = {arXiv},
arxivId = {1603.07285v2},
author = {Dumoulin, Vincent and Visin, Francesco and Box, George E P},
eprint = {1603.07285v2},
file = {:Users/maucher/Library/Application Support/Mendeley Desktop/Downloaded/Dumoulin, Visin, Box - 2018 - A guide to convolution arithmetic for deep learning.pdf:pdf},
title = {{A guide to convolution arithmetic for deep learning}},
url = {http://ethanschoonover.com/solarized},
year = {2018}
}
@inproceedings{karpathy,
author = {Karpathy, Andrej and Li, Fei-Fei},
booktitle = {{\{}IEEE{\}} Conference on Computer Vision and Pattern Recognition, {\{}CVPR{\}} 2015, Boston, MA, USA, June 7-12, 2015},
doi = {10.1109/CVPR.2015.7298932},
pages = {3128--3137},
title = {{Deep visual-semantic alignments for generating image descriptions}},
url = {http://dx.doi.org/10.1109/CVPR.2015.7298932},
year = {2015}
}
@inproceedings{Cho2014,
abstract = {In this paper, we propose a novel neural network model called RNN Encoder- Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
archivePrefix = {arXiv},
arxivId = {1406.1078},
author = {Cho, Kyunghyun and {Van Merri{\"{e}}nboer}, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
booktitle = {EMNLP 2014 - 2014 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference},
doi = {10.3115/v1/d14-1179},
eprint = {1406.1078},
isbn = {9781937284961},
pages = {1724--1734},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Learning phrase representations using RNN encoder-decoder for statistical machine translation}},
year = {2014}
}
@techreport{Goodfellow,
abstract = {We propose a new framework for estimating generative models via an adversar-ial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1 2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
archivePrefix = {arXiv},
arxivId = {1406.2661v1},
author = {Goodfellow, Ian J and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
eprint = {1406.2661v1},
file = {:Users/maucher/Library/Application Support/Mendeley Desktop/Downloaded/Goodfellow et al. - Unknown - Generative Adversarial Nets.pdf:pdf},
title = {{Generative Adversarial Nets}},
url = {http://www.github.com/goodfeli/adversarial}
}
@inproceedings{Vaswani2017,
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
booktitle = {Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, {\{}USA{\}}},
editor = {Guyon, Isabelle and von Luxburg, Ulrike and Bengio, Samy and Wallach, Hanna M and Fergus, Rob and Vishwanathan, S V N and Garnett, Roman},
pages = {6000--6010},
title = {{Attention is All you Need}},
url = {http://papers.nips.cc/paper/7181-attention-is-all-you-need},
year = {2017}
}
@book{Alpay10,
author = {Alpaydin, E},
edition = {2},
isbn = {978-0-262-01243-0},
keywords = {v1205 zzz.vhb book ai learn algorithm},
publisher = {MIT Press},
title = {{Introduction to Machine Learning}},
year = {2010}
}
@inproceedings{Sutskever2014,
abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
archivePrefix = {arXiv},
arxivId = {1409.3215},
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1409.3215},
issn = {10495258},
number = {January},
pages = {3104--3112},
publisher = {Neural information processing systems foundation},
title = {{Sequence to sequence learning with neural networks}},
volume = {4},
year = {2014}
}
@article{maucherDBNbosch,
author = {Maucher, Johannes},
journal = {Technical Report for Robert Bosch GmbH, CR/AEY3},
title = {{Deep Neural Networks: Architectures and Learning}},
year = {2012}
}
@techreport{Isola,
archivePrefix = {arXiv},
arxivId = {1611.07004v3},
author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A and Research, Berkeley Ai},
eprint = {1611.07004v3},
file = {:Users/maucher/Library/Application Support/Mendeley Desktop/Downloaded/Isola et al. - Unknown - Image-to-Image Translation with Conditional Adversarial Networks.pdf:pdf},
title = {{Image-to-Image Translation with Conditional Adversarial Networks}},
url = {https://github.com/phillipi/pix2pix.}
}
@inproceedings{graves,
author = {Graves, Alex and Mohamed, Abdel-rahman and Hinton, Geoffrey E},
booktitle = {{\{}IEEE{\}} International Conference on Acoustics, Speech and Signal Processing, {\{}ICASSP{\}} 2013, Vancouver, BC, Canada, May 26-31, 2013},
doi = {10.1109/ICASSP.2013.6638947},
pages = {6645--6649},
title = {{Speech recognition with deep recurrent neural networks}},
url = {http://dx.doi.org/10.1109/ICASSP.2013.6638947},
year = {2013}
}
@techreport{Jin,
abstract = {Automatic generation of facial images has been well studied after the Generative Adversarial Network(GAN) came out. There exists some attempts applying the GAN model to the problem of generating facial images of anime characters, but none of the existing work gives a promising result. In this work, we explore the training of GAN models specialized on an anime facial image dataset. We address the issue from both the data and the model aspect, by collecting a more clean, well-suited dataset and leverage proper, empirical application of DRAGAN. With quantitative analysis and case studies we demonstrate that our efforts lead to a stable and high-quality model. Moreover, to assist people with anime character design, we build a website 1 with our pre-trained model available online, which makes the model easily accessible to general public.},
archivePrefix = {arXiv},
arxivId = {1708.05509v1},
author = {Jin, Yanghua and Zhang, Jiakai and Li, Minjun and Tian, Yingtao and Zhu, Huachun and Fang, Zhihao},
eprint = {1708.05509v1},
file = {:Users/maucher/Library/Application Support/Mendeley Desktop/Downloaded/Jin et al. - Unknown - Towards the Automatic Anime Characters Creation with Generative Adversarial Networks.pdf:pdf},
title = {{Towards the Automatic Anime Characters Creation with Generative Adversarial Networks}},
url = {http://make.girls.moe}
}
@inproceedings{Bengio07greedy,
author = {Bengio, Yoshua and Lamblin, Pascal and Popovici, Dan and Larochelle, Hugo and Montr�al, Universit� De and Qu�bec, Montr�al},
booktitle = {In NIPS},
publisher = {MIT Press},
title = {{Greedy layer-wise training of deep networks}},
year = {2007}
}
@article{Wang2018,
archivePrefix = {arXiv},
arxivId = {1804.07461},
author = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
eprint = {1804.07461},
journal = {CoRR},
title = {{{\{}GLUE:{\}} {\{}A{\}} Multi-Task Benchmark and Analysis Platform for Natural Language Understanding}},
url = {http://arxiv.org/abs/1804.07461},
volume = {abs/1804.0},
year = {2018}
}
@article{Arel10,
address = {Piscataway, NJ, USA},
author = {Arel, Itamar and Rose, Derek C and Karnowski, Thomas P},
doi = {http://dx.doi.org/10.1109/MCI.2010.938364},
issn = {1556-603X},
journal = {Comp. Intell. Mag.},
month = {nov},
number = {4},
pages = {13--18},
publisher = {IEEE Press},
title = {{Research frontier: deep machine learning--a new frontier in artificial intelligence research}},
url = {http://dx.doi.org/10.1109/MCI.2010.938364},
volume = {5},
year = {2010}
}
@article{Dai2019,
abstract = {Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80{\%} longer than RNNs and 450{\%} longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.},
archivePrefix = {arXiv},
arxivId = {1901.02860},
author = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V. and Salakhutdinov, Ruslan},
eprint = {1901.02860},
file = {:Users/maucher/Library/Application Support/Mendeley Desktop/Downloaded/Dai et al. - 2019 - Transformer-XL Attentive Language Models Beyond a Fixed-Length Context.pdf:pdf},
month = {jan},
pages = {2978--2988},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context}},
url = {http://arxiv.org/abs/1901.02860},
year = {2019}
}
@article{Ledig2016,
abstract = {Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.},
archivePrefix = {arXiv},
arxivId = {1609.04802},
author = {Ledig, Christian and Theis, Lucas and Huszar, Ferenc and Caballero, Jose and Cunningham, Andrew and Acosta, Alejandro and Aitken, Andrew and Tejani, Alykhan and Totz, Johannes and Wang, Zehan and Shi, Wenzhe},
eprint = {1609.04802},
file = {:Users/maucher/Library/Application Support/Mendeley Desktop/Downloaded/Ledig et al. - 2016 - Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network.pdf:pdf},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
month = {sep},
pages = {105--114},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network}},
url = {http://arxiv.org/abs/1609.04802},
volume = {2017-Janua},
year = {2016}
}
@inproceedings{Ran06,
author = {Ranzato, Marc'Aurelio and Poultney, Christopher S and Chopra, Sumit and LeCun, Yann},
booktitle = {NIPS'06},
pages = {1137--1144},
title = {{Efficient Learning of Sparse Representations with an Energy-Based Model.}},
year = {2006}
}
@inproceedings{Bengio07,
author = {Bengio, Y and Lamblin, P and Popovici, P and Larochelle, H},
booktitle = {Advances in Neural Information Processing Systems},
keywords = {imported},
publisher = {MIT Press, Cambridge, MA.},
title = {{Greedy Layer-Wise Training of Deep Networks}},
volume = {19},
year = {2007}
}
@techreport{Radforda,
abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension , and summarization, are typically approached with supervised learning on task-specific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset-matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
file = {:Users/maucher/Library/Application Support/Mendeley Desktop/Downloaded/Radford et al. - Unknown - Language Models are Unsupervised Multitask Learners.pdf:pdf},
title = {{Language Models are Unsupervised Multitask Learners}},
url = {https://github.com/codelucas/newspaper}
}
@inproceedings{Heck98,
address = {Norwell, MA, USA},
author = {Heckerman, David},
booktitle = {Proceedings of the NATO Advanced Study Institute on Learning in graphical models},
pages = {301--354},
publisher = {Kluwer Academic Publishers},
title = {{A tutorial on learning with Bayesian networks}},
url = {http://dl.acm.org/citation.cfm?id=299068.299081},
year = {1998}
}
@techreport{Zhu,
abstract = {Monet photo photo Monet Figure 1: Given any two unordered image collections X and Y , our algorithm learns to automatically "translate" an image from one into the other and vice versa: (left) Monet paintings and landscape photos from Flickr; (center) zebras and horses from ImageNet; (right) summer and winter Yosemite photos from Flickr. Example application (bottom): using a collection of paintings of famous artists, our method learns to render natural photographs into the respective styles. Abstract Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G : X → Y such that the distribution of images from G(X) is indistinguishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping F : Y → X and introduce a cycle consistency loss to enforce F (G(X)) ≈ X (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
archivePrefix = {arXiv},
arxivId = {1703.10593v6},
author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A and Research, Berkeley Ai},
eprint = {1703.10593v6},
file = {:Users/maucher/Library/Application Support/Mendeley Desktop/Downloaded/Zhu et al. - Unknown - Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks Monet Photos.pdf:pdf},
title = {{Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks Monet Photos}}
}
@inbook{BenGal,
booktitle = {Encyclopedia of Statistics in Quality and Reliability},
doi = {10.1002/9780470061572.eqr089},
isbn = {9780470061572},
keywords = {Markov models,causality,context,probabilistic graphical models,reasoning},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{Bayesian Networks}},
url = {http://dx.doi.org/10.1002/9780470061572.eqr089},
year = {2008}
}
@article{Mao2016,
abstract = {Unsupervised learning with generative adversarial networks (GANs) has proven hugely successful. Regular GANs hypothesize the discriminator as a classifier with the sigmoid cross entropy loss function. However, we found that this loss function may lead to the vanishing gradients problem during the learning process. To overcome such a problem, we propose in this paper the Least Squares Generative Adversarial Networks (LSGANs) which adopt the least squares loss function for the discriminator. We show that minimizing the objective function of LSGAN yields minimizing the Pearson {\$}\backslashchi{\^{}}2{\$} divergence. There are two benefits of LSGANs over regular GANs. First, LSGANs are able to generate higher quality images than regular GANs. Second, LSGANs perform more stable during the learning process. We evaluate LSGANs on five scene datasets and the experimental results show that the images generated by LSGANs are of better quality than the ones generated by regular GANs. We also conduct two comparison experiments between LSGANs and regular GANs to illustrate the stability of LSGANs.},
archivePrefix = {arXiv},
arxivId = {1611.04076},
author = {Mao, Xudong and Li, Qing and Xie, Haoran and Lau, Raymond Y. K. and Wang, Zhen and Smolley, Stephen Paul},
eprint = {1611.04076},
file = {:Users/maucher/Library/Application Support/Mendeley Desktop/Downloaded/Mao et al. - 2016 - Least Squares Generative Adversarial Networks.pdf:pdf},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
month = {nov},
pages = {2813--2821},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Least Squares Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1611.04076},
volume = {2017-Octob},
year = {2016}
}
@inproceedings{LeCunn01,
author = {LeCun, Y and Bottou, L and Bengio, Y and Haffner, P},
booktitle = {Intelligent Signal Processing},
pages = {306--351},
publisher = {IEEE Press},
title = {{Gradient-Based Learning Applied to Document Recognition}},
year = {2001}
}
@inproceedings{LeCun07,
annote = {(keynote address)},
author = {LeCun, Yann and Chopra, Sumit and Ranzato, Marc'Aurelio and Huang, Fu-Jie},
booktitle = {Proc. International Conference on Document Analysis and Recognition (ICDAR)},
title = {{Energy-Based Models in Document Recognition and Computer Vision}},
year = {2007}
}
@misc{Lee02,
author = {Lee, Tai Sing and Mumford, David},
title = {{Hierarchical Bayesian Inference in the Visual Cortex}},
year = {2002}
}
@book{cowell99,
address = {New York},
author = {Cowell, Robert G and Dawid, A Philip and Lauritzen, Steffen L and Spiegelhalter, David J},
isbn = {0-387-98767-3},
keywords = {Messagepassing},
pages = {xii+321},
publisher = {Springer-Verlag},
series = {Statistics for Engineering and Information Science},
title = {{Probabilistic networks and expert systems}},
year = {1999}
}
@article{MnihKSGAWR13,
archivePrefix = {arXiv},
arxivId = {1312.5602},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin A},
eprint = {1312.5602},
journal = {CoRR},
title = {{Playing Atari with Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1312.5602},
volume = {abs/1312.5},
year = {2013}
}
@incollection{Sche10,
author = {Scherer, Dominik and M�ller, Andreas and Behnke, Sven},
booktitle = {Artificial Neural Networks - ICANN 2010},
editor = {Diamantaras, Konstantinos and Duch, Wlodek and Iliadis, Lazaros},
isbn = {978-3-642-15824-7},
pages = {92--101},
publisher = {Springer Berlin / Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Evaluation of Pooling Operations in Convolutional Architectures for Object Recognition}},
volume = {6354},
year = {2010}
}
@article{Bengio09,
author = {Bengio, Yoshua},
journal = {Foundations and Trends in Machine Learning},
number = {1},
pages = {1--127},
title = {{Learning Deep Architectures for AI}},
volume = {2},
year = {2009}
}
@techreport{Raffel2016,
abstract = {We propose a simplified model of attention which is applicable to feed-forward neural networks and demonstrate that the resulting model can solve the synthetic "addition" and "multiplication" long-term memory problems for sequence lengths which are both longer and more widely varying than the best published results for these tasks. 1 MODELS FOR SEQUENTIAL DATA Many problems in machine learning are best formulated using sequential data and appropriate models for these tasks must be able to capture temporal dependencies in sequences, potentially of arbitrary length. One such class of models are recurrent neural networks (RNNs), which can be considered a learnable function f whose output h t = f (x t , h t−1) at time t depends on input x t and the model's previous state h t−1. Training of RNNs with backpropagation through time (Werbos, 1990) is hindered by the vanishing and exploding gradient problem (Pascanu et al., 2012; Hochreiter {\&} Schmidhuber, 1997; Bengio et al., 1994), and as a result RNNs are in practice typically only applied in tasks where sequential dependencies span at most hundreds of time steps. Very long sequences can also make training computationally inefficient due to the fact that RNNs must be evaluated sequentially and cannot be fully parallelized. 1.1 ATTENTION A recently proposed method for easier modeling of long-term dependencies is "attention". Attention mechanisms allow for a more direct dependence between the state of the model at different points in time. Following the definition from (Bahdanau et al., 2014), given a model which produces a hidden state h t at each time step, attention-based models compute a "context" vector c t as the weighted mean of the state sequence h by c t = T j=1 $\alpha$ tj h j where T is the total number of time steps in the input sequence and $\alpha$ tj is a weight computed at each time step t for each state h j. These context vectors are then used to compute a new state sequence s, where s t depends on s t−1 , c t and the model's output at t − 1. The weightings $\alpha$ tj are then computed by e tj = a(s t−1 , h j), $\alpha$ tj = exp(e tj) T k=1 exp(e tk) where a is a learned function which can be thought of as computing a scalar importance value for h j given the value of h j and the previous state s t−1. This formulation allows the new state sequence s to have more direct access to the entire state sequence h. Attention-based RNNs have proven effective in a variety of sequence transduction tasks, including machine translation (Bahdanau et al., 2014), image captioning (Xu et al., 2015), and speech recognition (Chan et al., 2015; Bahdanau et al., 2015). Attention can be seen as analogous to the "soft addressing" mechanisms of the recently proposed Neural Turing Machine (Graves et al., 2014) and End-To-End Memory Network (Sukhbaatar et al., 2015) models.},
archivePrefix = {arXiv},
arxivId = {1512.08756v5},
author = {Raffel, Colin and Ellis, Daniel P W},
eprint = {1512.08756v5},
file = {:Users/maucher/Library/Application Support/Mendeley Desktop/Downloaded/Raffel, Ellis - 2016 - Workshop track-ICLR 2016 FEED-FORWARD NETWORKS WITH ATTENTION CAN SOLVE SOME LONG-TERM MEMORY PROBLEMS.pdf:pdf},
title = {{Workshop track-ICLR 2016 FEED-FORWARD NETWORKS WITH ATTENTION CAN SOLVE SOME LONG-TERM MEMORY PROBLEMS}},
year = {2016}
}
@inproceedings{MikolovRNN,
author = {Mikolov, Tomas and Karafi{\'{a}}t, Martin and Burget, Luk{\'{a}}s and Cernock{\'{y}}, Jan and Khudanpur, Sanjeev},
booktitle = {{\{}INTERSPEECH{\}} 2010, 11th Annual Conference of the International Speech Communication Association, Makuhari, Chiba, Japan, September 26-30, 2010},
pages = {1045--1048},
title = {{Recurrent neural network based language model}},
url = {http://www.isca-speech.org/archive/interspeech{\_}2010/i10{\_}1045.html},
year = {2010}
}
@book{Pearl89,
author = {Pearl, Judea},
pages = {I--XIX, 1--552},
publisher = {Morgan Kaufmann},
series = {Morgan Kaufmann series in representation and reasoning},
title = {{Probabilistic reasoning in intelligent systems - networks of plausible inference}},
year = {1989}
}
@techreport{Wang,
abstract = {The Super-Resolution Generative Adversarial Network (SR-GAN) [1] is a seminal work that is capable of generating realistic textures during single image super-resolution. However, the hallucinated details are often accompanied with unpleasant artifacts. To further enhance the visual quality, we thoroughly study three key components of SRGAN-network architecture, adversarial loss and perceptual loss, and improve each of them to derive an Enhanced SRGAN (ESRGAN). In particular , we introduce the Residual-in-Residual Dense Block (RRDB) without batch normalization as the basic network building unit. Moreover, we borrow the idea from relativistic GAN [2] to let the discriminator predict relative realness instead of the absolute value. Finally, we improve the perceptual loss by using the features before activation, which could provide stronger supervision for brightness consistency and texture recovery. Benefiting from these improvements, the proposed ESRGAN achieves consistently better visual quality with more realistic and natural textures than SRGAN and won the first place in the PIRM2018-SR Challenge 1 [3]. The code is available at https://github.com/xinntao/ESRGAN.},
archivePrefix = {arXiv},
arxivId = {1809.00219v2},
author = {Wang, Xintao and Yu, Ke and Wu, Shixiang and Gu, Jinjin and Liu, Yihao and Dong, Chao and Loy, Chen Change and Qiao, Yu and Tang, Xiaoou},
eprint = {1809.00219v2},
file = {:Users/maucher/Library/Application Support/Mendeley Desktop/Downloaded/Wang et al. - Unknown - ESRGAN Enhanced Super-Resolution Generative Adversarial Networks.pdf:pdf},
isbn = {1809.00219v2},
title = {{ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks}},
url = {https://github.com/xinntao/ESRGAN.}
}
@techreport{BarredoArrieta2019,
abstract = {In the last few years, Artificial Intelligence (AI) has achieved a notable momentum that, if harnessed appropriately, may deliver the best of expectations over many application sectors across the field. For this to occur shortly in Machine Learning, the entire community stands in front of the barrier of explainability, an inherent problem of the latest techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of AI (namely, expert systems and rule based models). Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is widely acknowledged as a crucial feature for the practical deployment of AI models. The overview presented in this article examines the existing literature and contributions already done in the field of XAI, including a prospect toward what is yet to be reached. For this purpose we summarize previous efforts made to define explainability in Machine Learning, establishing a novel definition of explainable Machine Learning that covers such prior conceptual propositions with a major focus on the audience for which the explainability is sought. Departing from this definition, we propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at explaining Deep Learning methods for which a second dedicated taxonomy is built and examined in detail. This critical literature analysis serves as the motivating background for a series of challenges faced by XAI, such as the interesting crossroads of data fusion and explainability. Our prospects lead toward the concept of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of AI methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to the field of XAI with a thorough taxonomy that can serve as reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the benefits of AI in their activity sectors, without any prior bias for its lack of interpretability.},
archivePrefix = {arXiv},
arxivId = {1910.10045v2},
author = {{Barredo Arrieta}, Alejandro and D{\'{i}}az-Rodr{\'{i}}guez, Natalia and Ser, Javier Del and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and Garcia, Salvador and Gil-Lopez, Sergio and Molina, Daniel and Benjamins, Richard and Chatila, Raja and Herrera, Francisco},
eprint = {1910.10045v2},
file = {:Users/maucher/Library/Application Support/Mendeley Desktop/Downloaded/Barredo Arrieta et al. - 2019 - Explainable Artificial Intelligence (XAI) Concepts, Taxonomies, Opportunities and Challenges toward Resp.pdf:pdf},
keywords = {Accountability,Comprehensibility,Data Fusion,Deep Learning,Explainable Artificial Intelligence,Fairness,Interpretability,Machine Learning,Privacy,Responsible Artificial Intelligence * Correspondin,Transparency},
title = {{Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI}},
year = {2019}
}
@techreport{Zhang,
abstract = {Synthesizing high-quality images from text descriptions is a challenging problem in computer vision and has many practical applications. Samples generated by existing text-to-image approaches can roughly reflect the meaning of the given descriptions, but they fail to contain necessary details and vivid object parts. In this paper, we propose Stacked Generative Adversarial Networks (StackGAN) to generate 256×256 photo-realistic images conditioned on text descriptions. We decompose the hard problem into more manageable sub-problems through a sketch-refinement process. The Stage-I GAN sketches the primitive shape and colors of the object based on the given text description, yielding Stage-I low-resolution images. The Stage-II GAN takes Stage-I results and text descriptions as inputs, and generates high-resolution images with photo-realistic details. It is able to rectify defects in Stage-I results and add compelling details with the refinement process. To improve the diversity of the synthesized images and stabilize the training of the conditional-GAN, we introduce a novel Conditioning Augmentation technique that encourages smoothness in the latent conditioning manifold. Extensive experiments and comparisons with state-of-the-arts on benchmark datasets demonstrate that the proposed method achieves significant improvements on generating photo-realistic images conditioned on text descriptions.},
archivePrefix = {arXiv},
arxivId = {1612.03242v2},
author = {Zhang, Han and Xu, Tao and Li, Hongsheng and Zhang, Shaoting and Wang, Xiaogang and Huang, Xiaolei and Metaxas, Dimitris},
eprint = {1612.03242v2},
file = {:Users/maucher/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - Unknown - StackGAN Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks.pdf:pdf},
title = {{StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks}},
url = {https://github.com/hanzhanggit/StackGAN.}
}
@misc{DLT-10,
author = {Lab, LISA},
howpublished = {$\backslash$url{\{}http://deeplearning.net/tutorial/{\}}},
title = {{Deep Learning Tutorials v0.1}},
year = {2011}
}
@techreport{Devlin,
abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Rad-ford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result , the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5{\%} (7.7{\%} point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
archivePrefix = {arXiv},
arxivId = {1810.04805v2},
author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Google, Kristina Toutanova and Language, A I},
eprint = {1810.04805v2},
file = {:Users/maucher/Library/Application Support/Mendeley Desktop/Downloaded/Devlin et al. - Unknown - BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf:pdf},
isbn = {1810.04805v2},
title = {{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}},
url = {https://github.com/tensorflow/tensor2tensor}
}
@article{Saul96,
author = {Saul, Lawrence K and Jaakkola, Tommi and Jordan, Michael I},
journal = {Journal of Artificial Intelligence Research},
pages = {61--76},
title = {{Mean Field Theory for Sigmoid Belief Networks}},
volume = {4},
year = {1996}
}
@techreport{Mirza,
abstract = {Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.},
archivePrefix = {arXiv},
arxivId = {1411.1784v1},
author = {Mirza, Mehdi and Osindero, Simon},
eprint = {1411.1784v1},
file = {:Users/maucher/Library/Application Support/Mendeley Desktop/Downloaded/Mirza, Osindero - Unknown - Conditional Generative Adversarial Nets.pdf:pdf},
title = {{Conditional Generative Adversarial Nets}}
}
@techreport{Karras,
abstract = {We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CELEBA images at 1024 2. We also propose a simple way to increase the variation in generated images , and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally , we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CELEBA dataset.},
archivePrefix = {arXiv},
arxivId = {1710.10196v3},
author = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
eprint = {1710.10196v3},
file = {:Users/maucher/Library/Application Support/Mendeley Desktop/Downloaded/Karras et al. - Unknown - PROGRESSIVE GROWING OF GANS FOR IMPROVED QUALITY, STABILITY, AND VARIATION(2).pdf:pdf},
title = {{PROGRESSIVE GROWING OF GANS FOR IMPROVED QUALITY, STABILITY, AND VARIATION}},
url = {https://youtu.be/G06dEcZ-QTg.}
}
@techreport{Radford,
abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsuper-vised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolu-tional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks-demonstrating their applicability as general image representations .},
archivePrefix = {arXiv},
arxivId = {1511.06434v2},
author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
eprint = {1511.06434v2},
file = {:Users/maucher/Library/Application Support/Mendeley Desktop/Downloaded/Radford, Metz, Chintala - Unknown - UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS.pdf:pdf},
title = {{UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS}}
}

@inproceedings{yang-etal-2016-hierarchical,
	title = "Hierarchical Attention Networks for Document Classification",
	author = "Yang, Zichao  and
	Yang, Diyi  and
	Dyer, Chris  and
	He, Xiaodong  and
	Smola, Alex  and
	Hovy, Eduard",
	booktitle = "Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
	month = jun,
	year = "2016",
	address = "San Diego, California",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/N16-1174",
	doi = "10.18653/v1/N16-1174",
	pages = "1480--1489",
}

@inproceedings{li-etal-2015-hierarchical,
	title = "A Hierarchical Neural Autoencoder for Paragraphs and Documents",
	author = "Li, Jiwei  and
	Luong, Thang  and
	Jurafsky, Dan",
	booktitle = "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
	month = jul,
	year = "2015",
	address = "Beijing, China",
	publisher = "Association for Computational Linguistics",
	url = "https://www.aclweb.org/anthology/P15-1107",
	doi = "10.3115/v1/P15-1107",
	pages = "1106--1115",
}
