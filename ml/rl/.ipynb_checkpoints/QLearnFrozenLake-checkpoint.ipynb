{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Q-Learning\n",
    "\n",
    "* Author: Johannes Maucher\n",
    "* Last update: 16.09.2021\n",
    "\n",
    "\n",
    "This notebook demonstrates Q-Learning by an example, where an agent has to navigate from a start-state to a goal-state. The [Frozen Lake Environment](https://gym.openai.com/envs/FrozenLake-v0/)is provided by [Open AI gym](https://gym.openai.com)\n",
    "\n",
    "\n",
    "## Environment\n",
    "\n",
    "**Description:** (from [Frozen Lake](https://gym.openai.com/envs/FrozenLake-v0/))\n",
    "\n",
    "*Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend.*\n",
    "\n",
    "As depicted below the environment consists of 16 states, indexed by 0 to 15 from the upper right to the lower left corner. There are 4 different types of states:\n",
    "\n",
    "* S: Start (safe)\n",
    "* F: Frozen (safe)\n",
    "* H: Hole (Game lost)\n",
    "* G: Goal (Game won)\n",
    "\n",
    "The task is to find a policy for the agent to navigate efficiently from the Start (S) to the Goal (G) state. \n",
    "\n",
    "![Frozen Lake pic](https://maucher.home.hdm-stuttgart.de/Pics/FrozenWorld.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions\n",
    "\n",
    "The agent can move left (0), down (1), right (2) or up (3). If the agent moves to a wall it remains in the current state.\n",
    "\n",
    "## Transition- and Reward-Model\n",
    "In states `H` and `G` the game is over. In states `S` and `F` the transition modell is defined by:\n",
    "* the probability that the agent actually moves in the direction, defined by the selected action is $1/3$.\n",
    "* the probability that the agent moves to the field left-perpendicular to the direction given by the selected action is $1/3$.\n",
    "* the probability that the agent moves to the field right-perpendicular to the direction given by the selected action is $1/3$.\n",
    "\n",
    "The reward for turning into state `G` is 1. Turning in any other state yields no reward (r=0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some functions of gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "np.random.seed(123456)\n",
    "np.set_printoptions(precision=4,suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.render(mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[b'S', b'F', b'F', b'F'],\n",
       "       [b'F', b'H', b'F', b'H'],\n",
       "       [b'F', b'F', b'F', b'H'],\n",
       "       [b'H', b'F', b'F', b'G']], dtype='|S1')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.env.desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.env.reward_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(16)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Environment-, Action-, Transition- and reward-model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {0: [(0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False)]},\n",
       " 1: {0: [(0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True)],\n",
       "  1: [(0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 2, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 0, 0.0, False)]},\n",
       " 2: {0: [(0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 6, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 1, 0.0, False),\n",
       "   (0.3333333333333333, 6, 0.0, False),\n",
       "   (0.3333333333333333, 3, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 6, 0.0, False),\n",
       "   (0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 2, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 1, 0.0, False)]},\n",
       " 3: {0: [(0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 7, 0.0, True)],\n",
       "  1: [(0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 7, 0.0, True),\n",
       "   (0.3333333333333333, 3, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 7, 0.0, True),\n",
       "   (0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 3, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 3, 0.0, False),\n",
       "   (0.3333333333333333, 2, 0.0, False)]},\n",
       " 4: {0: [(0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 8, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True)],\n",
       "  2: [(0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 0, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 0, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False)]},\n",
       " 5: {0: [(1.0, 5, 0, True)],\n",
       "  1: [(1.0, 5, 0, True)],\n",
       "  2: [(1.0, 5, 0, True)],\n",
       "  3: [(1.0, 5, 0, True)]},\n",
       " 6: {0: [(0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 10, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 7, 0.0, True)],\n",
       "  2: [(0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 7, 0.0, True),\n",
       "   (0.3333333333333333, 2, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 7, 0.0, True),\n",
       "   (0.3333333333333333, 2, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True)]},\n",
       " 7: {0: [(1.0, 7, 0, True)],\n",
       "  1: [(1.0, 7, 0, True)],\n",
       "  2: [(1.0, 7, 0, True)],\n",
       "  3: [(1.0, 7, 0, True)]},\n",
       " 8: {0: [(0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 12, 0.0, True)],\n",
       "  1: [(0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 12, 0.0, True),\n",
       "   (0.3333333333333333, 9, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 12, 0.0, True),\n",
       "   (0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 4, 0.0, False),\n",
       "   (0.3333333333333333, 8, 0.0, False)]},\n",
       " 9: {0: [(0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 13, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 8, 0.0, False),\n",
       "   (0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 10, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True)],\n",
       "  3: [(0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 5, 0.0, True),\n",
       "   (0.3333333333333333, 8, 0.0, False)]},\n",
       " 10: {0: [(0.3333333333333333, 6, 0.0, False),\n",
       "   (0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 11, 0.0, True)],\n",
       "  2: [(0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 11, 0.0, True),\n",
       "   (0.3333333333333333, 6, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 11, 0.0, True),\n",
       "   (0.3333333333333333, 6, 0.0, False),\n",
       "   (0.3333333333333333, 9, 0.0, False)]},\n",
       " 11: {0: [(1.0, 11, 0, True)],\n",
       "  1: [(1.0, 11, 0, True)],\n",
       "  2: [(1.0, 11, 0, True)],\n",
       "  3: [(1.0, 11, 0, True)]},\n",
       " 12: {0: [(1.0, 12, 0, True)],\n",
       "  1: [(1.0, 12, 0, True)],\n",
       "  2: [(1.0, 12, 0, True)],\n",
       "  3: [(1.0, 12, 0, True)]},\n",
       " 13: {0: [(0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 12, 0.0, True),\n",
       "   (0.3333333333333333, 13, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 12, 0.0, True),\n",
       "   (0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False)],\n",
       "  2: [(0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 9, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 9, 0.0, False),\n",
       "   (0.3333333333333333, 12, 0.0, True)]},\n",
       " 14: {0: [(0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False)],\n",
       "  1: [(0.3333333333333333, 13, 0.0, False),\n",
       "   (0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 15, 1.0, True)],\n",
       "  2: [(0.3333333333333333, 14, 0.0, False),\n",
       "   (0.3333333333333333, 15, 1.0, True),\n",
       "   (0.3333333333333333, 10, 0.0, False)],\n",
       "  3: [(0.3333333333333333, 15, 1.0, True),\n",
       "   (0.3333333333333333, 10, 0.0, False),\n",
       "   (0.3333333333333333, 13, 0.0, False)]},\n",
       " 15: {0: [(1.0, 15, 0, True)],\n",
       "  1: [(1.0, 15, 0, True)],\n",
       "  2: [(1.0, 15, 0, True)],\n",
       "  3: [(1.0, 15, 0, True)]}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.env.P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3333333333333333, 4, 0.0, False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start=0\n",
    "targetDirection=1 # 0:left, 1:down, 2:right, 3:up\n",
    "drift=1           # 0:drift to right, 1 no drift, 2: drift to left\n",
    "env.env.P[start][targetDirection][drift]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability for this drift: 0.333\n",
      "New state:  4\n",
      "Reward: 0.00\n",
      "Game over?: False\n"
     ]
    }
   ],
   "source": [
    "print(\"Probability for this drift: %1.3f\"%env.env.P[start][targetDirection][drift][0])\n",
    "print(\"New state: %2d\"%env.env.P[start][targetDirection][drift][1])\n",
    "print(\"Reward: %2.2f\"%env.env.P[start][targetDirection][drift][2])\n",
    "print(\"Game over?: %s\"%env.env.P[start][targetDirection][drift][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interaction with gym environment\n",
    "Execute action and obtain new state and reward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old state:  0\n",
      "New state:  0 \t Reward:  0.0 \t Game Over?:  False\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "action=0\n",
    "print(\"Old state: \",env.env.s)\n",
    "s1,r,d,_=env.step(action) # s1 is new state, r is reward and d is True if game is over\n",
    "print(\"New state: \",s1, \"\\t Reward: \",r, \"\\t Game Over?: \",d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Old state:  0\n",
      "Intended Action:  1\n",
      "New state:  0 \t Reward:  0.0 \t Game Over?:  False\n",
      "----------\n",
      "Old state:  0\n",
      "Intended Action:  2\n",
      "New state:  0 \t Reward:  0.0 \t Game Over?:  False\n",
      "----------\n",
      "Old state:  0\n",
      "Intended Action:  2\n",
      "New state:  1 \t Reward:  0.0 \t Game Over?:  False\n",
      "----------\n",
      "Old state:  1\n",
      "Intended Action:  1\n",
      "New state:  2 \t Reward:  0.0 \t Game Over?:  False\n",
      "----------\n",
      "Old state:  2\n",
      "Intended Action:  0\n",
      "New state:  6 \t Reward:  0.0 \t Game Over?:  False\n",
      "----------\n",
      "Old state:  6\n",
      "Intended Action:  3\n",
      "New state:  2 \t Reward:  0.0 \t Game Over?:  False\n",
      "----------\n",
      "Old state:  2\n",
      "Intended Action:  3\n",
      "New state:  1 \t Reward:  0.0 \t Game Over?:  False\n",
      "----------\n",
      "Old state:  1\n",
      "Intended Action:  3\n",
      "New state:  2 \t Reward:  0.0 \t Game Over?:  False\n",
      "----------\n",
      "Old state:  2\n",
      "Intended Action:  3\n",
      "New state:  2 \t Reward:  0.0 \t Game Over?:  False\n",
      "----------\n",
      "Old state:  2\n",
      "Intended Action:  0\n",
      "New state:  1 \t Reward:  0.0 \t Game Over?:  False\n",
      "----------\n",
      "Old state:  1\n",
      "Intended Action:  3\n",
      "New state:  2 \t Reward:  0.0 \t Game Over?:  False\n",
      "----------\n",
      "Old state:  2\n",
      "Intended Action:  0\n",
      "New state:  1 \t Reward:  0.0 \t Game Over?:  False\n",
      "----------\n",
      "Old state:  1\n",
      "Intended Action:  0\n",
      "New state:  1 \t Reward:  0.0 \t Game Over?:  False\n",
      "----------\n",
      "Old state:  1\n",
      "Intended Action:  2\n",
      "New state:  1 \t Reward:  0.0 \t Game Over?:  False\n",
      "----------\n",
      "Old state:  1\n",
      "Intended Action:  2\n",
      "New state:  1 \t Reward:  0.0 \t Game Over?:  False\n",
      "----------\n",
      "Old state:  1\n",
      "Intended Action:  0\n",
      "New state:  5 \t Reward:  0.0 \t Game Over?:  True\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "d=False\n",
    "while not d:\n",
    "    print(\"-\"*10)\n",
    "    print(\"Old state: \",env.env.s)\n",
    "    action=np.random.randint(4)\n",
    "    print(\"Intended Action: \",action)\n",
    "    s1,r,d,_=env.step(action) # s1 is new state, r is reward and d is True if game is over\n",
    "    print(\"New state: \",s1, \"\\t Reward: \",r, \"\\t Game Over?: \",d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Q-Learning in Table Representation\n",
    "\n",
    "In this section Q-learning, as defined in the pseudo code below is implemented:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Q Learning Pseudo Code Image](https://maucher.home.hdm-stuttgart.de/Pics/QlearningPseudoCode.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization and hyperparameter setting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize table with all zeros\n",
    "Q = np.zeros([env.observation_space.n,env.action_space.n])\n",
    "# Set learning parameters\n",
    "lr = .9   # parameter \\alpha  \n",
    "y = .95 #discount \\nu\n",
    "num_episodes = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Table of Q-values: \n",
      " [[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Initial Table of Q-values: \\n\",Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the array above each row belongs to a state and each column belongs to an action. The entry in row i, column j is the $Q(s,a)$-value for the j.th action in the i.th state. Initially all these values are zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.1192, -1.0442, -0.8618, -2.1046]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randn(1,env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code-cell below Q-learning is implemented. Note that the action-selection, in particular the Explore/Exploit-Trade-Off, is implemented such that with an increasing number of episodes the random contribution to action-selection decreases: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create lists to contain total rewards and steps per episode\n",
    "rList = []\n",
    "for i in range(num_episodes):\n",
    "    #Reset environment and get first new observation\n",
    "    s = env.reset()\n",
    "    rAll = 0\n",
    "    d = False\n",
    "    j = 0\n",
    "    #The Q-Table learning algorithm\n",
    "    while j < 99:\n",
    "        j+=1\n",
    "        #Choose an action by greedily (with noise) picking from Q table\n",
    "        options=Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1))\n",
    "        a = np.argmax(options)\n",
    "        #Get new state and reward from environment\n",
    "        s1,r,d,_ = env.step(a)\n",
    "        #print(s,a,s1)\n",
    "        #Update Q-Table with new knowledge\n",
    "        Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])\n",
    "        rAll += r\n",
    "        s = s1\n",
    "        if d == True:\n",
    "            break\n",
    "    #jList.append(j)\n",
    "    rList.append(rAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 0.492\n"
     ]
    }
   ],
   "source": [
    "print(\"Score over time: \" +  str(sum(rList)/num_episodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learned Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final $Q(s_t,a_t)$-values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4192, 0.003 , 0.0034, 0.004 ],\n",
       "       [0.0002, 0.    , 0.0003, 0.0556],\n",
       "       [0.0006, 0.0105, 0.0017, 0.002 ],\n",
       "       [0.0012, 0.    , 0.0001, 0.0072],\n",
       "       [0.5757, 0.    , 0.0003, 0.0006],\n",
       "       [0.    , 0.    , 0.    , 0.    ],\n",
       "       [0.0011, 0.0001, 0.0005, 0.    ],\n",
       "       [0.    , 0.    , 0.    , 0.    ],\n",
       "       [0.0003, 0.0003, 0.0004, 0.7436],\n",
       "       [0.0001, 0.7932, 0.0002, 0.    ],\n",
       "       [0.1688, 0.0001, 0.    , 0.0003],\n",
       "       [0.    , 0.    , 0.    , 0.    ],\n",
       "       [0.    , 0.    , 0.    , 0.    ],\n",
       "       [0.0007, 0.0007, 0.7261, 0.    ],\n",
       "       [0.    , 0.9656, 0.    , 0.    ],\n",
       "       [0.    , 0.    , 0.    , 0.    ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each state (row in the array above) the action of the learned strategy is the positon with the maximal value in the corresponding row: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy=np.argmax(Q,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 3, 1, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best action in state  0 is  0\n",
      "Best action in state  3 is  3\n",
      "Best action in state  1 is  3\n",
      "Best action in state  3 is  3\n",
      "Best action in state  0 is  0\n",
      "Best action in state  0 is  0\n",
      "Best action in state  0 is  0\n",
      "Best action in state  0 is  0\n",
      "Best action in state  3 is  3\n",
      "Best action in state  1 is  3\n",
      "Best action in state  0 is  0\n",
      "Best action in state  0 is  0\n",
      "Best action in state  0 is  0\n",
      "Best action in state  2 is  1\n",
      "Best action in state  1 is  3\n",
      "Best action in state  0 is  0\n"
     ]
    }
   ],
   "source": [
    "actionList=[\"Left\",\"Down\",\"Right\",\"Up\"]\n",
    "for state in strategy:\n",
    "    print(\"Best action in state %2d is %2s\"%(state,strategy[state]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Old state:  0\n",
      "Intended Action:  0\n",
      "New state:  0 \t Reward:  0.0 \t Game Over?:  False\n",
      "----------\n",
      "Old state:  0\n",
      "Intended Action:  0\n",
      "New state:  0 \t Reward:  0.0 \t Game Over?:  False\n",
      "----------\n",
      "Old state:  0\n",
      "Intended Action:  0\n",
      "New state:  0 \t Reward:  0.0 \t Game Over?:  False\n",
      "----------\n",
      "Old state:  0\n",
      "Intended Action:  0\n",
      "New state:  0 \t Reward:  0.0 \t Game Over?:  False\n",
      "----------\n",
      "Old state:  0\n",
      "Intended Action:  0\n",
      "New state:  0 \t Reward:  0.0 \t Game Over?:  False\n",
      "----------\n",
      "Old state:  0\n",
      "Intended Action:  0\n",
      "New state:  0 \t Reward:  0.0 \t Game Over?:  False\n",
      "----------\n",
      "Old state:  0\n",
      "Intended Action:  0\n",
      "New state:  0 \t Reward:  0.0 \t Game Over?:  False\n",
      "----------\n",
      "Old state:  0\n",
      "Intended Action:  0\n",
      "New state:  0 \t Reward:  0.0 \t Game Over?:  False\n",
      "----------\n",
      "Old state:  0\n",
      "Intended Action:  0\n",
      "New state:  0 \t Reward:  0.0 \t Game Over?:  False\n",
      "----------\n",
      "Old state:  0\n",
      "Intended Action:  0\n",
      "New state:  0 \t Reward:  0.0 \t Game Over?:  False\n",
      "----------\n",
      "Old state:  0\n",
      "Intended Action:  0\n",
      "New state:  4 \t Reward:  0.0 \t Game Over?:  False\n",
      "----------\n",
      "Old state:  4\n",
      "Intended Action:  0\n",
      "New state:  4 \t Reward:  0.0 \t Game Over?:  False\n",
      "----------\n",
      "Old state:  4\n",
      "Intended Action:  0\n",
      "New state:  4 \t Reward:  0.0 \t Game Over?:  False\n",
      "----------\n",
      "Old state:  4\n",
      "Intended Action:  0\n",
      "New state:  4 \t Reward:  0.0 \t Game Over?:  False\n",
      "----------\n",
      "Old state:  4\n",
      "Intended Action:  0\n",
      "New state:  8 \t Reward:  0.0 \t Game Over?:  False\n",
      "----------\n",
      "Old state:  8\n",
      "Intended Action:  3\n",
      "New state:  9 \t Reward:  0.0 \t Game Over?:  False\n",
      "----------\n",
      "Old state:  9\n",
      "Intended Action:  1\n",
      "New state:  10 \t Reward:  0.0 \t Game Over?:  False\n",
      "----------\n",
      "Old state:  10\n",
      "Intended Action:  0\n",
      "New state:  9 \t Reward:  0.0 \t Game Over?:  False\n",
      "----------\n",
      "Old state:  9\n",
      "Intended Action:  1\n",
      "New state:  10 \t Reward:  0.0 \t Game Over?:  False\n",
      "----------\n",
      "Old state:  10\n",
      "Intended Action:  0\n",
      "New state:  14 \t Reward:  0.0 \t Game Over?:  False\n",
      "----------\n",
      "Old state:  14\n",
      "Intended Action:  1\n",
      "New state:  14 \t Reward:  0.0 \t Game Over?:  False\n",
      "----------\n",
      "Old state:  14\n",
      "Intended Action:  1\n",
      "New state:  15 \t Reward:  1.0 \t Game Over?:  True\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "d=False\n",
    "while not d:\n",
    "    print(\"-\"*10)\n",
    "    print(\"Old state: \",env.env.s)\n",
    "    #action=np.random.randint(4)\n",
    "    action=strategy[env.env.s]\n",
    "    print(\"Intended Action: \",action)\n",
    "    s1,r,d,_=env.step(action) # s1 is new state, r is reward and d is True if game is over\n",
    "    print(\"New state: \",s1, \"\\t Reward: \",r, \"\\t Game Over?: \",d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game terminated with:\n",
      "Final state:  15 \t Reward:  1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Game terminated with:\")\n",
    "print(\"Final state: \",s1, \"\\t Reward: \",r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(InputLayer(batch_input_shape=(1, 16)))\n",
    "model.add(Dense(50, activation='sigmoid'))\n",
    "model.add(Dense(4, activation='linear'))\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 of 500\n",
      "Episode 101 of 500\n",
      "Episode 201 of 500\n",
      "Episode 301 of 500\n",
      "Episode 401 of 500\n"
     ]
    }
   ],
   "source": [
    "# now execute the q learning\n",
    "y = 0.95\n",
    "eps = 0.5\n",
    "decay_factor = 0.999\n",
    "num_episodes=500\n",
    "r_avg_list = []\n",
    "for i in range(num_episodes):\n",
    "    s = env.reset()\n",
    "    eps *= decay_factor\n",
    "    if i % 100 == 0:\n",
    "        print(\"Episode {} of {}\".format(i + 1, num_episodes))\n",
    "    done = False\n",
    "    r_sum = 0\n",
    "    while not done:\n",
    "        if np.random.random() < eps:\n",
    "            a = np.random.randint(0, 4)\n",
    "        else:\n",
    "            a = np.argmax(model.predict(np.identity(16)[s:s + 1]))\n",
    "        new_s, r, done, _ = env.step(a)\n",
    "        target = r + y * np.max(model.predict(np.identity(16)[new_s:new_s + 1]))\n",
    "        target_vec = model.predict(np.identity(16)[s:s + 1])[0]\n",
    "        target_vec[a] = target\n",
    "        model.fit(np.identity(16)[s:s + 1], target_vec.reshape(-1, 4), epochs=1, verbose=0)\n",
    "        s = new_s\n",
    "        r_sum += r\n",
    "    r_avg_list.append(r_sum/(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAYfElEQVR4nO3de4wd110H8O+368RN4zSPehMS24ld5AKmSkqyTYNKSyjQ2CnCIJBwCqREDSY0QUX80TiqKKAioVKBStU0xgomrRJikJJSE20JEZAGEZV6neblpE63zsMbm3rzJk/Xzo8/7qzv3es7e+Zez+M353w/krX3Mbv+nZmd78yec88MzQwiItJ+b2m6ABERKYcCXUQkEgp0EZFIKNBFRCKhQBcRicSipv7jpUuX2sqVK5v670VEWmnnzp3PmNn4oPcaC/SVK1diamqqqf9eRKSVSD6Z9566XEREIqFAFxGJhAJdRCQSCnQRkUgo0EVEIhEMdJJbSR4g+XDO+yT5RZLTJB8keX75ZYqISEiRM/SbAKxd4P11AFZn/zYCuOHYyxIRkWEFA93M7gHw3AKLrAfwVev4FoBTSJ5ZVoHSnGdffgPfeGh/02XU7vCbhn/asReHDr+Zu8yj+1/Cziefr7Gq4Tz/ykFMOtp2kw/tx/OvHGy6jOiV0Ye+DMDenucz2WtHIbmR5BTJqdnZ2RL+a6nS7351Cr9/y314LrEd8R937MWnbnsQf//fT+Qus+5v/gu/dsO99RU1pN+7eSc+cct9OPDS602Xgh+89Do+cct9uOrmnU2XEr0yAp0DXht41wwz22JmE2Y2MT4+cOaqOLL3+dcAYMEz1Rg9/2rnAPbcq+09kD2dbbuDDrbdwUOdGp5+4bWGK4lfGYE+A2BFz/PlAPaV8HNFRGQIZQT6dgCXZ592uQjAi2bmp/NORCQRwYtzkbwVwMUAlpKcAfAnAI4DADPbDGASwKUApgG8CuCKqooVEZF8wUA3s8sC7xuAq0urSERERqKZoiIikVCgi4hEQoEuIhIJBbqISCQU6CIikVCgi4hEQoEuIhIJBbqISCQU6CIikVCgi4hEQoEuQQOvhSwi7ijQRUQioUCXINMpukgrKNAlyNTpItIKCnQJ0hl6e2nbpUWBLkHKBDkWOqjUR4EuubQjtpc52nhzXXaOSoqWAl2CPIWDDEebLi0KdMlFdr4qFNqH2cbzMKA99/sz9/sk1VGgi0TMw8HYQQnJUKBLkIdQkNF42HTqsquPAl1EJBIKdAny0A8ro/Fwdtx8BelQoEuQg0yQEXnYdPr9qY8CXYK0P7aXjzB1UUQSFOgiUikfB5U0KNAlyEM/rIxK2y4lCnQJUiS0l4djsYMSkqFAlyAPoSCj8bDp9PtTHwW6FKA9sq08hKk+9lofBbqIVMrDQSUVhQKd5FqSu0lOk9w04P2TSf4LyQdI7iJ5RfmlSlO0Q7aXzo7TEgx0kmMArgewDsAaAJeRXNO32NUAHjGz8wBcDOCvSB5fcq3SEEVCe3k4GHuoIRVFztAvBDBtZnvM7CCAbQDW9y1jAE5i55qdSwA8B+BQqZVKY7RDtpeHbae/EupTJNCXAdjb83wme63XlwD8BIB9AB4C8Ekze7P/B5HcSHKK5NTs7OyIJUvdtEO2l4dt5+GgkooigT7osvT9m+gSAPcDOAvAewB8ieTbj/omsy1mNmFmE+Pj40OWKnXTjtheHieDOSwpOkUCfQbAip7ny9E5E+91BYDbrWMawOMAfrycEqVp2hHbS9suLUUCfQeA1SRXZQOdGwBs71vmKQA/DwAkzwDwYwD2lFmo1E+3oGsvOrrfm25BV59FoQXM7BDJawDcCWAMwFYz20Xyquz9zQA+C+Amkg+h00VzrZk9U2HdUiMP/bAyGg8HY/3+1CcY6ABgZpMAJvte29zzeB+AD5dbmnjhIRRkNB7CVL8/9dFMUcmlHbG9PA2KzlXiqKRoKdBFIqYQTYsCXXJpULS95gZFPWy6ub8WNChaPQW6BHnoh5XReOh6ab6CdCjQRaRSDo4pyVCgS665HVE7ZPvMnZn72HRZLT6KiZoCXYK0H7aXQjQtCnTJ1R0UVSq0TXemaPPbTjNF66NAl6DmI0FG5eFY7KCEZCjQRaRSHg4qqVCgS5B2yPbSpkuLAl0KUCy0lYeDscZg6qNAlyDtj+3lIUybryAdCnQJ0g7ZXh62nYNjSjIU6JJLO2J7eTgzn2OaWFQbBboEaUdsL227tCjQJZcmFrVX92qLDradJhbVRoEuQQ4iQUblYOM5KCEZCnQRqZT+wKuPAl1y6WqLYV67ozxdbVGDovVRoEuQi35Yp7yHlPf6pFwKdMnl6IJ9bnldNZ4GRXW1xfoo0CWo+Ujwy2uXyxwP5TkoIRkKdBGplPeDXkwU6JJLg6JhXleNr0HR7KuHYiKnQJcgD/2wXnkPKZ0dp0WBLrm6M0WbrcMzrwe77qCoAxoUrY0CXYJchIJT7g92DurzetCLkQJdcrkPK8nlqatFYzH1UaBLkKdwkOF4ODvWr099CgU6ybUkd5OcJrkpZ5mLSd5PchfJb5ZbpjRJ+2M+72HlvT4p16LQAiTHAFwP4BcBzADYQXK7mT3Ss8wpAL4MYK2ZPUXy9IrqlRpppmiYhzPgQY4Mijoob64EDYpWr8gZ+oUAps1sj5kdBLANwPq+ZT4K4HYzewoAzOxAuWWK1EddTOXS+qxPkUBfBmBvz/OZ7LVe7wJwKsm7Se4kefmgH0RyI8kpklOzs7OjVSy1OTKY5fQstCrD5I/XrNLEojQVCfRBfyj1b5pFAC4A8BEAlwD4Y5LvOuqbzLaY2YSZTYyPjw9drDRDO2I+76tGZ8dpCfaho3NGvqLn+XIA+wYs84yZvQLgFZL3ADgPwGOlVCmNSHVi0TDN9RqYniYW6WqL9Slyhr4DwGqSq0geD2ADgO19y3wdwAdILiL5NgDvA/BouaVKUzyEQp2G6nKproxS+DjeuCgiCcEzdDM7RPIaAHcCGAOw1cx2kbwqe3+zmT1K8l8BPAjgTQA3mtnDVRYuUpXUxgyq5uOgkoYiXS4ws0kAk32vbe57/nkAny+vNGlad4ZfWntkTIOiHs6ONShaH80UlSDthwtwvnIUomlRoEuu1AdFi7Tba/eMBkXTpECXAjzEQo2OfIY73G7vBzsP9Xk96MVIgS7SR/FTLg8HlVQo0CVXqpc9HWY80euqsSH+yqiaBkXro0CXoFT3wyLt9v4JIOflSckU6JIr3UHR7Oy2QMO9rhpfg6KdKjQoWj0FugR5+LO9TsN0NXk/2Hn/C0LKpUCXXKlmQQzN9hTkqY7FNEGBLkGp7YjdywYXWDaK+K+W1lF9FOgSlOruWOhA5nzlpHYwTp0CXXJ1B0XTSoUjg6JFJhZVXcyIuoOizVeomaL1UaCL9Gs+A6OS2PlAoxTokivVHTE0Eab3Lxav6+jIxCIH9WliUX0U6BKU2o4Y6mLqfdtDl8ZCPGy71LrsmqRAlyDvoVWVQhOLnK8a5+VJyRTokivZmaKBjy1azmNPjgyKOth4cxVoULR6CnSRPqEI9BCSraLVVRsFuuRKdYZfqN3zztCdrpzu1Rab1702TsOFJECBLkGp7ocx3OAi2Y2XKAW65Ep+YlHuxxZrLGZEmliUJgW6BDUfCfUKD4r6/xz6HA/1OSghGQp0kSF5CMk20fqqjwJdcg1zK7aYDDPL0kOXxiAaFE2TAl2CvIZW9do/KOq9PimXAl1yJTuxaO5rgUFRr6tGg6JpUqCL9AkdwDyEZJtobdVHgS65hrlzT0yG+dii1490erraIjzVEjkFugSltiN2D2SDG96Ga7nM8VCfhxpSoUCXoFS7GAp9ysX7qnFfoJRJgS65kh8UzXvf/J+jdwdFm6dB0foUCnSSa0nuJjlNctMCy72X5GGSv15eiSL1Cg+KyjC8jjPEKBjoJMcAXA9gHYA1AC4juSZnuc8BuLPsIqUZqQ6KYqhB0RrKGYGnQVHdgq4+Rc7QLwQwbWZ7zOwggG0A1g9Y7g8A3AbgQIn1iQeJ7YmhQdHel72vGQ9nxw5KSEaRQF8GYG/P85nstSNILgPwqwA2L/SDSG4kOUVyanZ2dthapSHJ7o8RDIo6L09KViTQBw1l9P+efAHAtWZ2eKEfZGZbzGzCzCbGx8cLlihNSXZQdJirLTqNzO4t6BouBLoFXZ0WFVhmBsCKnufLAezrW2YCwLbsl2gpgEtJHjKzfy6jSJE6hULaQ0i2iYdun1QUCfQdAFaTXAXgaQAbAHy0dwEzWzX3mORNAO5QmLdf91Zsae2QoXbbgGW98XS1xTle11VMgoFuZodIXoPOp1fGAGw1s10kr8reX7DfXNovtf1wmM+hew8pDwdjByUko8gZOsxsEsBk32sDg9zMfufYyxJPUt0h23w9dEmTZopKrlQHsWKYWERHG2/uoOeopGgp0CWoDQFWpiNXW8x7vwUTi+Z4qM9DDalQoEuuVAdFERwU9b8+uoOizdeqmaL1UaCL5CiSP95Dynt9Ui4FuuRKdmLRUQ/yFvBxBjyIrraYJgW6SJ9QF5OHkGwTrwe9GCnQJVfwIlWR6n4OPacPvQWDoq6utmjzv0p1FOgSlNqOGAqg+ddy8S21g3HqFOgSlGokFLsFne+147w8KZkCXXKlPihaqMul+nJG4mpikWliUV0U6CJ9NChartROCJqkQJdcyQ+K5t6Czv/FubqDos0XqIlF9VGgS1ByO2LoBhfz3vC9cjxsOw81pEKBLpKj2KBo9XUcC+flSckU6JKrOyiaVixYz7BoeFmffN2CToOidVGgi/QJXj7XQUi2idZXfRTokivVGX5DTSxyum50tcU0KdAlKLX9cLjrofteOy7Kc1FEGhToEpTq/lgkrL2vGu/1SbkU6JIr1UGsGG5B56nGbpeLp6ripECXIA/9sHUKfcalHROL+h80pztBTaqmQJdcGhTNeX/eY58rJzQOUKcjtXgoJnIKdFmAn1CoV/FBUa8rx9PBONVLSDRBgS5hHlKhAXEMinqvUMqkQJdcieZ4gXb7XzGeKnTUnR89BbrkKj4BPi6h9rbjFnTzvzZJg6L1UaBLLk/3paxTqN1tGBT1NP6hQdH6KNAlV+jOPbEa6o5FTleNpzP07mr0UEzcFOgS5CIUGlDo8rnVl3FMUjsYp06BLrkU5DnvtyAkPVWoQdH6FAp0kmtJ7iY5TXLTgPd/k+SD2b97SZ5XfqlSt+4V+9ISCqA2XJzLHI1Epvp71IRgoJMcA3A9gHUA1gC4jOSavsUeB/CzZnYugM8C2FJ2oVK/VM+sQpeenRfodRQ0Ak+fUOr253uoJm5FztAvBDBtZnvM7CCAbQDW9y5gZvea2fPZ028BWF5umdKkNnQxVKFQ/jhfNQrRtBQJ9GUA9vY8n8ley/NxAN8Y9AbJjSSnSE7Nzs4Wr1KaoSwYqA0HOE857umvhdgVCfRBF1EduG1I/hw6gX7toPfNbIuZTZjZxPj4ePEqpRGpftos1P08v8vF58rxNIfA1UcoI7eowDIzAFb0PF8OYF//QiTPBXAjgHVm9mw55UmTUh3MOhLSRT626HTleDor7k4s8lBN3Iqcoe8AsJrkKpLHA9gAYHvvAiTPBnA7gN82s8fKL1Oa0B0UTWtHDF0dsA0Ti+DorNjRB26iFzxDN7NDJK8BcCeAMQBbzWwXyauy9zcD+AyAdwD4Mju3uTlkZhPVlS118hAKTdDEImmbIl0uMLNJAJN9r23ueXwlgCvLLU2apiDPeb8FIemyQpdFxUUzRSWXp7ve1CnU7jZNLPJQXqpjMU1QoEuuVD+dEJoIYzmPPfFUV6pjMU1QoEsuXW0x5/0W3STaQ4hqULQ+CnQJcpAJjSjWbt8rx3d1UjYFuuRLNQ2Cg6L+efqrSje4qI8CXXJ5CoU6DTcoWn09o/A0/hH6XL+UR4EuuTz1w9bpSHNz220DHvniafwj1at2NkGBLrk8TR+vU3hQdPBjV1yeoUvVFOgiIpFQoEsuT5NT6hRq9/zPoftcOb4mhekUvS4KdMnlqR+2TqF2t6HLRYOiaVKgSy5PoVCnULvnTSyqoZ5ReKow1d+jJijQJSi1/TD0qYx5XS5OU8pTd5mv7p+4KdBFRCKhQJeB2nC9ksoErg7YhvXh6bPfqc5naIICXQaav++ltSOGrg7YO7jnNaM8DUSmOp+hCQp0GWh+P3FjZTQi2N7eT7k4jykP206DovVRoEtQqjtioVvQOV83zsuTkinQZaCU+ztDZ93e14y3bef9r5iYKNBloDbMhqxKqP/Z+8Qid/XNq8dDQfFSoMtA7kKhRsGJRa6m7RzN28E45fGYuinQZSDvoVWl4a626G/tzKvJQXltmFkbCwW6iEgkFOgyUNpdLjbv61Hv5zz2wlt9bbhUQiwU6BLkoR+2CfldLr66NPp56xJyvrqiokCXgczbaV6NLNCJ7m3QsZ+38Q8NitZHgS4DeQuFOgWvDui8O8pbd9n8QVEHBUVMgS4iEgkFugzkrR+2TqGrA7bprxcP9anLpT4KdBko4S70IT+HXnU1w3N3MHZQQioU6DJQytdDD94k2vnVFr39BdGGyw3HolCgk1xLcjfJaZKbBrxPkl/M3n+Q5Pnllyp10hn6Atdy6X3scOV4+4SS9wNgTIKBTnIMwPUA1gFYA+Aykmv6FlsHYHX2byOAG0quU0REAhjqYyP50wD+1MwuyZ5fBwBm9hc9y/wtgLvN7Nbs+W4AF5vZ/ryfOzExYVNTU0MX/M3HZvHndzwy9PfJcA6bYc/sKwCAJYsX4cyT39pwRfV54tlX8MPDhkVvIVYtPfGo919+4xD2v/g6AGD8pMU45YTj6i5xQd623f4XX8fLbxwCAPzo+Il4C9loPR78xntX4MoPvHOk7yW508wmBr23qMD3LwOwt+f5DID3FVhmGYB5gU5yIzpn8Dj77LML/NdHW7J4EVafsWSk75XhvPusk3H6SYux78XXmi6lVqvPWIIVp70Ne597NXeZDy4+DmNjxAuvHqyxsuJ+8qyT8SNvX4ynX2h+260+YwmWnXIC/velN3D4zTebLseFpUsWV/JziwT6oMNp/2l9kWVgZlsAbAE6Z+gF/u+jXHDOqbjgnAtG+VYRkagVGRSdAbCi5/lyAPtGWEZERCpUJNB3AFhNchXJ4wFsALC9b5ntAC7PPu1yEYAXF+o/FxGR8gW7XMzsEMlrANwJYAzAVjPbRfKq7P3NACYBXApgGsCrAK6ormQRERmkSB86zGwSndDufW1zz2MDcHW5pYmIyDA0U1REJBIKdBGRSCjQRUQioUAXEYlEcOp/Zf8xOQvgyRG/fSmAZ0ospw3U5jSozWk4ljafY2bjg95oLNCPBcmpvGsZxEptToPanIaq2qwuFxGRSCjQRUQi0dZA39J0AQ1Qm9OgNqehkja3sg9dRESO1tYzdBER6aNAFxGJROsCPXTD6rYiuZXkAZIP97x2Gsm7SH4v+3pqz3vXZetgN8lLmqn62JBcQfI/ST5KchfJT2avR9tukm8l+W2SD2Rt/rPs9WjbDHTuTUzyOyTvyJ5H3V4AIPkEyYdI3k9yKnut2nabWWv+oXP53u8DeCeA4wE8AGBN03WV1LYPAjgfwMM9r/0lgE3Z400APpc9XpO1fTGAVdk6GWu6DSO0+UwA52ePTwLwWNa2aNuNzt29lmSPjwPwPwAuirnNWTv+CMA/ALgjex51e7O2PAFgad9rlba7bWfoFwKYNrM9ZnYQwDYA6xuuqRRmdg+A5/peXg/gK9njrwD4lZ7Xt5nZG2b2ODrXob+wjjrLZGb7zey+7PH/AXgUnXvRRttu63g5e3pc9s8QcZtJLgfwEQA39rwcbXsDKm132wI972bUsTrDsjs/ZV9Pz16Pbj2QXAngp9A5Y4263Vn3w/0ADgC4y8xib/MXAHwKQO8domNu7xwD8G8kd5LcmL1WabsL3eDCkUI3o05AVOuB5BIAtwH4QzN7iRzUvM6iA15rXbvN7DCA95A8BcDXSL57gcVb3WaSvwTggJntJHlxkW8Z8Fpr2tvn/Wa2j+TpAO4i+d0Fli2l3W07Q0/tZtQ/IHkmAGRfD2SvR7MeSB6HTpjfYma3Zy9H324AMLMXANwNYC3ibfP7AfwyySfQ6SL9EMmbEW97jzCzfdnXAwC+hk4XSqXtblugF7lhdUy2A/hY9vhjAL7e8/oGkotJrgKwGsC3G6jvmLBzKv53AB41s7/ueSvadpMcz87MQfIEAL8A4LuItM1mdp2ZLTezlejsr/9hZr+FSNs7h+SJJE+aewzgwwAeRtXtbnokeISR40vR+TTE9wF8uul6SmzXrQD2A/ghOkfrjwN4B4B/B/C97OtpPct/OlsHuwGsa7r+Edv8M+j8WfkggPuzf5fG3G4A5wL4TtbmhwF8Jns92jb3tONidD/lEnV70fkk3gPZv11zWVV1uzX1X0QkEm3rchERkRwKdBGRSCjQRUQioUAXEYmEAl1EJBIKdBGRSCjQRUQi8f+6ohAR2UyoWQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(np.arange(len(r_avg_list)),r_avg_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
