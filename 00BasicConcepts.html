
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Basic Concepts of Data Mining and Machine Learning &#8212; Machine Learning Lecture</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=62ba249389abaaa9ffc34bf36a076bdc1d65ee18" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=f31d14ad54b65d19161ba51d4ffff3a77ae00456"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="K - Nearest Neighbour Classification / Regression" href="machinelearning/knn.html" />
    <link rel="prev" title="Intro and Overview Machine Learning Lecture" href="intro.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/hdmlogomed.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Machine Learning Lecture</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Intro and Overview Machine Learning Lecture
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Basic Concepts of Data Mining and Machine Learning
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Conventional ML
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="machinelearning/knn.html">
   K - Nearest Neighbour Classification / Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="machinelearning/parametricClassification1D.html">
   Bayes- and Naive Bayes Classifier
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="machinelearning/LinReg.html">
   Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="machinelearning/heartRateRegression.html">
   Example Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="machinelearning/LinearClassification.html">
   Linear Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="machinelearning/svm.html">
   Support Vector Machines (SVM)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="machinelearning/gp.html">
   Gaussian Process
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="machinelearning/GaussianProcessRegression.html">
   Gaussian Process: Implementation in Python
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Neural Networks
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="neuralnetworks/01NeuralNets.html">
   Neural Networks Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="neuralnetworks/02RecurrentNeuralNetworks.html">
   Recurrent Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="neuralnetworks/03ConvolutionNeuralNetworks.html">
   Convolutional Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="neuralnetworks/convolutionDemos.html">
   Animations of Convolution and Deconvolution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="neuralnetworks/03KerasMLPandCNNcifar.html">
   Implementing Neural Networks with Keras
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="neuralnetworks/04KerasPretrainedClassifiers.html">
   Applying Pretrained Deep Neural Networks for Image Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="neuralnetworks/05KerasPretrainedCovid.html">
   Apply Pretrained Neural Networks on new Task
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Autoencoder
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="neuralnetworks/04VariationalAutoencoder.html">
   Variational Autoencoder (VAE) to generate handwritten digits
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  GAN
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="gan/GAN.html">
   Generative Adversarial Nets (GAN)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="gan/DCGAN.html">
   DCGAN Keras Implementation
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Reinforcement Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="rl/reinforcement.html">
   Reinforcement Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="rl/DQN.html">
   Deep Reinforcement Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="rl/QLearnFrozenLake.html">
   Example Q-Learning
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Text
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="text/01ModellingWordsAndTexts.html">
   Representations for Words and Texts
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="text/02TextClassification.html">
   Text classification with CNNs and LSTMs
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Graph Neural Networks
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="neuralnetworks/GraphNeuralNetworks.html">
   Graph Neural Networks (GNN)
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Transformer
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="transformer/attention.html">
   Sequence-To-Sequence, Attention, Transformer
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="transformer/intent_classification_with_bert.html">
   Intent Classification with BERT
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="referenceSection.html">
   References
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/00BasicConcepts.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/00BasicConcepts.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#overview-data-mining-process">
   Overview Data Mining Process
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#machine-learning-definition-concepts-categories">
   Machine Learning: Definition, Concepts, Categories
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#definition-machine-learning">
     Definition Machine Learning
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#categorisation-of-machine-learning-approaches">
     Categorisation of Machine Learning Approaches
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#supervised-learning">
       Supervised Learning
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#unsupervised-learning">
       Unsupervised Learning
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#reinforcement-learning-learning-from-feedback">
       Reinforcement Learning: Learning from Feedback
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#general-scheme-for-machine-learning">
     General Scheme for Machine Learning
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#general-concept-of-supervised-learning">
     General Concept of Supervised Learning
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#required-amount-of-data-supervised-learning">
     Required Amount of data (supervised learning)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#further-categories">
     Further Categories
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#self-supervised-learning">
       Self-Supervised Learning
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#semi-supervised-learning">
       Semi-Supervised Learning
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#supervised-pretraining">
       Supervised Pretraining
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#few-shot-learning">
       Few-Shot Learning
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cross-validation">
     Cross Validation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#performance-metrics">
     Performance Metrics
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#classification">
       Classification
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#regression">
       Regression
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bias-and-variance-overfitting-and-underfitting">
     Bias and Variance, Overfitting and Underfitting
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#bias-variance-tradeoff">
       Bias-Variance-Tradeoff
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#overfitting-underfitting">
       Overfitting / Underfitting
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-incomplete-list-of-ai-applications">
   A incomplete list of AI Applications
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#current-problems-challenges-of-ai-and-ml">
   Current Problems/Challenges of AI and ML
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Basic Concepts of Data Mining and Machine Learning</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#overview-data-mining-process">
   Overview Data Mining Process
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#machine-learning-definition-concepts-categories">
   Machine Learning: Definition, Concepts, Categories
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#definition-machine-learning">
     Definition Machine Learning
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#categorisation-of-machine-learning-approaches">
     Categorisation of Machine Learning Approaches
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#supervised-learning">
       Supervised Learning
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#unsupervised-learning">
       Unsupervised Learning
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#reinforcement-learning-learning-from-feedback">
       Reinforcement Learning: Learning from Feedback
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#general-scheme-for-machine-learning">
     General Scheme for Machine Learning
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#general-concept-of-supervised-learning">
     General Concept of Supervised Learning
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#required-amount-of-data-supervised-learning">
     Required Amount of data (supervised learning)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#further-categories">
     Further Categories
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#self-supervised-learning">
       Self-Supervised Learning
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#semi-supervised-learning">
       Semi-Supervised Learning
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#supervised-pretraining">
       Supervised Pretraining
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#few-shot-learning">
       Few-Shot Learning
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cross-validation">
     Cross Validation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#performance-metrics">
     Performance Metrics
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#classification">
       Classification
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#regression">
       Regression
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bias-and-variance-overfitting-and-underfitting">
     Bias and Variance, Overfitting and Underfitting
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#bias-variance-tradeoff">
       Bias-Variance-Tradeoff
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#overfitting-underfitting">
       Overfitting / Underfitting
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-incomplete-list-of-ai-applications">
   A incomplete list of AI Applications
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#current-problems-challenges-of-ai-and-ml">
   Current Problems/Challenges of AI and ML
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="basic-concepts-of-data-mining-and-machine-learning">
<h1>Basic Concepts of Data Mining and Machine Learning<a class="headerlink" href="#basic-concepts-of-data-mining-and-machine-learning" title="Permalink to this headline">#</a></h1>
<ul class="simple">
<li><p>Author: Johannes Maucher</p></li>
<li><p>Last Update: 04.10.2022</p></li>
</ul>
<div class="section" id="overview-data-mining-process">
<h2>Overview Data Mining Process<a class="headerlink" href="#overview-data-mining-process" title="Permalink to this headline">#</a></h2>
<p>The <strong>Cross-industry standard process for data mining (CRISP)</strong> proposes a common approach for realizing data mining projects:</p>
<img src="https://maucher.home.hdm-stuttgart.de/Pics/CRISPsmall.png" alt="Drawing" style="width: 400px;"/>
<p>In the first phase of CRISP the overall business-case, which shall be supported by the data mining process must be clearly defined and understood. Then the goal of the data mining project itself must be defined. This includes the specification of metrics for measuring the performance of the data mining project.</p>
<p>In the second phase data must be gathered, accessed, understood and described. Quantitiy and qualitity of the data must be assessed on a high-level.</p>
<p>In the third phase data must be investigated and understood more thoroughly. Common means for understanding data are e.g. visualization and the calculation of simple statistics. Outliers must be detected and processed, sampling rates must be determined, features must be selected and eventually be transformed to other formats.</p>
<p>In the modelling phase various algorithms and their hyperparameters are selected and applied. Their performance on the given data is determined in the evaluation phase.</p>
<p>The output of the evaluation is usually fed back to the first phases (business- and data-understanding). Applying this feedback the techniques in the overall process are adapted and optimized. Usually only after several iterations of this process the evaluation yields good results and the project can be deployed.</p>
</div>
<div class="section" id="machine-learning-definition-concepts-categories">
<h2>Machine Learning: Definition, Concepts, Categories<a class="headerlink" href="#machine-learning-definition-concepts-categories" title="Permalink to this headline">#</a></h2>
<p>Machine Learning constitutes one of the 4 categories of Artificial Intelligence (AI). As shown in the image below, the other categories are <em>Search and Planning</em>, <em>Knowledge and Inference</em> and <em>Modelling of Uncertainty</em>.</p>
<img src="http://maucher.home.hdm-stuttgart.de/Pics/categoriesEnglish.png" alt="Drawing" width="600">
<p>Currently, Machine Learning is by far the most important AI category.</p>
<p>The following cartoon depicts the idea of supervised learning. A teacher provides labels for the input and a relation (model) between input and label is learned. Actually, the cartoon sketches a crucial problem of supervised Machine Learning: If we have only a small amount of training data, the learned model is overfits to a probably irrelevant feature.</p>
<img src="http://maucher.home.hdm-stuttgart.de/Pics/MLcartoon.JPG" alt="Drawing" width="300">
<div class="section" id="definition-machine-learning">
<h3>Definition Machine Learning<a class="headerlink" href="#definition-machine-learning" title="Permalink to this headline">#</a></h3>
<p>There is no unique definition of Machine Learning. One of the most famous definitions has been formulated in <a class="reference external" href="http://www.cs.cmu.edu/~tom/mlbook.html">Tom Mitchell, Machine Learning</a>:</p>
<ul class="simple">
<li><p>A computer is said to learn from <strong>experience E</strong> with respect to some <strong>task T</strong> and some <strong>performance measure P</strong> , if its performance on T, as measured by P, improves with experience E.</p></li>
</ul>
<p>This definition has a very pragmatic implication: At the very beginning of any Machine Learning project one should specify T, E and P! In some projects the determination of these elements is trivial, in particular the <em>task T</em> is usually clear. However, the determination of <em>experience E</em> and <em>performance measure P</em> can be sophisticated. Spend time to specify these elements. It will help you to understand, design and evaluate your project.</p>
<p><strong>Examples:</strong> What would be T, E and P for</p>
<ul class="simple">
<li><p>a spam-classifier</p></li>
<li><p>an intelligent search-engine, which provides individual results on queries</p></li>
<li><p>a recommender-system for an online-shop</p></li>
</ul>
</div>
<div class="section" id="categorisation-of-machine-learning-approaches">
<h3>Categorisation of Machine Learning Approaches<a class="headerlink" href="#categorisation-of-machine-learning-approaches" title="Permalink to this headline">#</a></h3>
<p>The field of Machine Learning is usually categorized with respect to two dimensions: The first dimension is the question <em>What shall be learned?</em> and the second asks for <em>How shall be learned?</em>. The resulting 2-dimensional matrix is depicted below:</p>
<img src="http://maucher.home.hdm-stuttgart.de/Pics/mlCategories.png" style="width:800px" align="center"><p>On an abstract level there exist 4 answers on the first question. One can either learn</p>
<ul class="simple">
<li><p>a classifier, e.g. object recognition, spam-filter, Intrusion detection, …</p></li>
<li><p>a regression-model, e.g. time-series prediction, like weather- or stock-price forecasts, range-prediction for electric vehicles, estimation of product-quantities, …</p></li>
<li><p>associations between instances, e.g. document clustering, customer-grouping, quantisation problems, automatic playlist-generation, ….</p></li>
<li><p>associations between features, e.g. market basket analysis (customer who buy cheese, also buy wine, …)</p></li>
<li><p>strategie, e.g. for automatic driving or games</p></li>
</ul>
<img src="https://maucher.home.hdm-stuttgart.de/Pics/classReg.PNG" alt="Drawing" style="width: 800px;"/>
<p>On the 2nd dimension, which asks for <em>How to learn?</em>, the answers are:</p>
<ul class="simple">
<li><p>supervised: This category requires a <em>teacher</em> who provides labels (target-values) for each training-element. For example in face-recognition the teacher most label the inputs (pictures of faces) with the name of the corresponding persons. In general labeling is expensive and labeled data is scarce.</p></li>
<li><p>unsupervised learning: In this case training data consists only of inputs - no teacher is required for labeling. For example pictures can be clustered, such that similar pictures are assigned to the same group.</p></li>
<li><p>Reinforcement learning: In this type no teacher who lables each input-instance is available. However, there is a critics-element, which provides feedback from time-to-time. For example an intelligent agent in a computer game maps each input state to a corresponding action. Only after a possibly long sequence of actions the agent gets feedback in form of an increasing/decreasing High-Score.</p></li>
</ul>
<div class="section" id="supervised-learning">
<h4>Supervised Learning<a class="headerlink" href="#supervised-learning" title="Permalink to this headline">#</a></h4>
<img src="http://maucher.home.hdm-stuttgart.de/Pics/introExampleLearning.png" style="width:800px" align="center"><p><strong>Apply Learned Modell:</strong>
<img src="http://maucher.home.hdm-stuttgart.de/Pics/introExampleLearningApply.png" style="width:800px" align="center"></p>
</div>
<div class="section" id="unsupervised-learning">
<h4>Unsupervised Learning<a class="headerlink" href="#unsupervised-learning" title="Permalink to this headline">#</a></h4>
<img src="http://maucher.home.hdm-stuttgart.de/Pics/introExampleLearningUnsupervised.png" style="width:800px" align="center"><p><strong>Apply learned Model:</strong>
<img src="http://maucher.home.hdm-stuttgart.de/Pics/introExampleLearningUnsupervisedApply.png" style="width:800px" align="center"></p>
<hr class="docutils" />
<p><strong>Associations between Instances (Clustering) and Associations between Features (Association Rule Mining):</strong>
<img src="https://maucher.home.hdm-stuttgart.de/Pics/unsupervisedML.png" style="width:400px" align="center"></p>
</div>
<hr class="docutils" />
<div class="section" id="reinforcement-learning-learning-from-feedback">
<h4>Reinforcement Learning: Learning from Feedback<a class="headerlink" href="#reinforcement-learning-learning-from-feedback" title="Permalink to this headline">#</a></h4>
<img src="https://maucher.home.hdm-stuttgart.de/Pics/LeaMayerSteepleEugene.jpg" style="width:500px" align="center"></div>
</div>
<div class="section" id="general-scheme-for-machine-learning">
<h3>General Scheme for Machine Learning<a class="headerlink" href="#general-scheme-for-machine-learning" title="Permalink to this headline">#</a></h3>
<p>In Machine Learning one distinguishes</p>
<ul class="simple">
<li><p>training-phase,</p></li>
<li><p>validation-phase,</p></li>
<li><p>test-phase,</p></li>
<li><p>operational phase.</p></li>
</ul>
<p>Training and validation are shown in the image below.</p>
<ol class="simple">
<li><p>In the <strong>training phase</strong> training-data is applied to learn a general model. The model either describes the structure of the training data (in the case of unsupervised learning) or a function, which maps input-data to outputs.</p></li>
<li><p><strong>Validation phase:</strong> During the training-phase the model is fitted to the training-data. However, the primal goal is not to find a model, which is maximally fitted to training data. Instead we like to learn a model, which <strong>generalizes well</strong>, i.e. it shall be good on new data, which has not been seen during training. For this a validation-data partition, which is disjoint to the training-data partition, is applied to the model. This means, that for each input of the validation-partition the learned models prediction is calculated and compared to the true output (label). In this way an error-statistic, more general: a performance-measure, can be calculated. The performance measure on the validation data is used to assess the learned model, to compare different models and to select the best trained model.</p></li>
<li><p><strong>Test phase:</strong> In order to estimate the selected models performance in real life, one applies a test-data partition, which is disjoint to training-data and validation-data. The performance on the test-data tells us the accuracy we can expect in the operational mode.</p></li>
<li><p><strong>Operational mode:</strong> If the performance in the preceding test phase is sufficient for our purpose, the model can be deployed to the target device. In the operational mode, the model receives new input data and has to calculate the corresponding output (class, prediction, etc.)</p></li>
</ol>
<img src="https://maucher.home.hdm-stuttgart.de/Pics/Learning.png" alt="Drawing" style="width: 800px;"/><p>As shown in the picture above, usually the available data can not be passed directly to the machine-learning algorithm. Instead it must be processed in order to transform it to a corresponding format and to extract meaningful features. The usual formal, accepted by all machine-learning algorithms is a 2-dimensional array, whose rows are the instances (e.g. documents, images, customers, …) and whose columns are the features, which describe the instances (e.g. words, pixels, bought products, …):</p>
<img src="https://maucher.home.hdm-stuttgart.de/Pics/dataMatrix.png" alt="Drawing" style="width: 800px;"/>
<p>The image below depicts such a 2-dimensional array of training-data for the applications <em>Object Recognition, Document Classification, Personality Classification, Temperature Prediction</em> and <em>Recommender System</em>.</p>
<img src="https://maucher.home.hdm-stuttgart.de/Pics/mlDataExamples.png" alt="Drawing" style="width: 800px;"/>
</div>
<div class="section" id="general-concept-of-supervised-learning">
<h3>General Concept of Supervised Learning<a class="headerlink" href="#general-concept-of-supervised-learning" title="Permalink to this headline">#</a></h3>
<p>The general concept of supervised learning is sketched in the image below. This concept is realized by almost all algorithms for supervised machine learning, in particular all neural networks learn according to this approach. In the following description it is assumed that the ML algorithm is a Neural Network.</p>
<img src="https://maucher.home.hdm-stuttgart.de/Pics/learnGradientDescent.png" alt="Drawing" style="width: 600px;"/>
<p><strong>General concept of iterative supervised learning:</strong></p>
<ol class="simple">
<li><p>The network parameters <span class="math notranslate nohighlight">\(\Theta\)</span> are initialized randomly</p></li>
<li><p>Apply the input vectors <span class="math notranslate nohighlight">\(\mathbf{x}_p\)</span> of one or more training instances to the input of the network</p></li>
<li><p>Calculate the corresponding output of the network <span class="math notranslate nohighlight">\(y=f(\mathbf{x}_p,\Theta)\)</span></p></li>
<li><p>Calculate the error between the calulated outputs <span class="math notranslate nohighlight">\(y\)</span> and the corresponding target labels <span class="math notranslate nohighlight">\(r\)</span></p></li>
<li><p>Depending on the calculated errors, adjust the network weights <span class="math notranslate nohighlight">\(\Theta\)</span>, such that in the sequel the network outputs are closer to the targets (i.e. the error decreases).</p></li>
<li><p>Repeat this until the calculated error is small enough.</p></li>
</ol>
</div>
<div class="section" id="required-amount-of-data-supervised-learning">
<h3>Required Amount of data (supervised learning)<a class="headerlink" href="#required-amount-of-data-supervised-learning" title="Permalink to this headline">#</a></h3>
<p>One of the crucial questions at the beginning of each ML-procect is</p>
<p><em>How much labeled data is required?</em></p>
<p>The answer is: <em>It depends!</em></p>
<p>Unfortunately one calculate the required amount of labeled data for</p>
<ul class="simple">
<li><p>training</p></li>
<li><p>validation</p></li>
<li><p>test.</p></li>
</ul>
<p>However, the main factors, which influence the amount of labeled data, are known and depicted below:</p>
<img src="https://maucher.home.hdm-stuttgart.de/Pics/amountDataNew.png" alt="Drawing" style="width: 600px;"/></div>
<div class="section" id="further-categories">
<h3>Further Categories<a class="headerlink" href="#further-categories" title="Permalink to this headline">#</a></h3>
<p>Even though Machine Learning showed amazing accuracy in a wide range of tasks such as object classification, machine translation and automated content generation, many experts are convinced that in order to create human-level AI new approaches to ML and AI must be invented. The current methods, in particular supervised learning, is supposed to get stuck in a suboptimum, far from human-level AI. The reason for these doubts is that supervised ML requires large amounts of labeled data and in general labeling is expensive. On the other hand a main factor of human intelligence is <strong>common sense</strong>, i.e. knowledge on general concepts such as <em>gravity</em> or <em>object permanence</em>. Human’s create <em>common sense</em> from their birth on, mainly by unsupervised observation of the world. It is because of this understanding of common concepts that makes our learning of specific things efficient. For example, based on the knowledge of <em>gravity</em>, we do not need much specific training samples in order to predict the trajectory of a stone, which is thrown away.</p>
<p>It is clear that supervised learning alone will not be sufficient to teach machines something like common sense, since it is impossible to label everything. Therefore, recent ML research has a strong focus on finding new methods for learning from unlabeled data. In particular concepts that exploit both, a relatively small set of labeled data and a large set of unlabeled data, are investigated. Two main categories of this type are <em>self-supervised learning</em> and <em>semi-supervised learning</em>.</p>
<div class="section" id="self-supervised-learning">
<h4>Self-Supervised Learning<a class="headerlink" href="#self-supervised-learning" title="Permalink to this headline">#</a></h4>
<p>Self-supervised learning consists of 2 stages: First large amounts of unlabeled data are applied to <em>learn a feature-extractor, which  provides a good representation of the given data domain</em>. This feature-extractor is passed to stage 2, where it can be applied for many different tasks with the same input data domain. In order to learn these task-specific models in the 2nd stage only relatively small amounts of task-specific labeled data is required. This concept is depicted in the image below. Examples are e.g. <a class="reference internal" href="text/01ModellingWordsAndTexts.html"><span class="doc std std-doc">Word2Vec</span></a> or transformers like <a class="reference internal" href="transformer/attention.html"><span class="doc std std-doc">BERT</span></a>. In both examples unlabeled text data (e.g. the entire Wikipedia, or tons of books) are applied for pretraining. During pretraining the feature-extractor is trained such that it predicts masked words (target) from the surrounding words (input).</p>
<img src="https://maucher.home.hdm-stuttgart.de/Pics/self_supervised_learning" alt="Drawing" style="width: 800px;"/>
<p>Self-supervised learning is only applicable, if there exist semantic relations between the elements <span class="math notranslate nohighlight">\(x_i\)</span> of the unlabeled data. This is true e.g. for text (words within sentences are related), images (neighbouring pixels are correlated) and video (successive frames within the video are correlated).</p>
</div>
<div class="section" id="semi-supervised-learning">
<h4>Semi-Supervised Learning<a class="headerlink" href="#semi-supervised-learning" title="Permalink to this headline">#</a></h4>
<p>Semi-supervised learning is another approach to learn models from relatively small amounts of labeled data and large amounts of unlabeled data. The trick in self-supervised learning was to apply in stage 1 a supervised-learning approach on unlabeled data. This is feasible only if there exists semantic relations between the elements of the input-data-vectors. Now, in semi-supervised learning, we do not have to impose this hard restriction on the structure of data. However, there exist other assumptions, which must be fullfilled (at least one), e.g. the</p>
<ul class="simple">
<li><p><strong>Continuity Assumption:</strong> Input-vectors, which are close to each other, likely share the same label.</p></li>
<li><p><strong>Cluster Assumption:</strong> The set of all input-vectors is grouped in different clusters and vectors within the same cluster likely share the same label.</p></li>
<li><p><strong>Manifold-Assumption:</strong> Input vectors lie approximately on a manifold (subspace) of much lower dimension than the input space.</p></li>
</ul>
<p>Semi-supervised learning may refer to either <strong>transductive learning</strong> or <strong>inductive learning</strong>.The goal of transductive learning is to infer the correct labels <em>for the given unlabeled data only</em>. The goal of inductive learning is to infer the <em>correct mapping from input data to target in general</em>. (Source: <a class="reference external" href="https://en.wikipedia.org/wiki/Semi-supervised_learning">Wikipedia</a>).</p>
<img src="https://maucher.home.hdm-stuttgart.de/Pics/SemiSupervised.png" alt="Drawing" style="width: 800px;"/>
<p>There exists many approaches for semi-supervised learning. One is to use the labeled data to learn an initial classification model. Then the unlabeled data is applied to learn better class-specific distributions, which provide better classifiers.</p>
</div>
<div class="section" id="supervised-pretraining">
<h4>Supervised Pretraining<a class="headerlink" href="#supervised-pretraining" title="Permalink to this headline">#</a></h4>
<p>A common and extremely efficient approach to apply deep neural networks in the case of only relatively small amounts of labeled training data is to integrate neural networks, which have been <strong>pretrained on a very large labeled dataset</strong>. Such pretrained deep neural networks can be downloaded e.g. from</p>
<ul class="simple">
<li><p><a class="reference external" href="https://keras.io/api/applications/">Keras applications</a></p></li>
<li><p><a class="reference external" href="https://www.tensorflow.org/hub">Tensorflow Hub</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/models">Huggingface</a></p></li>
</ul>
<p>As in the case of self-supervised learning, the <strong>learned feature extractor</strong> constitutes a basis, which can be adapted and fine-tuned for different tasks. Again, a relatively small amount of task-specific labeled data is required for fine-tuning.</p>
<img src="https://maucher.home.hdm-stuttgart.de/Pics/supervised_pretraining.png" alt="Drawing" style="width: 800px;"/>
</div>
<div class="section" id="few-shot-learning">
<h4>Few-Shot Learning<a class="headerlink" href="#few-shot-learning" title="Permalink to this headline">#</a></h4>
<p>The term <strong>Few-Shot-Learning</strong> refers to approaches, which provide models that can be applied for classification, even though there is only a very small amount of labeled data - too small to apply conventional ML. In the extreme case of <strong>One-Shot-Learning</strong> only one labeled training instance per class is required.</p>
<p>Few-Shot-Learning is often described as <strong>N-way-K shot meta learning classification</strong>. This means that for each of the <span class="math notranslate nohighlight">\(N\)</span> classes <span class="math notranslate nohighlight">\(K\)</span> labeled training instances are available. Typical numbers for <span class="math notranslate nohighlight">\(N\)</span> and <span class="math notranslate nohighlight">\(K\)</span> are 10 and 5, respectively.</p>
<p>As sketched in the image below (Image Source: <a class="reference external" href="https://www.borealisai.com/research-blogs/tutorial-2-few-shot-learning-and-meta-learning-i/?utm_source=pocket_mylist">https://www.borealisai.com</a>), the idea of meta learning is to <em>learn how to classify</em> by training a network on different but similar tasks. The more similar tasks are applied to learn the network’s weight, the better it’s capability to distinguish classes in general.</p>
<img src="https://maucher.home.hdm-stuttgart.de/Pics/NoverKfewshot.png" alt="Drawing" style="width: 800px;"/>
<p>A quit popular realisation of few-shot-learning is the <strong>Matching Networks</strong> approach, which has been introduced in <a class="reference external" href="https://arxiv.org/pdf/1606.04080.pdf">O. Vinyals et al</a>. The picture below describes the idea of this approach:</p>
<img src="https://maucher.home.hdm-stuttgart.de/Pics/matchingnetworks.png" alt="Drawing" style="width: 600px;"/>
<p>Training instances (here <span class="math notranslate nohighlight">\(K=4\)</span> different classes) are mapped into an embedding space. The mapping is realized by a neural network <span class="math notranslate nohighlight">\(g\)</span> with learnable parameters <span class="math notranslate nohighlight">\(\Theta\)</span>. The query-image is also mapped to the same embedding space. The function <span class="math notranslate nohighlight">\(f\)</span> applied for mapping the query image need not be the same as the function <span class="math notranslate nohighlight">\(g\)</span>. In the embedded space a simple nearest-neighbour search is applied to determine the class of the query image. I.e. for the given representation of the query-image the closest representation of a training image is determined. The class of this closest training image is the decision. The more similar tasks are available to learn the functions <span class="math notranslate nohighlight">\(f\)</span> and <span class="math notranslate nohighlight">\(g\)</span> the better the accuracy on similar tasks.</p>
</div>
</div>
<div class="section" id="cross-validation">
<h3>Cross Validation<a class="headerlink" href="#cross-validation" title="Permalink to this headline">#</a></h3>
<p>K-fold cross-Validation is the standard validation method if labeled data is rare. The entire set of labeled data is partitioned into k (<span class="math notranslate nohighlight">\(k=10\)</span> in the example below) disjoint subsets. The entire evaluation consists of k iterations. In the <a class="reference external" href="http://i.th">i.th</a> iteration, the <a class="reference external" href="http://i.th">i.th</a> partition (subset) is applied for validation, all other partitions are applied for training the model. In each iteration the model’s performance, e.g. accuracy, is determined on the validation-partition. Finally, the overall performance is the average performance over all k performance values.</p>
<img src="https://maucher.home.hdm-stuttgart.de/Pics/CrossValidation.jpg" alt="Drawing" style="width: 800px;"/></div>
<div class="section" id="performance-metrics">
<h3>Performance Metrics<a class="headerlink" href="#performance-metrics" title="Permalink to this headline">#</a></h3>
<p>Below <span class="math notranslate nohighlight">\(r_i\)</span> denotes the true label, given in the labeled dataset and <span class="math notranslate nohighlight">\(y_i\)</span> denotes the corresponding label as predicted by the learned model. Obviously the goal is to learn a model, whose predictions <span class="math notranslate nohighlight">\(y_i\)</span> are as close as possible to the true labels <span class="math notranslate nohighlight">\(r_i\)</span>. For classification and regression there exists different metrics to measure the difference between true and predicted labels. The most important metrics are summarized below. Metrics, provided by scikit-learn, can be found here: <a class="reference external" href="http://scikit-learn.org/stable/modules/model_evaluation.html">Scores in scikit-learn</a>.</p>
<div class="section" id="classification">
<h4>Classification<a class="headerlink" href="#classification" title="Permalink to this headline">#</a></h4>
<p>For classification the most prominent metric is <strong>accuracy</strong>, which is just the rate of correct classifications. For the analysis of a classifier, determination of accuracy alone is not sufficient. The metrics defined below provide more subtle information on correct and erroneous events. All of the defined evaluation metrics can be obtained from the confusion matrix. For a binary classifier, the <strong>confusion matrix</strong> is depicted below. For a <em>K</em>-class classifier, the confusion matrix has size <span class="math notranslate nohighlight">\(K \times K\)</span>. The rows correspond to the true labels, the columns to the predicted labels.</p>
<img src="https://maucher.home.hdm-stuttgart.de/Pics/confusionMat.png" style="width:300px" align="center"><p><strong>Accuracy:</strong> The rate of overall correct classifications:</p>
<div class="math notranslate nohighlight">
\[
ACC=\frac{TP+TN}{FP+FN+TP+TN}
\]</div>
<p><strong>Error Rate:</strong> The rate of overall erroneous classifications:</p>
<div class="math notranslate nohighlight">
\[
ERR=\frac{FP+FN}{FP+FN+TP+TN}
\]</div>
<p><strong>False Positive Rate:</strong></p>
<div class="math notranslate nohighlight">
\[
FPR=\frac{FP}{FP+TN}
\]</div>
<p><strong>True Positive Rate:</strong></p>
<div class="math notranslate nohighlight">
\[
TPR=\frac{TP}{FN+TP}
\]</div>
<p><strong>Precision:</strong> How much of the samples, which have been classified as <em>positive</em> are actual <em>positive</em></p>
<div class="math notranslate nohighlight">
\[
PRE=\frac{TP}{FP+TP}
\]</div>
<p><strong>Recall:</strong>(=TPR): How much of the true <em>positive</em> samples has been classified as <em>positive</em></p>
<div class="math notranslate nohighlight">
\[
REC=\frac{TP}{FN+TP}
\]</div>
<p><strong>F1-Score:</strong> Harmonic mean of Precision and Recall</p>
<div class="math notranslate nohighlight">
\[
F1=2\frac{PRE \cdot REC }{PRE + REC}
\]</div>
</div>
<div class="section" id="regression">
<h4>Regression<a class="headerlink" href="#regression" title="Permalink to this headline">#</a></h4>
<p>Also regression models can be scored by a variety of metrics. The most prominent are</p>
<ul class="simple">
<li><p>mean absolute error (MAE)</p></li>
<li><p>mean squared error (MSE)</p></li>
<li><p>median absolute error (MEDE)</p></li>
<li><p>coefficient of determination (<span class="math notranslate nohighlight">\(R^2\)</span>)</p></li>
</ul>
<p>If <span class="math notranslate nohighlight">\(y_i\)</span> is the predicted value for the <a class="reference external" href="http://i.th">i.th</a> element and <span class="math notranslate nohighlight">\(r_i\)</span> is it’s true value, then these metrics are defined as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{array}[lcl]
 NMAE &amp; = &amp;   \frac{1}{N}\sum\limits_{i=1}^N |y_i-r_i| \\
 MSE &amp; = &amp;   \frac{1}{N}\sum\limits_{i=1}^N (y_i-r_i)^2  \\
 MEDE &amp; = &amp;  median\left( \; |y_i-r_i|, \; \forall \; i \; \in [1,..,N]\right) \\
\end{array}
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[
R^2  =  1- \frac{SS_e}{SS_r}, \quad \mbox{ with } SS_e=\sum_{i=1}^N(r_i-y_i)^2, \quad  SS_r=\sum_{i=1}^N(r_i-\overline{r})^2 \quad \mbox { and } \quad \overline{r}=\frac{1}{N} \sum_{i=1}^N r_i
\]</div>
<p>Another frequently used regression metric is the <strong>Root Mean Squared Logarithmic Error (RMSLE)</strong>, which is caluclated as follows:</p>
<div class="math notranslate nohighlight">
\[
RMSLE = \sqrt{\frac{1}{N} \sum\limits_{i=1}^N(\ln(r_i)-\ln(y_i))^2}
\]</div>
<p>For RMSLE there is no explicit scoring function in scikit-learn, but it can be easily computed via the NMSE-function. The RMSLE is well suited for the case that the error (i.e. the difference between <span class="math notranslate nohighlight">\(y_i\)</span> and <span class="math notranslate nohighlight">\(r_i\)</span>) increases with the values of <span class="math notranslate nohighlight">\(r_i\)</span>. Then large errors at high values of <span class="math notranslate nohighlight">\(r_i\)</span> are weighted less by RMSLE.</p>
</div>
</div>
<div class="section" id="bias-and-variance-overfitting-and-underfitting">
<h3>Bias and Variance, Overfitting and Underfitting<a class="headerlink" href="#bias-and-variance-overfitting-and-underfitting" title="Permalink to this headline">#</a></h3>
<div class="section" id="bias-variance-tradeoff">
<h4>Bias-Variance-Tradeoff<a class="headerlink" href="#bias-variance-tradeoff" title="Permalink to this headline">#</a></h4>
<p>The goal of <strong>supervised machine learning</strong> is to learn a model</p>
<div class="math notranslate nohighlight">
\[f: \cal{X} \rightarrow \cal{Y}\]</div>
<p>which maps inputs from <span class="math notranslate nohighlight">\(\cal{X}\)</span> to targets from <span class="math notranslate nohighlight">\(\cal{Y}\)</span> from a set of given training data <span class="math notranslate nohighlight">\(T=\{x_p,y_p\}_{p=1}^N\)</span> with <span class="math notranslate nohighlight">\(x_p \in \cal{X}\)</span> and <span class="math notranslate nohighlight">\(y_p \in \cal{Y}\)</span>. The learned model <span class="math notranslate nohighlight">\(f\)</span>, shall represent the deterministic part of the mapping from input <span class="math notranslate nohighlight">\(x\)</span> to target <span class="math notranslate nohighlight">\(y\)</span>. However, in Machine Learning, it is assumed that this mapping contains also a non-deterministic prediction error <span class="math notranslate nohighlight">\(\epsilon\)</span>, i.e.</p>
<div class="math notranslate nohighlight">
\[
y=f(x)+\epsilon.
\]</div>
<p>The prediction error consists of the following 3 parts:</p>
<ul class="simple">
<li><p>irreducible error</p></li>
<li><p>bias error</p></li>
<li><p>variance error</p></li>
</ul>
<p>The irreducible error can not be avoided. It is caused, e.g. by latent variables, which influence the mapping but are not part of the input <span class="math notranslate nohighlight">\(x\)</span> or by erroneous measurements. However, bias-error and variance-error can be minimized by a careful selection and configuration of the ML algorithm. The ML engineer tries to learn a model, which minimizes both, bias- and variance-error.</p>
<p>A <strong>high bias</strong> means that a learning algorithm has been selected, which imposes strict conditions on the type of the model, that can be learned. For example <strong>Linear Regression</strong> has a high bias, because only linear models can be learned. If the data can not be well fitted by such a linear model, the bias-error is high.</p>
<p>A <strong>high variance</strong> means that the learned model strongly varies with varying training data - even if the overall data distribution is identical. Algorithms, which are able to strongly adapt to the given training data are e.g. Support Vector Machines (SVM) with non-linear kernels or decision trees.</p>
<p>In the figure below the top-row (part 1 and part 2) shows a linear model of high bias (only a linear function can be learned). The variance is low because even though the training data in part 1 and part 2 is different, the learned models are similar. The lower row (part 3 and part 4) refers to an algorithm (non-linear SVM) of low-bias (many different types of functions can be learned), but high-variance, because the learned model in part 3 and part 4 are significantly different.</p>
<p>The goal is to find an algorithm, which learns a model of low bias and low variance. However, usually low bias yields high variance and vice versa. The goal is to find a good trade-off between them</p>
<img src="https://maucher.home.hdm-stuttgart.de/Pics/BiasVariance.png" alt="Drawing" style="width: 600px;"/></div>
<div class="section" id="overfitting-underfitting">
<h4>Overfitting / Underfitting<a class="headerlink" href="#overfitting-underfitting" title="Permalink to this headline">#</a></h4>
<p>The goal of supervised machine learning is to find a model, which performs well on new data, i.e. data, which has not be seen during training. It is in general no problem, to find an algorithm, which learns a model, that is well adapted to the given training data. However, it is a big challenge to find a model, that performs well on previously unseen data.</p>
<p><strong>Overfitting</strong> means, that a model has been strongly adapted to the training data, but it performs bad on new data. Algorithms of low-bias are able to learn models, which are strongly fittet to training data. However, then often the variance and the probability of overfitting are high.</p>
<p><strong>Underfitting</strong> means, that the learned model is weakly adapted to the training data. Algorithms of high-bias yield an increased probability of underfitting.</p>
<p>The image below sketches the relation between bias, variance, over- and underfitting (Image source: <a class="reference external" href="https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229">https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229</a>)</p>
<img src="https://maucher.home.hdm-stuttgart.de/Pics/biasvarianceoverfitting.png" alt="Drawing" style="width: 800px;"/></div>
</div>
</div>
<div class="section" id="a-incomplete-list-of-ai-applications">
<h2>A incomplete list of AI Applications<a class="headerlink" href="#a-incomplete-list-of-ai-applications" title="Permalink to this headline">#</a></h2>
<img src="https://maucher.home.hdm-stuttgart.de/Pics/AIapplicationsEnterprise.png" alt="Drawing" style="width: 600px;"/>
<img src="https://maucher.home.hdm-stuttgart.de/Pics/AIapplicationsIndustry.png" alt="Drawing" style="width: 600px;"/>
</div>
<div class="section" id="current-problems-challenges-of-ai-and-ml">
<h2>Current Problems/Challenges of AI and ML<a class="headerlink" href="#current-problems-challenges-of-ai-and-ml" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p><strong>Data efficiency:</strong> In order to learn complex tasks large amounts of training data are required</p></li>
<li><p><strong>Explainability/Interpretability:</strong> Models generate/predict an output for a given input, but they don’t explain why</p></li>
<li><p><strong>Confidence:</strong> Many applications require that the model outputs not only a prediction but also confidence-measure.</p></li>
<li><p><strong>Integrate Domain Knowledge:</strong> Neural Networks currently learn from data, but they are not able to integrate domain knowledge from experts. Integration of data- and expert-knowledge is desired.</p></li>
<li><p><strong>Common Sense</strong> makes human learning efficient. It is hard to learn common sense knowledge in ML.</p></li>
<li><p>ML models usually learn correlations but not <strong>Causality</strong>.</p></li>
</ul>
<img src="https://maucher.home.hdm-stuttgart.de/Pics/captionbotexample.png" alt="Drawing" style="width: 800px;"/></div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="intro.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Intro and Overview Machine Learning Lecture</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="machinelearning/knn.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">K - Nearest Neighbour Classification / Regression</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Prof. Dr. Johannes Maucher<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>